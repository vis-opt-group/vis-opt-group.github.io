<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0027)https://rsliu.tech/PEC.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  

  <title>PEC</title>
  <link rel="stylesheet" href="./PEC_files/bootstrap.min.css">
  <link rel="stylesheet" href="./PEC_files/temp.css">
  <link rel="stylesheet" href="./PEC_files/MalleConv_style.css">
  <link href="https://www.google.com/favicon.ico" rel="icon">
</head>

<body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.1043.0" data-gr-ext-installed="">

<div class="container">
  <table border="0" align="center">
    <tbody><tr>
      <td width="1600" height="150" align="center" valign="middle">
      <span class="title"><h1 style="font-weight:600;font-family:Book Antiqua;line-height:1.2;font-size:2.2rem">Practical Exposure Correction: Great Truths Are Always Simple</h1></span></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h5 style="font-family:Book Antiqua">
            Long Ma<sup>1</sup>, 
            Tianjiao Ma<sup>1</sup>, 
            Xinwei Xue<sup>2</sup>, 
            Xin Fan<sup>2</sup>, 
	      Zhongxuan Luo<sup>1</sup>, 
		Risheng Liu<sup>2, *</sup>, 
             </h5></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h4 style="font-weight:500;font-family:Book Antiqua"> <sup>1</sup> School of Software Technology, Dalian University of Technology<br><sup>2</sup> DUT-RU International School of Information Science &amp; Engineering, Dalian University of Technology</h4> </td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h5 style="font-weight:600;font-family:Book Antiqua"><a href="https://arxiv.org/pdf/2212.14245.pdf" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://github.com/vis-opt-group/PEC" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Supplementary Materials] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </h5></td>
    </tr>
  </tbody></table>
  <br>
  <p><img src="./PEC_files/Fig1.png" style="margin:auto;max-width:100%" align="middle"></p>
  <div class="text" style="text-align: left;">
      <h2 style="font-weight:600;font-family:Book Antiqua">Abstract</h2>
      <p style="text-align:justify;line-height:1.5em;font-family:Book Antiqua;font-size:1.2rem">Improving the visual quality of the given degraded observation by correcting exposure level is a fundamental task
in the computer vision community. Existing works commonly lack adaptability towards unknown scenes because of the data-driven patterns (deep networks) and limited regularization (traditional optimization), and they usually need time-consuming inference. These two points heavily limit
their practicability. In this paper, we establish a Practical
Exposure Corrector (PEC) that assembles the characteristics of efficiency and performance. To be concrete, we rethink the exposure correction to provide a linear solution
with exposure-sensitive compensation. Around generating
the compensation, we introduce an exposure adversarial
function as the key engine to fully extract valuable information from the observation. By applying the defined function, we construct a segmented shrinkage iterative scheme
to generate the desired compensation. Its shrinkage nature supplies powerful support for algorithmic stability and robustness. Extensive experimental evaluations fully reveal
the superiority of our proposed PEC. The code is available at <a href="https://rsliu.tech/PEC" target="_blank">https://rsliu.tech/PEC.</a></p>
  </div>
</div>

<br>

<!-- <div class="container">
  <h2>Main Idea</h2>
    <div class="overview">
    <p>
      Our main goal is to automatically decide which feature maps to compute for each input video in order to classify it correctly with the minimum computation. The intuition behind our proposed method is that there are many similar feature maps along the temporal and channel dimensions. For each video instance, we estimate the ratio of feature maps that need to be fully computed along the temporal dimension and channel dimension. Then, for the other feature maps, <strong>reconstruct them from those pre-computed feature maps using cheap linear operations</strong>.
    </p>

    </div>
</div> -->

<!-- <br> -->
<!-- 
<div class="container">
    <h2>Method</h2>
      <img class="img_responsive" src="./MalleConv/arch_iared.png" alt="Teaser" style="margin:auto;max-width:80%;align=center">
      <div class="pipelines" style="text-align: left;">
        <p>
          Illustration of our IA-RED<sup>2</sup> framework. We divide the transformer into D groups. Each group contains a multi-head interpreter and L combinations of the MSA and FFN. Before input to the MSA and FFN, the patch tokens will be evaluated by the multi-head interpreter to drop some uninformative patches. The multi-head interpreters are optimized by reward considering both the efficiency and accuracy.
        </p>
      </div>
</div>
<br>
<div class="container">
  <h2>Qualitative Results of Interpretability-Aware Heatmaps</h2>
    <img class="img_responsive" src="./MalleConv/qual_heatmap.png" alt="Teaser" style="margin:auto;max-width:100%">
    <div class="pipelines" style="text-align: left;">
      <p>
        Qualitative result of the heatmaps which hightlight the informative region of the input images of <strong>MemNet</strong>, <strong>raw attention</strong> at the second block, and <strong>our method</strong> with DeiT-S model. We find that our method can obviously better interpret the part-level stuff of the objects of interest.
      </p>
    </div>
</div>
<br>
<div class="container">
  <h2>Qualitative Results of Redundancy Reduction</h2>
    <img class="img_responsive" src="./MalleConv/iared_red.png" alt="Teaser" style="margin:auto;max-width:100%">
    <div class="pipelines" style="text-align: left;">
      <p>
        We visualize the hierarchical redundancy reduction process of our method with the DeiT-S model. The number on the upper-left corner of each image indicates the ratio of the remaining patches. From left to right, we can see that the network drops the redundant patches and focuses more on the high-level features of the objects.
      </p>
    </div>
</div>




<br>

<div class="container">
  <h2>Reference</h2>
    <div class="overview">
    <p>
B. Pan and Y. Jiang and R. Panda and Z. Wang and R. Feris and A. Oliva. <strong>IA-RED<sup>2</sup>: Interpretability-Aware Redundancy Reduction for Vision Transformer.</strong> arXiv 2021 <a href="http://people.csail.mit.edu/bpan/ia-red/ia-red_files/ia_bib.txt">[Bibtex]</a>
<br>


</p></div></div><br>

<div class="container">
  <h2>Acknowledgements</h2>
    <div class="overview">
    <p>
      We thank IBM for the donation to MIT of the Satori GPU cluster. This work is supported by the MIT-IBM Watson AI Lab and its member companies, Nexplore and Woodside.
<br> -->


<p></p><br>

<grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>

</body></html>
