<!DOCTYPE html>
<!-- saved from url=(0024)http://www.vog/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="shortcut icon" href="./vog_files/vog.ico" type="image/x-icon">
<title>VOG</title>
<meta name="keywords" content="">
<meta name="description" content="">
<link type="image/x-icon" rel="shortcut icon" href="http://www.vog/logo.ico">

<link rel="stylesheet" type="text/css" media="all" href="./vog_files/style.css">
<script src="./vog_files/jquery.js.ä¸‹è½½"></script> 
<script src="./vog_files/zoom.js.ä¸‹è½½"></script>
<style>
body{font-family:"Georgia"}
init {list-style-type:none}
.bigimg {
	width: 1000px!important;
	height: 563px!important ;
	position: fixed;
	left: 0;
	top: 0;
	right: 0;
	bottom: 0;
	margin: auto;
	display: none;
	z-index: 9999;
	border: 10px solid #fff;
}
.mask {
	position: fixed;
	left: 0;
	top: 0;
	right: 0;
	bottom: 0;
	background-color: #fff;
	opacity: 0.5;
	filter: Alpha(opacity=50);
	z-index: 98;
	transition: all 1s;
	display: none
}
.bigbox {
	width: 840px;
	background: #fff;
	border: 1px solid #ededed;
	margin: 0 auto;
	border-radius: 10px;
	overflow: hidden;
	padding: 10px;
}
.bigbox>.imgbox {
	width: 400px;
	height: 250px;
	float: left;
	border-radius: 5px;
	overflow: hidden;
	margin: 0 10px 10px 10px;
}
.bigbox>.imgbox>img {
	width: 100%;
}
.imgbox:hover {
	cursor: zoom-in
}
.mask:hover {
	cursor: zoom-out
}
.mask>img {
	position: fixed;
	right: 10px;
	top: 10px;
	width: 60px;
}
.mask>img:hover {
	cursor: pointer
}
/*Exhibition Research field*/
.init {list-style-type: none}
.divcss{
width:110%;
//border:1px solid;
}

.divcss_left{
padding-top:10px;
float:left;
width:20%;
//border:1px solid;
}
.divcss_left img{
box-shadow:5px 2px 15px #000
}
 
.divcss_right{
float:right;
width:75%;
border-top:5px solid #DDD;
padding-top:15px;
font-family:Constantia;
font-size:1.05em;
color:#000;
//border:1px solid;
} 

.divcss_right a{
color:#55D;
text-decoration:none;
font-style:italic;
}

.divcss_right a:hover{
color:#55D;
}

.clear{ clear:both}

/****************News******************/
.divnewscss{
width:90%;
box-shadow:2px 2px 18px #222;
}

.divnewslicss{
border-bottom:1px solid #DDD;
padding-top:0.5em;
padding-bottom:0.5em;
padding-left:0.5em;
//margin-top:-1.3em;
//margin-bottom:-1.3em;
font-family:Georgia;
font-size:1.2em;
color:#444;
transition:background-color 0.5s;
-ms-transition:background-color 0.5s; 
-moz-transition:background-color 0.5s; /* Firefox 4 */
-webkit-transition:background-color 0.5s; /* Safari and Chrome */
-o-transition:background-color 0.5s; /* Opera */
//border:1px solid #F00;
}


.divnewslicss:hover{
background-color:#DDD;
}

.newsli_left{
width:80%;
float:left;
padding-right:3em;
border-right:1px solid #AAA;

}

.newsli_right{
width:20%;
text-align:center;
float:right;
}


/**********People*****************/
.divpeople-facultycontainer{
width:100%;
margin-top:-5%;
}

.divpeople-mctitle{
text-align:center;
font-family:Georgia;
font-size:2.6em;
margin-bottom:60px;
background:#E8E8E8;
}

.divpeople-mcsubtitle{
font-family:Comic Sans MS;
font-size:1.7em;
margin-top:40px;
margin-bottom:40px;
color:#222;
text-align:center;
}

.divpeople-container{
padding-top:10px;
}

a .divpeople-protrait img{
width:100%;
border-radius:110px;
box-shadow:0 5px 13px #666;
transition:all 0.5s;
-ms-transition:all 0.5s; 
-moz-transition:all 0.5s; /* Firefox 4 */
-webkit-transition:all 0.5s; /* Safari and Chrome */
-o-transition:all 0.5s; /* Opera */
transition:all 0.5s;
}

a .divpeople-protrait:hover img{ 
border-radius:20px; 
}

.divpeople-spinfo{
text-align:center;
//margin-top:-1em;
}

.divpeople-name{
width:100%;
font-family:Georgia;
font-weight:bold;
font-size:1.2em;
}

.divpeople-sname{
width:120%;
margin-left:-10%;
font-family:Georgia;
font-weight:bold;
font-size:1.2em;
}


.divpeople-grade{
width:100%;
font-family:Georgia;
font-size:1em;
}

.divpeople-appendinfo{
width:100%;
font-family:Georgia;
font-size:1em;
color:#3AF;
font-style:italic;
}

.divpeople-1people{
margin:0 auto;
width:20%;
text-align:center;
}

.divpeople-2people1{
width:20%;
text-align:center;
float:left;
margin-left:25%;
margin-right:5%;
}

.divpeople-2people2{
width:20%;
text-align:center;
float:left;
margin-left:5%;
margin-right:25%;
}

.divpeople-3people1{
width:20%;
text-align:center;
float:left;
margin-left:12%;
margin-right:4%;}

.divpeople-3people2{
width:20%;
text-align:center;
float:left;
margin-left:4%;
margin-right:4%;}

.divpeople-3people3{
width:20%;
text-align:center;
float:left;
margin-left:4%;
margin-right:12%;}


.divpeople-4people1{
width:20%;
text-align:center;
float:left;
margin-left:4%;
margin-right:2%;}


.divpeople-4people2{
width:20%;
text-align:center;
float:left;
margin-left:2%;
margin-right:2%;}

.divpeople-4people3{
width:20%;
text-align:center;
float:left;
margin-left:2%;
margin-right:2%;}

.divpeople-4people4{
width:20%;
text-align:center;
float:left;
margin-left:2%;
margin-right:4%;
}


/**************Publication************/
.divcssp{
width:100%;
//border:1px solid #f00;
border-top:5px solid #DDD;
}

.subp{
padding-top:10px;
//border:1px solid #00f;
}

/********pic blow up************/

.divcss_pleft_1{
padding-top:10px;
margin-bottom:-20px;
float:left;
width:18%;
transition:all 0.5s;
}

.divcss_pleft_1 img{
box-shadow:5px 2px 15px #000;
border:1px solid #FAA;
transition:all 0.5s;
-ms-transition:all 0.5s; 
-moz-transition:all 0.5s; /* Firefox 4 */
-webkit-transition:all 0.5s; /* Safari and Chrome */
-o-transition:all 0.5s; /* Opera */

}

.divcss_pleft_1:hover img{
position:absolute;
width:580px;
z-index:2;
}

/*****word part for high pic********/

.divcss_pright_pichigh{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:7%;
padding-bottom:7%;
font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright_pichigh a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright_pichigh a:hover{
color:#05E;
}

/*****word part for short pic********/

.divcss_pright_picshort{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:15px;
margin-bottom:-20px;
font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright_picshort a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright_picshort a:hover{
color:#05E;
}

/*****************************/
.divcss_pright_1{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:40px;
padding-bottom:18px;
margin-bottom:-20px;
font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright_1 a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright_1 a:hover{
color:#05E;
}




/*****publication general part*******/

.divcss_pleft{
padding-top:10px;
margin-bottom:-20px;
float:left;
width:18%;
}

.divcss_pleft img{
box-shadow:5px 2px 15px #000;
border:1px solid #FAA;
}
 
.divcss_pright{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:15px;

font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright a:hover{
color:#05E;
}


/***********Calendar Part**********/
.divcalendar{
width:100%;
box-shadow:5px 2px 15px #000;
text-align:center;
}

.divcalendarlicss{
border-bottom:1px solid #DDD;

padding-left:0.5em;
font-family:Georgia;
font-size:1.05em;
color:#444;

}

.calendarli_1{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
//border:1px solid ;
}

.calendarli_2{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
border-left:1px solid #AAA;
}

.calendarli_3{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
border-left:1px solid #AAA;
}

.calendarli_4{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
border-left:1px solid #AAA;
}


/*************************************/
/* Default css file for jemdoc. */

.test{
   width:110%;
   margin-left:-5%;
   margin-top:-4em;
}

table#tlayout {
    margin-top:-2em;
    border-collapse: collapse;
    background: white;
    font-family:Georgia,serif
}


#layout-menu {
	background: #fdf4f2;
	border: 1px solid #fff;
	border-top:1px solid;
	padding-top: 0.5em;
	padding-left: 8px;
	padding-right: 8px;
	font-size: 15px;
	width: auto;
	white-space: nowrap;
    text-align: left;
    vertical-align: top;
}

#layout-menu td {
	background: #f3e6e6;
    vertical-align: top;
}

#layout-content {
	padding-top: 1.0em;
	padding-left: 1.0em;
	padding-right: 1.0em;
    border: 1px solid #fff;
    border-top:1px solid;
    background: white;
    text-align: left;
    vertical-align: top;
}

table#tlayout #layout-menu a {
	line-height: 1.5em;
	margin-left: 0.5em;
    //color:#000;
}

tt {
    background: #ffffdd;
}

pre, tt {
	font-size: 90%;
	font-family: monaco, monospace;
}

table#tlayout a, a > tt {
	color: #430086;
	//text-decoration: underline;
}
a:hover {
	text-decoration:underline;
	color: #430086;
}

#layout-menu a.current:link, #layout-menu a.current:visited {
	color: #8b0000;
	border-bottom: 1px gray solid;
}
#layout-menu a:link, #layout-menu a:visited, #layout-menu a:hover {
	color: #8b0000;
	text-decoration: none;
}


div.menu-category {
	border-bottom: 1px solid gray;
	margin-top: 0.8em;
	padding-top: 0.2em;
	padding-bottom: 0.1em;
	font-weight: bold;
    margin-left:-8px;
}

table#tlayout .menu-category a{
    font-size:1.1em;
    color:#333;
}

div.menu-item {
	padding-left: 16px;
	text-indent: -16px;
}

div#toptitle {
	padding-bottom: 0.2em;
	margin-bottom: 1.5em;
	border-bottom: 3px double gray;
}

/* Reduce space if we begin the page with a title. */
div#toptitle + h2, div#toptitle + h3 {
	margin-top: -0.7em;
}

div#subtitle {
	margin-top: 0.0em;
	margin-bottom: 0.0em;
	padding-top: 0em;
	padding-bottom: 0.1em;
}

strong {
	font-weight: bold;
}


table#tlayout h1,table#tlayout h2,table#tlayout h3 {
	color: #430086;
	margin-top: 0.7em;
	margin-bottom: 0.3em;
	padding-bottom: 0.2em;
	line-height: 1.0;
	padding-top: 0.5em;
	border-bottom: 1px solid #aaaaaa;
}

table#tlayout h1{
    font-size:2.2em;  
}

table#tlayout h2 {
	padding-top: 0.8em;
	font-size: 180%;
}

table#tlayout h2 + h3 {
    padding-top: 0.2em;
}

table#tlayout h3 {
	font-size: 110%;
	border-bottom: none;
}

table#tlayout p {
	margin-top: 0.0em;
	margin-bottom: 0.8em;
	padding: 0;
	line-height: 1.3;
}

pre {
	padding: 0;
	margin: 0;
}



table#tlayout ul, ol, dl {
	margin-top: 0.2em;
	padding-top: 0;
	margin-bottom: 0.8em;
}

table#tlayout dt {
	margin-top: 0.5em;
	margin-bottom: 0;
}

table#tlayout dl {
	margin-left: 20px;
}

table#tlayout dd {
	color: #222222;
}

table#tlayout dd > *:first-child {
	margin-top: 0;
}

table#tlayout ul {
	list-style-position: outside;
	list-style-type: square;
//    border:1px solid #00f;
    padding-left:2.5em;
}

table#tlayout p + ul, p + ol {
	margin-top: -0.5em;
}

table#tlayout li ul, li ol {
	margin-top: -0.3em;
}

table#tlayout ol {
	list-style-position: outside;
	list-style-type: decimal;
}

table#tlayout li p, dd p {
	margin-bottom: 0.3em;
}


table#tlayout ol ol {
	list-style-type: lower-alpha;
}

table#tlayout ol ol ol {
	list-style-type: lower-roman;
}

table#tlayout p + div.codeblock {
	margin-top: -0.6em;
}

table#tlayout div.codeblock, div.infoblock {
	margin-right: 0%;
	margin-top: 1.2em;
	margin-bottom: 1.3em;
}

table#tlayout div.blocktitle {
	font-weight: bold;
	color: #8b0000;
	margin-top: 1.2em;
	margin-bottom: 0.1em;
}

table#tlayout div.blockcontent {
	border: 1px solid silver;
	padding: 0.3em 0.5em;
}

table#tlayout div.infoblock > div.blockcontent {
	background: #ffffee;
}

table#tlayout div.blockcontent p + ul, div.blockcontent p + ol {
	margin-top: 0.4em;
}

table#tlayout div.infoblock p {
	margin-bottom: 0em;
}

table#tlayout div.infoblock li p, div.infoblock dd p {
	margin-bottom: 0.5em;
}

table#tlayout div.infoblock p + p {
	margin-top: 0.8em;
}

table#tlayout div.codeblock > div.blockcontent {
	background: #f6f6f6;
}

table#tlayout span.pycommand {
	color: #000070;
}

table#tlayout span.statement {
	color: #008800;
}
table#tlayout span.builtin {
	color: #000088;
}
table#tlayout span.special {
	color: #990000;
}
table#tlayout span.operator {
	color: #880000;
}
table#tlayout span.error {
	color: #aa0000;
}


@media print {
	#layout-menu { display: none; }
}

#fwtitle {
	margin: 2px;
}

#fwtitle #toptitle {
	padding-left: 0.5em;
	margin-bottom: 0.5em;
}

#layout-content h1:first-child, #layout-content h2:first-child, #layout-content h3:first-child {
	margin-top: -0.7em;
}

table#tlayout div#toptitle h1, #layout-content div#toptitle h1 {
	margin-bottom: 0.0em;
	padding-bottom: 0.1em;
	padding-top: 0;
	margin-top: 0.5em;
	border-bottom: none;
}


table {
    border: 1px solid #333;
    margin: 0 auto;
    background: #fff;
}

table.icon {
    border: 0px solid #333;
    background: #fff;
}


td {
    padding: 2px;
    padding-left: 0.5em;
    padding-right: 0.5em;
    text-align: left;
    border: 0px solid gray;
}


table.imgtable, table.imgtable td {
    border: 0px solid #333;
    text-align: left;
    margin: 0 auto;
}

.project-subtitle{
font-size:1.1em;
padding-top:10px;
padding-bottom:10px;
margin-left:-2.2em;
font-weight:600;
}

/**********Recruitment part************/

@keyframes first
{
0%  {color:#ffffff;left:-10%}
100% {left:0%}
}

@keyframes second
{
0%  {color:#ffffff;bottom:-100px}
100% {bottom:0px}
}

@keyframes third
{
0%  {color:#ffffff;left:-10%}
100% {left:0%}
}

@keyframes forth
{
0%  {color:#ffffff;right:-10%}
100% {right:0%}
}


.recruitment-container{
	//font-family:Georgia;
	margin-top:-5%;
}

.summary{
line-height:1.8em;
padding-left:0.3em;
color:#ee0000;
width:100%;
font-size:1.25em;
position:relative;
animation: third 1.5s;
}
.sub-summary
{
font-size:1.1em;
padding-left:0.3em;
width:100%;
position:relative;
animation: forth 1.5s;
}
.recruitment-title{
font-family:Georgia;
background:#ddd;
padding-left:0.3em;
font-size:1.8em;
margin-top:50px;
position:relative;
animation: first 1.5s;
}

.recruitment-content{
line-height:1.8em;
padding-top:1em;
padding-left:0.3em;

font-size:1.1em;
position:relative;
animation: second 1.5s;
}

.reruitment-teacher{
color:#223399;
margin-top:-15px;
width:50%;
font-size:1.2em;
}

.reruitment-teacher1{
color:#223399;
width:50%;
font-size:1.2em;
}


</style>
<script>
        $(function(){
            var obj = new zoom('mask', 'bigimg','smallimg');
            obj.init();
        })
    </script>
</head>

<body>
<br>
<ol id="menu" class="init">
  <li><a href="https://vis-opt-group.github.io/People.html">People</a></li>
    <li><a href="https://vis-opt-group.github.io/Project.html">Project</a></li>
    <li class="cur"><a href="https://vis-opt-group.github.io/Publication.html">Publication</a></li>
    <li><a href="https://vis-opt-group.github.io/">Home</a></li>
  </ol>
<div id="container">   <div id="site_title_big"><a href="http://www.vog/" style="text-decoration: none"><img src="./vog_files/2.png" width="600"></a></div>
<br><br>
<div class="wrap">
	<div id="primary" class="content-area">
		<main id="main" class="site-main" role="main">
<article id="post-2308" class="post-2308 page type-page status-publish hentry">
	<div class="entry-content">
<div class="test">
<p></p><table id="tlayout" summary="Table for page layout.">
<tbody>
<tr>
<td id="layout-content"></td>
</tr>
<tr valign="top"><!-----------------------------ä¾§è¾¹æ ?---------------------------------------------------->
<td id="layout-content" width="1200px">
<h1>2021</h1></td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss13&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/gao_TPAMI.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>,  Jiaxin Gao, Jin Zhang, Deyu Meng, Zhouchen Lin<br><strong><span style="font-size: 12px;"><strong>IEEE TPAMI</strong></span></strong><b> [<a href="https://arxiv.org/pdf/2101.11517">Paper</a>|<a href="https://github.com/vis-opt-group/BLO">Project Page</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss13" name="111ss13" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Bi-Level Optimization (BLO) is originated from the area of economic game theory and then introduced into the optimization community. BLO is able to handle problems with a hierarchical structure, involving two levels of optimization tasks, where one task is nested inside the other. In machine learning and computer vision fields, despite the different motivations and mechanisms, a lot of complex problems, such as hyper-parameter optimization, multi-task and meta learning, neural architecture search, adversarial learning and deep reinforcement learning, actually all contain a series of closely related subproblms. In this paper, we first uniformly express these complex learning and vision problems from the perspective of BLO. Then we construct a best-response-based single-level reformulation and establish a unified algorithmic framework to understand and formulate mainstream gradient-based BLO methodologies, covering aspects ranging from fundamental automatic differentiation schemes to various accelerations, simplifications, extensions and their convergence and complexity properties. Last but not least, we discuss the potentials of our unified BLO framework for designing new algorithms and point out some promising directions for future research.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021investigating,<br>
  title={Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond},<br>
  author={Liu, Risheng and Gao, Jiaxin and Zhang, Jin and Meng, Deyu and Lin, Zhouchen},<br>
  journal={arXiv preprint arXiv:2101.11517},<br>
  year={2021}<br>
}</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content">
<div class="publication_container" onclick="toggleDetails(&#39;111ss1&#39;);">

    <div class="publication_image"><img src="./vog_files/1.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning Deformable Image Registration from Optimization: Perspective, Modules, Bilevel Training and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zi Li, Xin Fan, Chenying Zhao, Hao Huang, Zhongxuan Luo.<br><strong><span style="font-size: 12px;"><strong>IEEE TPAMI</strong></span></strong><b> [<a href="https://arxiv.org/pdf/2004.14557">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss1" name="111ss1" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Conventional deformable registration methods aim at solving an optimization model carefully designed on image pairs and
their computational costs are exceptionally high. In contrast, recent deep learning-based approaches can provide fast deformation
estimation. These heuristic network architectures are fully data-driven and thus lack explicit geometric constraints which are
indispensable to generate plausible deformations, e.g., topology-preserving. Moreover, these learning-based approaches typically pose
hyper-parameter learning as a black-box problem and require considerable computational and human effort to perform many training runs.
To tackle the aforementioned problems, we propose a new learning-based framework to optimize a diffeomorphic model via multi-scale
propagation. Specifically, we introduce a generic optimization model to formulate diffeomorphic registration and develop a series of
learnable architectures to obtain propagative updating in the coarse-to-fine feature space. Further, we propose a new bilevel self-tuned
training strategy, allowing efficient search of task-specific hyper-parameters. This training strategy increases the flexibility to various types
of data while reduces computational and human burdens. We conduct two groups of image registration experiments on 3D volume
datasets including image-to-atlas registration on brain MRI data and image-to-image registration on liver CT data. Extensive results
demonstrate the state-of-the-art performance of the proposed method with diffeomorphic guarantee and extreme efficiency. We also
apply our framework to challenging multi-modal image registration, and investigate how our registration to support the down-streaming
tasks for medical image analysis including multi-modal fusion and image segmentation.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">@article{liu2021learning,<br>
  	title={Learning deformable image registration from optimization: perspective, modules, bilevel training and beyond},<br>
  	author={Liu, Risheng and Li, Zi and Fan, Xin and Zhao, Chenying and Huang, Hao and Luo, Zhongxuan},<br>
  	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},<br>
  	year={2021},<br>
  	publisher={IEEE}<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss3&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/mu_tip_1.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Triple-level Model Inferred Collaborative Network Architecture for Video Deraining.
</b><br>

      <p style="line-height:2em">Pan Mu, Zhu Liu, Yaohua Liu, <b>Risheng Liu*</b>, Xin Fan<br>
	  <strong><span style="font-size: 12px;">
	  <strong>IEEE TIP</strong></span></strong><b> 
	   [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9628137">Paper</a>|<a href="https://github.com/vis-opt-group/TMICS">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss3" name="111ss3" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Video deraining is an important issue for outdoor
vision systems and has been investigated extensively. However,
designing optimal architectures by the aggregating model formation and data distribution is a challenging task for video
deraining. In this paper, we develop a model-guided triplelevel optimization framework to deduce network architecture
with cooperating optimization and auto-searching mechanism, named Triple-level Model Inferred Cooperating Searching
(TMICS), for dealing with various video rain circumstances.
In particular, to mitigate the problem that existing methods
cannot cover various rain streaks distribution, we first design
a hyper-parameter optimization model about task variable and
hyper-parameter. Based on the proposed optimization model,
we design a collaborative structure for video deraining. This
structure includes Dominant Network Architecture (DNA) and
Companionate Network Architecture (CNA) that is cooperated
by introducing an Attention-based Averaging Scheme (AAS). To
better explore inter-frame information from videos, we introduce
a macroscopic structure searching scheme that searches from
Optical Flow Module (OFM) and Temporal Grouping Module
(TGM) to help restore latent frame. In addition, we apply
the differentiable neural architecture searching from a compact
candidate set of task-specific operations to discover desirable rain
streaks removal architectures automatically. Extensive experiments on various datasets demonstrate that our model shows
significant improvements in fidelity and temporal consistency
over the state-of-the-art works.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">@ARTICLE{Mu_TIP_2021,<br>
  author={Mu, Pan and Liu, Zhu and Liu, Yaohua and Liu, Risheng and Fan, Xin},<br>
  journal={IEEE Transactions on Image Processing}, <br>
  title={Triple-level Model Inferred Collaborative Network Architecture for Video Deraining}, <br>
  year={2021},<br>
  doi={10.1109/TIP.2021.3128327}}
  </p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss15&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/long_TNNLS2.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Underexposed Image Correction via Hybrid Priors Navigated Deep Propagation.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Long Ma, Jiaao Zhang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong><b> [<a href="https://arxiv.org/pdf/1907.07408">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss15" name="111ss15" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Enhancing visual qualities for underexposed imagesis  an  extensively  concerned  task  that  plays  important  roles  invarious areas of multimedia and computer vision. Most existingmethods  often  fail  to  generate  high-quality  results  with  appro-priate  luminance  and  abundant  details.  To  address  these  issues,we  in  this  work  develop  a  novel  framework,  integrating  bothknowledge  from  physical  principles  and  implicit  distributionsfrom  data  to  solve  the  underexposed  image  correction  task.More  concretely,  we  propose  a  new  perspective  to  formulatethis  task  as  an  energy-inspired  model  with  advanced  hybridpriors. A propagation procedure navigated by the hybrid priorsis  well  designed  for  simultaneously  propagating  the  reflectanceand  illumination  toward  desired  results.  We  conduct  extensiveexperiments to verify the necessity of integrating both underlyingprinciples (i.e., with knowledge) and distributions (i.e., from data)as navigated deep propagation. Plenty of experimental results ofunderexposed  image  correction  demonstrate  that  our  proposedmethod  performs  favorably  against  the  state-of-the-art  methodson  both  subjective  and  objective  assessments.  Additionally,  weexecute the task of face detection to further verify the naturalnessand  practical  value  of  underexposed  image  correction.  What'smore, we employ our method to single image haze removal whoseexperimental  results  further  demonstrate  its  superiorities.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021underexposed,<br>
  title={Underexposed Image Correction via Hybrid Priors Navigated Deep Propagation},<br>
  author={Liu, Risheng and Ma, Long and Zhang, Yuxi and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Neural Networks and Learning Systems},<br>
  year={2021},<br>
  publisher={IEEE}<br>
}
</p> </div>

    <div class="publication_links">  </div>

  </div>

</td>
</tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss6&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/long_TNNLS.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning Deep Context-Sensitive Decomposition for Low-Light Image Enhancement.</b><br>

      <p style="line-height:2em">Long Ma, <b>Risheng Liu*</b>, Jiaao Zhang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9420270">Paper</a>|<a href="https://github.com/KarelZhang/CSDNet-CSDGAN">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss6" name="111ss6" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Enhancing  the  quality  of  low-light  (LOL)  imagesplays  a   very   important  role  in  many   image   processing  andmultimedia applications. In recent years, a variety of deep learn-ing  techniques  have  been  developed  to  address  this  challengingtask.  A  typical  framework  is  tosimultaneously  estimate  theillumination  and  reflectance,  but  they  disregard  the  scene-levelcontextual  information  encapsulated  in  feature  spaces,  causingmany  unfavorable  outcomes,  e.g.,  details  loss,  color  unsatura-tion,  and  artifacts.  To  address  these  issues,  we  develop  a  newcontext-sensitive  decomposition  network  (CSDNet)  architectureto  exploit   the  scene-level   contextual   dependencies  on   spatialscales. More concretely, we build a two-stream estimation mech-anism including reflectance and illumination estimation network.We design a novel context-sensitive decomposition connection tobridge  the  two-stream  mechanism  by  incorporating  the  phys-ical  principle.  The  spatially  varying  illumination  guidance  isfurther  constructed  for  achieving  the  edge-aware  smoothnessproperty  of  the  illumination  component.  According  to  differenttraining patterns, we construct CSDNet (paired supervision) andcontext-sensitive  decomposition  generative  adversarial  network(CSDGAN) (unpaired supervision) to fully evaluate our designedarchitecture.  We  test  our  method  on  seven  testing  benchmarks[including  massachusetts  institute  of  technology  (MIT)-AdobeFiveK,  LOL,  ExDark,  and  naturalness  preserved  enhancement(NPE)] to conduct plenty of analytical and evaluated experiments.Thanks to our designed context-sensitive  decomposition connec-tion,  we  successfully  realized  excellent  enhanced  results  (withsufficient details,  vivid  colors,  and  few  noises),  which  fully indi-cates our superiority against existing state-of-the-art approaches.Finally,   considering   the   practical   needs   for   high   efficiency,we develop a lightweight CSDNet (named LiteCSDNet) by reduc-ing the number of channels. Furthermore, by sharing an encoderfor these two components, we obtain a more lightweight version(SLiteCSDNet  for  short).  SLiteCSDNet  just  contains  0.0301Mparameters  but  achieves  the  almost  same  performance  as  CSD-Net.Code  is  available  at  https://github.com/KarelZhang/CSDNet-CSDGAN.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{ma2021learning,<br>
  title={Learning deep context-sensitive decomposition for low-light image enhancement},<br>
  author={Ma, Long and Liu, Risheng and Zhang, Jiaao and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2021},<br>
  publisher={IEEE}<br>
}</p> </div>

    <div class="publication_links">  </div>

  </div>

</td>
</tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss20&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/jinyuan_TCSVT.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning a Deep Multi-scale Feature Ensemble and an Edge-attention Guidance for Image Fusion.</b><br>

      <p style="line-height:2em">Jinyuan Liu, Xin Fan, Ji Jiang, <b>Risheng Liu</b>, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TCSVT</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9349250">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss20" name="111ss20" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Image fusion integrates a series of images acquiredfrom   different   sensors,e.g.,   infrared   and   visible,   outputtingan  image  with  richer  information  than  either  one.  Traditionaland  recent  deep-based  methods  have  difficulties  in  preservingprominent  structures  and  recovering  vital  textural  details  forpractical applications. In this paper, we propose a deep networkfor infrared and visible image fusion cascading a feature learningmodule  with  a  fusion  learning  mechanism.  Firstly,  we  applya  coarse-to-fine  deep  architecture  to  learn  multi-scale  featuresfor  multi-modal  images,  which  enables  discovering  prominentcommon  structures  for  later  fusion  operations.  The  proposedfeature learning module requires no well-aligned image pairs fortraining.  Compared  with  the  existing  learning-based  methods,the  proposed  feature  learning  module  can  ensemble  numerousexamples  from  respective  modals  for  training,  increasing  theability  of  feature  representation.  Secondly,  we  design  an  edge-guided  attention  mechanism  upon  the  multi-scale  features  toguide the fusion focusing on common structures, thus recoveringdetails  while  attenuating  noise.  Moreover,  we  provide  a  newaligned  infrared  and  visible  image  fusion  dataset,  RealStreet,collected  in  various  practical  scenarios  for  comprehensive  eval-uation.  Extensive  experiments  on  two  benchmarks,  TNO  andRealStreet, demonstrate the superiority of the proposed methodover  the  state-of-the-art  in  terms  of  both  visual  inspection  andobjective analysis on six evaluation metrics. We also conduct theexperiments  on  the  FLIR  and  NIR  datasets,  containing  foggyweather  and  poor  light  conditions,  to  verify  the  generalizationand robustness of the proposed method.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021learning,<br>
  title={Learning a deep multi-scale feature ensemble and an edge-attention guidance for image fusion},<br>
  author={Liu, Jinyuan and Fan, Xin and Jiang, Ji and Liu, Risheng and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Circuits and Systems for Video Technology},<br>
  year={2021},<br>
  publisher={IEEE}<br>
}
</p> </div>

    <div class="publication_links">  </div>

  </div>

</td>
</tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss14&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/long_CVPR.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Retinex-Inspired Unrolling With Cooperative Prior Architecture Search for Low-Light Image Enhancement.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Long Ma, Jiaao Zhang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>CVPR</strong></span></strong><b> [<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Retinex-Inspired_Unrolling_With_Cooperative_Prior_Architecture_Search_for_Low-Light_Image_CVPR_2021_paper.html">Paper</a>|<a href="http://dutmedia.org/RUAS/">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss14" name="111ss14" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Low-light image enhancement plays very important rolesin low-level vision areas. Recent works have built a greatdeal of deep learning models to address this task. Howev-er, these approaches mostly rely on significant architectureengineering and suffer from high computational burden.In this paper, we propose a new method, named Retinex-inspired Unrolling with Architecture Search (RUAS), to con-struct lightweight yet effective enhancement network forlow-light images in real-world scenario. Specifically, build-ing upon Retinex rule, RUAS first establishes models tocharacterize the intrinsic underexposed structure of low-light images and unroll their optimization processes to con-struct our holistic propagation structure. Then by design-ing a cooperative reference-free learning strategy to dis-cover low-light prior architectures from a compact searchspace, RUAS is able to obtain a top-performing image en-hancement network, which is with fast speed and requiresfew computational resources. Extensive experiments veri-fy the superiority of our RUAS framework against recent-ly proposed state-of-the-art methods. The project page isavailable at http://dutmedia.org/RUAS/.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{liu2021retinex,<br>
  title={Retinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement},<br>
  author={Liu, Risheng and Ma, Long and Zhang, Jiaao and Fan, Xin and Luo, Zhongxuan},<br>
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>
  pages={10561--10570},<br>
  year={2021}<br>
}</p> </div>

    <div class="publication_links">  </div>

  </div>

</td>
</tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss16&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/long_ACM.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Bridging the Gap between Low-Light Scenes: Bilevel Learning for Fast Adaptation.</b><br>

      <p style="line-height:2em">Dian Jin, Long Ma, <b>Risheng Liu*</b>, Xin Fan<br>
	  <strong><span style="font-size: 12px;">
	  <strong>ACM MM</strong></span></strong><b> 
	   [<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475404?casa_token=GJC2_EvYXPMAAAAA:l-Q7Ilk6vAbAXWN_VnwNFWJ8h-HN6E2XsRuGshcW7FkJifNb1MbKn5emQGZ12xpqimgXIh_EkSDn0fs">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss16" name="111ss16" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Brightening low-light images of diverse scenes is a challenging but
widely concerned task in the multimedia community. Convolutional
Neural Networks (CNNs) based approaches mostly acquire the enhanced model by learning the data distribution from the specific
scenes. However, these works present poor adaptability (even fail)
when meeting real-world scenarios that never encountered before.
To conquer it, we develop a novel bilevel learning scheme for fast
adaptation to bridge the gap between low-light scenes. Concretely,
we construct a Retinex-induced encoder-decoder with an adaptive denoising mechanism, aiming at covering more practical cases.
Different from existing works that directly learn model parameters by using the massive data, we provide a new hyperparameter
optimization perspective to formulate a bilevel learning scheme
towards general low-light scenarios. This scheme depicts the latent
correspondence (i.e., scene-irrelevant encoder) and the respective
characteristic (i.e., scene-specific decoder) among different data distributions. Due to the expensive inner optimization, estimating the
hyper-parameter gradient exactly can be prohibitive, we develop an approximate hyper-parameter gradient method by introducing the
one-step forward approximation and finite difference approximation to ensure the high-efficient inference. Extensive experiments
are conducted to reveal our superiority against other state-of-theart methods. A series of analytical experiments are also executed
to verify our effectiveness.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{jin2021bridging,<br>
  title={Bridging the Gap between Low-Light Scenes: Bilevel Learning for Fast Adaptation},<br>
  author={Jin, Dian and Ma, Long and Liu, Risheng and Fan, Xin},<br>
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},<br>
  pages={2401--2409},<br>
  year={2021}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss2&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/mm_1.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Searching a Hierarchically
	Aggregated Fusion Architecture for
Fast Multi-Modality Image Fusion.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhu Liu, Jinyuan Liu, Xin Fan<br>
	  <strong><span style="font-size: 12px;">
	  <strong>ACM MM</strong></span></strong><b> 
	   [<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475299">Paper</a>|<a href="https://github.com/LiuzhuForFun/Hierarchical-NAS-Image-Fusion">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss2" name="111ss2" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Multi-modality image fusion refers to generating a complementary
image that integrates typical characteristics from source images. In
recent years, we have witnessed the remarkable progress of deep
learning models for multi-modality fusion. Existing CNN-based
approaches strain every nerve to design various architectures for
realizing these tasks in an end-to-end manner. However, these handcrafted designs are unable to cope with the high demanding fusion
tasks, resulting in blurred targets and lost textural details. To alleviate these issues, in this paper, we propose a novel approach,
aiming at searching effective architectures according to various
modality principles and fusion mechanisms. Specifically, we construct a hierarchically aggregated fusion architecture to extract
and refine fused features from feature-level and object-level fusion
perspectives, which is responsible for obtaining complementary
target/detail representations. Then by investigating diverse effective practices, we composite a more flexible fusion-specific search
space. Motivated by the collaborative principle, we employ a new
search strategy with different principled losses and hardware constraints for sufficient discovery of components. As a result, we can
obtain a task-specific architecture with fast inference time. Extensive quantitative and qualitative results demonstrate the superiority
and versatility of our method against state-of-the-art methods.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">@inproceedings{liu2021searching,<br>
  title={Searching a Hierarchically Aggregated Fusion Architecture for Fast Multi-Modality Image Fusion},<br>
  author={Liu, Risheng and Liu, Zhu and Liu, Jinyuan and Fan, Xin},<br>
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},<br>
  pages={1600--1608},<br>
  year={2021}<br>
}</p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss11&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/yaohua_NeurIPS.png " width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Towards Gradient-based Bilevel Optimization with non-convex Followers and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Yaohua Liu, Shanghai Zeng, Jin Zhang<br><strong><span style="font-size: 12px;"><strong>NeurIPS <b style="color:red">(Spotlight, Acceptance Rate &le; 3%)</b></strong></span></strong><b> [<a href="https://arxiv.org/pdf/2110.00455.pdf">Paper</a>|<a href="https://github.com/vis-opt-group/IAPTT-GM">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss11" name="111ss11" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In recent years, Bi-Level Optimization (BLO) techniques have received extensive attentions from both learning and vision communities. A variety of BLO models in complex and practical tasks are of non-convex follower structure in nature (a.k.a., without Lower-Level Convexity, LLC for short). However, this challenging class of BLOs is lack of developments on both efficient solution strategies and solid theoretical guarantees. In this work, we propose a new algorithmic framework, named Initialization Auxiliary and Pessimistic Trajectory Truncated Gradient Method (IAPTT-GM), to partially address the above issues. In particular, by introducing an auxiliary as initialization to guide the optimization dynamics and designing a pessimistic trajectory truncation operation, we construct a reliable approximate version of the original BLO in the absence of LLC hypothesis. Our theoretical investigations establish the convergence of solutions returned by IAPTT-GM towards those of the original BLO without LLC. As an additional bonus, we also theoretically justify the quality of our IAPTT-GM embedded with Nesterov's accelerated dynamics under LLC. The experimental results confirm both the convergence of our algorithm without LLC, and the theoretical findings under LLC.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021towards,<br>  title={Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond},<br>  author={Liu, Risheng and Liu, Yaohua and Zeng, Shangzhi and Zhang, Jin},<br>  journal={Advances in Neural Information Processing Systems},<br>  year={2021}<br>}
</p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss12&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/yaohua_ICMEW.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">BOML: A Modularized Bilevel Optimization Library in Python for Meta Learning.</b><br>

      <p style="line-height:2em">Yaohua Liu, <B>Risheng Liu*</b><br><strong><span style="font-size: 12px;"><strong>ICME <b style="color:red">(Best Open Source Project Award)</b></strong></span></strong><b> [<a href="https://arxiv.org/pdf/2009.13357.pdf">Paper</a>|<a href="https://github.com/vis-opt-group/BOML">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss12" name="111ss12" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Meta-learning (a.k.a. learning to learn) has recently emerged as a promising paradigm for a variety of applications. There are now many meta-learning methods, each focusing on different modeling aspects of base and meta learners, but all can be (re)formulated as specific bilevel optimization problems. This work presents BOML, a modularized optimization library that unifies several meta-learning algorithms into a common bilevel optimization framework. It provides a hierarchical optimization pipeline together with a variety of iteration modules, which can be used to solve the mainstream categories of meta-learning methods, such as meta-feature-based and meta-initialization-based formulations. The library is written in Python and is available at https://github.com/dut-media-lab/BOML.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{liu2021boml,<br>  title={BOML: A Modularized Bilevel Optimization Library in Python for Meta Learning},<br>  author={Liu, Yaohua and Liu, Risheng},<br>  booktitle={2021 IEEE International Conference on Multimedia \& Expo Workshops (ICMEW)},<br>  year={2021},<br>  organization={IEEE}<br>}
</p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<h2>2020</h2>
<div class="publication_container" onclick="toggleDetails(&#39;111ss21&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/jinyuan_TIP.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Bilevel Integrated Model with Data-driven Layer Ensemble for Multi-modality Image Fusion.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Jinyuan Liu, Zhiying Jiang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TIP</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9293146">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss21" name="111ss21" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Image  fusion  plays  a  critical  role  in  a  variety  ofvision  and learning applications. Current fusion approaches  aredesigned  to  characterize  source  images,  focusing  on  a  certaintype  of  fusion  task  while  limited  in  a  wide  scenario.  More-over,  other  fusion  strategies  (i.e.,  weighted  averaging,  choose-max)   cannot   undertake   the   challenging   fusion   tasks,   whichfurthermore  leads  to  undesirable  artifacts  facilely  emerged  intheir  fused  results.  In  this  paper,  we  propose  a  generic  imagefusion  method  with  a  bilevel  optimization  paradigm,  targetingon multi-modality image fusion tasks. Corresponding alternationoptimization is conducted on certain components decoupled fromsource  images.  Via  adaptive  integration  weight  maps,  we  areable  to  get  the  flexible  fusion  strategy  across  multi-modalityimages. We successfully applied it to three types of image fusiontasks, including infrared and visible, computed tomography andmagnetic  resonance  imaging,  and  magnetic  resonance  imagingand single-photon emission computed tomography image fusion.Results highlight the performance and versatility of our approachfrom both  quantitative and  qualitative aspects.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020bilevel,<br>
  title={A bilevel integrated model with data-driven layer ensemble for multi-modality image fusion},<br>
  author={Liu, Risheng and Liu, Jinyuan and Jiang, Zhiying and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Image Processing},<br>
  volume={30},<br>
  pages={1261--1274},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss22&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TMI.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Deep Framework Assembling Principled Modules for CS-MRI: Unrolling Perspective, Convergence Behaviors, and Practical Modeling.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Yuxi Zhang, Shichao Cheng, Zhongxuan Luo, Xin Fan<br><strong><span style="font-size: 12px;"><strong>IEEE TMI</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9157934">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss22" name="111ss22" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Compressed  Sensing  Magnetic  ResonanceImaging (CS-MRI) significantly accelerates MR acquisitionat  a  sampling  rate  much  lower  than  the  Nyquist  crite-rion.  A  major  challenge  for  CS-MRI  lies  in  solving  theseverely ill-posed inverse problem to reconstruct aliasing-free MR images from the sparsek-space data. Conventionalmethods typically optimize an energy function, producingrestoration  of  high  quality,  but  their  iterative  numericalsolvers unavoidably bring extremely large time consump-tion.   Recent   deep   techniques  provide   fast   restorationby  either  learning  direct  prediction  to  final  reconstruc-tion or plugging learned modules into the energy optimizer.Nevertheless,  these  data-driven  predictors  cannot  guar-antee  the  reconstruction following  principled  constraintsunderlying  the  domain  knowledge  so  that  the  reliabilityof  their  reconstruction  process  is  questionable.  In  thispaper, we propose a deep framework assembling principledmodules for CS-MRI that fuses learning strategy with theiterative  solver  of  a  conventional  reconstruction  energy.This  framework  embeds  an  optimal  condition  checkingmechanism, fosteringefficientandreliablereconstruction.We also apply the framework to three practical tasks,i.e., complex-valued data reconstruction, parallel imaging andreconstruction with Rician noise. Extensive experiments onboth benchmark and manufacturer-testing images demon-strate that the proposed method reliably converges to theoptimal  solution more  efficiently and  accurately than  thestate-of-the-art in various scenarios.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020deep,<br>
  title={A Deep Framework Assembling Principled Modules for CS-MRI: Unrolling Perspective, Convergence Behaviors, and Practical Modeling},<br>
  author={Liu, Risheng and Zhang, Yuxi and Cheng, Shichao and Luo, Zhongxuan and Fan, Xin},<br>
  journal={IEEE Transactions on Medical Imaging},<br>
  volume={39},<br>
  number={12},<br>
  pages={4150--4163},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss23&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TNNLS.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Location-aware and Regularization-adaptive Correlation Filters for Robust Visual Tracking.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Qianru Chen, Yuansheng Yao, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9158561">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss23" name="111ss23" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Correlation filter (CF) has recently been widely usedfor visual tracking. The estimation of the search window and thefilter-learning strategies is the key component of the CF trackers.Nevertheless, prevalent CF models separately address these issuesin  heuristic  manners.  The  commonly  used  CF  models  directlyset  the  estimated  location  in  the  previous  frame  as  the  searchcenter  for  the  current  one.  Moreover,  these  models  usually  relyon  simple  and  fixed  regularization  for  filter  learning,  and  thus,their performance is compromised by the search window size andoptimization heuristics. To break these limits, this article proposesa  location-aware  and  regularization-adaptive  CF  (LRCF)  forrobust  visual  tracking.  LRCF  establishes  a  novel  bilevel  opti-mization model to address simultaneously the location-estimationand   filter-training  problems.  We   prove   that   our   bilevel  for-mulation  can  successfully  obtain  a  globally  converged  CF  andthe  corresponding  object  location  in  a  collaborative  manner.Moreover, based on the LRCF framework, we design two trackersnamed  LRCF-S  and  LRCF-SA  and  a  series  of  comparisons  toprove  the  flexibility  and  effectiveness  of  the  LRCF  framework.Extensive  experiments  on  different challenging benchmark datasets  demonstrate  that  our  LRCF  trackers  perform  favorablyagainst  the state-of-the-art  methods in practice.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020location,<br>
  title={Location-aware and regularization-adaptive correlation filters for robust visual tracking},<br>
  author={Liu, Risheng and Chen, Qianru and Yao, Yuansheng and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={32},<br>
  number={6},<br>
  pages={2430--2442},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss24&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/mu_TIP.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Investigating Task-driven Latent Feasibility for Nonconvex Image Modeling.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Pan Mu, Jian Chen, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TIP</strong></span></strong><b> [<a href="https://arxiv.org/pdf/1910.08242">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss24" name="111ss24" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Properly  modeling  latent  image  distributions  playsan important role in a variety of image-related vision problems.Most   exiting   approaches   aim   to   formulate   this   problem   asoptimization  models  (e.g.,  Maximum  A  Posterior,  MAP)  withhandcrafted  priors.  In  recent  years,  different  CNN  modules  arealso  considered  as  deep  priors  to  regularize  the  image  model-ing  process.  However,  these  explicit  regularization  techniquesrequire  deep  understandings  on  the  problem  and  elaboratelymathematical skills. In this work, we provide a new perspective,named   Task-driven   Latent   Feasibility   (TLF),   to   incorporatespecific  task  information  to  narrow  down  the  solution  spacefor  the  optimization-based  image  modeling  problem.  Thanks  tothe  flexibility  of  TLF,  both  designed  and  trained  constraintscan  be  embedded  into  the  optimization  process.  By  introducingcontrol mechanisms based on the monotonicity and boundednessconditions,  we  can  also  strictly  prove  the  convergence  of  ourproposed inference process. We demonstrate that different typesof image modeling problems, such as image deblurring and rainstreaks removals, can all be appropriately addressed within ourTLF framework. Extensive experiments also verify the theoreticalresults  and  show  the  advantages  of  our  method  against  existingstate-of-the-art  approaches.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020investigating,<br>
  title={Investigating task-driven latent feasibility for nonconvex image modeling},<br>
  author={Liu, Risheng and Mu, Pan and Chen, Jian and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Image Processing},<br>
  volume={29},<br>
  pages={7629--7640},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss25&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TCSVT20.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning Hadamard-Product-Propagation for Image Dehazing and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shiqi Li, Jinyuan Liu, Long Ma, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TCSVT</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9125952">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss25" name="111ss25" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Image   dehazing   has   evolved   into   an   attractiveresearch  field  in  the  computer  vision  community  in  the  pastfew  decades.  Previous  traditional  approaches  attempt  to  designenergy-based  objective  functions.  However,  they  cannot  accu-rately  express  the  intrinsic characteristics  of  the  images,  posingweak  adaptation  ability for  real-world  complex  scenarios.  Morerecently,   deep   learning   techniques   for   image   dehazing   havematured and become more reliable, showing outstanding perfor-mance.  Nevertheless,  these  methods  heavily  depend  on  trainingdata, restricting their application ranges. More importantly, bothtraditional  and  deep  learning  approaches  all  ignore  a  commonissue,  noises/artifacts  always  appear  in  the  recovery  process.To  this end, a  new Hadamard-Product (HP) model is proposed,which  consists  of  a  series  of  data-driven  priors.  Based  on  thismodel,  we  derive  a  Learnable  Hadamard-Product-Propagation(LHPP)  by  cascading  a  series  of  principle-inspired  guidanceand recovery modules. In which, the principle-inspired guidancerelated  to  transmission  is  endowed  the  smoothness  property,the  other  recovery  module  satisfies  the  distribution  of  naturalimages. The Hadamard-product-based propagations is generatedin  our  developed  learnable  framework  for  the  task  of  imagedehazing.  In  this  way,  we  can  eliminate  noises/artifacts  in  therecovery  procedure  to  obtain  the  ideal  outputs.  Subsequently,since the generality of our HP model, we successfully extend ourLHPP  to  settle  low-light  image  enhancement  and  underwaterimage enhancement problems. A series of analytical experimentsare performed to verify our effectiveness. Plenty of performanceevaluations  on  three  complex  tasks  fully  reveal  our  superiorityagainst  multiple state-of-the-art  methods.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020learning,<br>
  title={Learning Hadamard-Product-Propagation for Image Dehazing and Beyond},<br>
  author={Liu, Risheng and Li, Shiqi and Liu, Jinyuan and Ma, Long and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Circuits and Systems for Video Technology},<br>
  volume={31},<br>
  number={4},<br>
  pages={1366--1379},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss26&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TMM.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Dual Neural Networks Coupling Data Regression with Explicit Priors for Monocular 3D Face Reconstruction.</b><br>

      <p style="line-height:2em">Xin Fan, Shichao Cheng, Kang Huyan, Minjun Hou, <b>Risheng Liu</b>, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TMM</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9093203">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss26" name="111ss26" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">We address the challenging issue of reconstructinga  3D  face  from  one  single  image  under  various  expressionsand illuminations, which is widely applied in multimedia tasks.Methods   built   upon   classical   parametric   morphable   models(3DMMs) gain success on reconstructing the global geometry ofa  3D  face,  but  fail  to  precisely  characterize  local  facial  details.Recently,  deep  neural  networks  (DNN)  have  been  applied  tothe  reconstruction  that  directly  predicts  depth  maps,  showingcompelling performance on detail recovery. Unfortunately, theirreconstruction  is  prone  to  structural  distortions  owing  to  thelack of explicit prior constraints. In this paper, we propose dualneural  networks  that  optimize  one  energy  coupling  data  fittingwith  local  explicit  geometric  prior.  Specifically,  we  build  oneresidual network upon traditional convolution layers in order todirectly predict 3D structures by fitting an input image. Meanwhile,we  devise  a  novel  architecture  stacking  shallow  networks  torefine 3D clouds with geometric priors given by Markov randomfields (MRFs). Quantitative evaluations demonstrate the superiorperformance of the dual networks over either end-to-end DNNsor parametric models. Comparisons with the state-of-the-art alsoshow competitive reconstruction quality on various conditions.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{fan2020dual,<br>
  title={Dual neural networks coupling data regression with explicit priors for monocular 3D face reconstruction},<br>
  author={Fan, Xin and Cheng, Shichao and Huyan, Kang and Hou, Minjun and Liu, Risheng and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Multimedia},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss27&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TCSVT20_2.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Real-world Underwater Enhancement: Challenges, Benchmarks, and Solutions under Natural Light.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xin Fan, Ming Zhu, Minjun Hou, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TCSVT</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8949763">Paper</a>|<a href="https://github.com/dlut-dimt/Realworld-Underwater-Image-Enhancement-RUIE-Benchmark">Data</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss27" name="111ss27" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Underwater  image  enhancement  is  such  an  impor-tant low-level vision task with many applications that numerousalgorithms have been proposed in recent years. These algorithmsdeveloped upon various assumptions demonstrate successes fromvarious  aspects  usingdifferentdata  sets  anddifferentmetrics.In   this   work,   we   setup   an   undersea   image   capturing   sys-tem,  and  construct  a  large-scaleReal-world  Underwater  ImageEnhancement(RUIE)  data  set  divided  into  three  subsets.  Thethree  subsets  target  at  three  challenging  aspects  for  enhance-ment,  i.e.,  image  visibility  quality,  color  casts,  and  higher-leveldetection/classification,  respectively.  We  conduct  extensive  andsystematic experiments on RUIE to evaluate the effectiveness andlimitations of various algorithms to enhance visibility and correctcolor casts on images with hierarchical categories of degradation.Moreover,  underwater  image  enhancement  in  practice  usuallyserves as a preprocessing step for mid-level and high-level visiontasks.   We   thus   exploit   the   object   detection   performance   onenhanced  images  as  a  brand  newtask-specificevaluation  crite-rion. The findings from these evaluations not only confirm whatis  commonly  believed,  but  also  suggest  promising  solutions  andnew  directions  for  visibility  enhancement,  color  correction,  andobject  detection  on  real-world  underwater  images.  The  bench-mark   is   available   at:   https://github.com/dlut-dimt/Realworld-Underwater-Image-Enhancement-RUIE-Benchmark.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020real,<br>
  title={Real-world underwater enhancement: Challenges, benchmarks, and solutions under natural light},<br>
  author={Liu, Risheng and Fan, Xin and Zhu, Ming and Hou, Minjun and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Circuits and Systems for Video Technology},<br>
  volume={30},<br>
  number={12},<br>
  pages={4861--4875},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss28&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TNNLS20_2.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Knowledge-driven Deep Unrolling for Robust Image Layer Separation.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhiying Jiang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8760253">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss28" name="111ss28" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Single-image layer separation targets to decomposethe observed image into two independent components in terms ofdifferent application demands. It is known that many vision andmultimedia  applications  can  be  (re)formulated  as  a  separationproblem.  Due  to  the  fundamentally  ill-posed  natural  of  theseseparations,  existing  methods  are  inclined  to  investigate  modelpriors  on  the  separated  components  elaborately.  Nevertheless,it is knotty to optimize the cost function with complicated modelregularizations.  Effectiveness  is  greatly  conceded  by  the  settlediteration mechanism, and the adaption cannot be guaranteed dueto the poor data fitting. What is more, for a universal framework,the  most  taxing  point  is  that  one  type  of  visual  cue  cannot  beshared  with  different  tasks.  To  partly  overcome  the  weaknessesmentioned earlier, we delve into a generic optimization unrollingtechnique  to  incorporate  deep  architectures  into  iterations  foradaptive  image  layer  separation.  First,  we  propose  a  generalenergy model with implicit priors, which is based on maximum aposterior, and employ the extensively accepted alternating direc-tion method of  multiplier to determine our elementary iterationmechanism. By  unrolling with one general  residual architectureprior  and  one  task-specific  prior,  we  attain  a  straightforward,flexible,  and  data-dependent  image  separation  framework  suc-cessfully. We apply our method to four different tasks, includingsingle-image-rain streak removal, high-dynamic-range tone map-ping,  low-light  image  enhancement,  and  single-image  reflectionremoval.  Extensive  experiments  demonstrate  that  the  proposedmethod is applicable to multipletasks and outperforms the stateof the arts by a  large  margin  qualitatively and quantitatively.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2019knowledge,<br>
  title={Knowledge-driven deep unrolling for robust image layer separation},<br>
  author={Liu, Risheng and Jiang, Zhiying and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE transactions on neural networks and learning systems},<br>
  volume={31},<br>
  number={5},<br>
  pages={1653--1666},<br>
  year={2019},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss9&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/xuan_BVFIM.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang<br><strong><span style="font-size: 12px;"><strong>ICML</strong></span></strong><b> [<a href="https://proceedings.mlr.press/v139/liu21o.html">Paper</a>|<a href="https://github.com/vis-opt-group/BVFSM">Code</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss9" name="111ss9" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Bi-level optimization model is able to capture a wide range of complex learning tasks with practical interest. Due to the witnessed efficiency in solving bi-level programs, gradient-based methods have gained popularity in the machine learning community. In this work, we propose a new gradient-based solution scheme, namely, the Bi-level Value-Function-based Interior-point Method (BVFIM). Following the main idea of the log-barrier interior-point scheme, we penalize the regularized value function of the lower level problem into the upper level objective. By further solving a sequence of differentiable unconstrained approximation problems, we consequently derive a sequential programming scheme. The numerical advantage of our scheme relies on the fact that, when gradient methods are applied to solve the approximation problem, we successfully avoid computing any expensive Hessian-vector or Jacobian-vector product. We prove the convergence without requiring any convexity assumption on either the upper level or the lower level objective. Experiments demonstrate the efficiency of the proposed BVFIM on non-convex bi-level problems.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@InProceedings{pmlr-v139-liu21o,<br>
title = {A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization},<br>
author = {Liu, Risheng and Liu, Xuan and Yuan, Xiaoming and Zeng, Shangzhi and Zhang, Jin},<br>
booktitle = {Proceedings of the 38th International Conference on Machine Learning},<br>
pages = {6882--6892},<br>
year = {2021},<br>
editor = {Meila, Marina and Zhang, Tong},<br>
volume = {139},<br>
series = {Proceedings of Machine Learning Research},<br>
month = {18--24 Jul},<br>
publisher = {PMLR},<br>
pdf = {http://proceedings.mlr.press/v139/liu21o/liu21o.pdf},<br>
url = {https://proceedings.mlr.press/v139/liu21o.html},<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss10&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/lizi_IJCAI.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px"> Bi-level Probabilistic Feature Learning for Deformable Image Registration.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zi Li, Yuxi Zhang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IJCAI</strong></span></strong><b> [<a href="https://www.ijcai.org/proceedings/2020/101">Paper</a>|<a href="https://github.com/Alison-brie/BiLevelReg">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss10" name="111ss10" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">We address the challenging issue of deformable registration that robustly and efficiently builds dense correspondences between images. Traditional approaches upon iterative energy optimization typically invoke expensive computational load. Recent learning-based methods are able to efficiently predict deformation maps by incorporating learnable deep networks. Unfortunately, these deep networks are designated to learn deterministic features for classification tasks, which are not necessarily optimal for registration. In this paper, we propose a novel bi-level optimization model that enables jointly learning deformation maps and features for image registration. The bi-level model takes the energy for deformation computation as the upper-level optimization while formulates the maximum a posterior (MAP) for features as the lower-level optimization. Further, we design learnable deep networks to simultaneously optimize the cooperative bi-level model, yielding robust and efficient registration. These deep networks derived from our bi-level optimization constitute an unsupervised end-to-end framework for learning both features and deformations. Extensive experiments of image-to-atlas and image-to-image deformable registration on 3D brain MR datasets demonstrate that we achieve state-of-the-art performance in terms of accuracy, efficiency, and robustness.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{LiuLZFL20,<br>
  	title={Bi-level Probabilistic Feature Learning for Deformable Image Registration},<br>
  	author={Risheng Liu, Zi Li, Yuxi Zhang, Xin Fan and Zhongxuan Luo},<br>
  	booktitle={International Joint inproceedings on Artificial Intelligence},<br>
  	year={2020},<br>
  	pages={723--730},<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss29&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_ICML20.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Generic First-Order Algorithmic Framework for Bi-Level Programming Beyond Lower-Level Singleton.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang<br><strong><span style="font-size: 12px;"><strong>ICML</strong></span></strong><b> [<a href="http://proceedings.mlr.press/v119/liu20l/liu20l.pdf">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss29" name="111ss29" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In recent years,  a variety of gradient-based bi-level optimization methods have been developedfor learning tasks.  However, theoretical guaran-tees of these existing approaches often heavily re-ly on the simplification that for each fixed upper-level variable, the lower-level solution must bea singleton (a.k.a., Lower-Level Singleton, LL-S). In this work, by formulating bi-level modelsfrom the optimistic viewpoint and aggregatinghierarchical objective information, we establishBi-level Descent Aggregation (BDA), a flexibleand modularized algorithmic framework for bi-level programming.  Theoretically, we derive anew methodology to prove the convergence ofBDA without the LLS condition.  Furthermore,we improve the convergence properties of conven-tional first-order bi-level schemes (under the LLSsimplification) based on our proof recipe. Exten-sive experiments justify our theoretical results anddemonstrate the superiority of the proposed BDAfor different tasks, including hyper-parameter op-timization and meta learning.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{liu2020generic,<br>
  title={A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton},<br>
  author={Liu, Risheng and Mu, Pan and Yuan, Xiaoming and Zeng, Shangzhi and Zhang, Jin},<br>
  booktitle={International Conference on Machine Learning},<br>
  pages={6305--6315},<br>
  year={2020},<br>
  organization={PMLR}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<h2>2019</h2>
<div class="publication_container" onclick="toggleDetails(&#39;111ss30&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TIP19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning converged propagations with deep prior ensemble for image enhancement.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Long Ma, Yi Wang, Lei Zhang<br><strong><span style="font-size: 12px;"><strong>IEEE TIP</strong></span></strong><b> [<a href="https://arxiv.org/pdf/1810.04012">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss30" name="111ss30" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Enhancing   visual   qualities   of   images   plays   veryimportant   roles   in   various   vision   and   learning   applications.In   the   past   few   years,   both   knowledge-driven   maximum   aposterior (MAP) with prior modelings and fully data-dependentconvolutional  neural  network  (CNN)  techniques  have  been  in-vestigated  to  address  specific  enhancement  tasks.  In  this  paper,by  exploiting  the  advantages  of  these  two  types  of  mechanismswithin  a  complementary  propagation  perspective,  we  proposea  unified  framework,  named  deep  prior  ensemble  (DPE),  forsolving  various  image  enhancement  tasks.  Specifically,  we  firstestablish the basic propagation scheme based on the fundamentalimage   modeling   cues   and   then   introduce   residual   CNNs   tohelp   predicting   the   propagation   direction   at   each   stage.   Bydesigning  prior  projections  to  perform  feedback  control,  wetheoretically  prove  that  even  with  experience-inspired  CNNs,DPE  is  definitely  converged  and  the  output  will  always  satisfyour  fundamental  task  constraints.  The  main  advantage  againstconventional  optimization-based  MAP  approaches  is  that  ourdescent directions are learned from collected training data, thusare  much  more  robust  to  unwanted  local  minimums.  While,compared with existing CNN type networks, which are often de-signed in heuristic manners without theoretical guarantees, DPEis able to gain advantages from rich task cues investigated on thebases of domain knowledges. Therefore, DPE actually provides ageneric  ensemble  methodology  to  integrate  both  knowledge  anddata-based  cues  for  different  image  enhancement  tasks.  Moreimportantly,  our  theoretical  investigations  verify  that  the  feed-forward  propagations  of  DPE  are  properly  controlled  towardour  desired  solution.  Experimental  results  demonstrate  that  theproposed DPE outperforms state-of-the-arts on a variety of imageenhancement  tasks  in  terms  of  both  quantitative  measure  andvisual  perception  quality.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2018learning,<br>
  title={Learning converged propagations with deep prior ensemble for image enhancement},<br>
  author={Liu, Risheng and Ma, Long and Wang, Yiyang and Zhang, Lei},<br>
  journal={IEEE Transactions on Image Processing},<br>
  volume={28},<br>
  number={3},<br>
  pages={1528--1543},<br>
  year={2018},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss31&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TIP19_2.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Deep proximal unrolling: Algorithmic framework, convergence analysis and applications.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shichao Cheng, Long Ma, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TIP</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8704990">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss31" name="111ss31" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Deep  learning  models  have  gained  great  successin  many  real-world  applications.  However,  most  existing  net-works  are  typically  designed  in  heuristic  manners,  thus  theseapproaches  lack  rigorous  mathematical  derivations  and  clearinterpretations.  Several  recent  studies  try  to  build  deep  modelsby  unrolling  a  particular  optimization  model  that  involves  taskinformation. Unfortunately, due to the dynamic nature of networkparameters, their resultant deep propagations do not possess thenice  convergence  property  as  the  original  optimization  schemedoes.  In  this  work,  we  develop  a  generic  paradigm  to  unrollnonconvex  optimization  for  deep  model  design.  Different  frommost  existing  frameworks,  which  just  replace  the  iterations  bynetwork  architectures,  we  prove  in  theory  that  the  propagationgenerated  by  our  proximally  unrolled  deep  model  can  globallyconverge to the critical-point of the original optimization model.Moreover, even if the task information is only partially available(e.g.,   no   prior   regularization),   we   can   still   train   convergentdeep  propagations.  We  also  extend  these  theoretical  investiga-tions  on  the  more  general  multi-block  models  and  thus  a  lotof  real-world  applications  can  be  successfully  handled  by  theproposed framework. Finally, we conduct experiments on variouslow-level vision tasks (i.e., non-blind deconvolution, dehazing, andlow-light image enhancement) and demonstrate the superiority ofour proposed framework, compared with existing state-of-the-artapproaches.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2019deep,<br>
  title={Deep proximal unrolling: Algorithmic framework, convergence analysis and applications},<br>
  author={Liu, Risheng and Cheng, Shichao and Ma, Long and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Image Processing},<br>
  volume={28},<br>
  number={10},<br>
  pages={5013--5026},<br>
  year={2019},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss33&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TPAMI19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">On the Convergence of Learning-based Iterative Methods for Nonconvex Inverse Problems.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shichao Cheng, Yi He, Xin Fan, Zhouchen Lin, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TPAMI</strong></span></strong><b> [<a href="https://arxiv.org/pdf/1808.05331">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss33" name="111ss33" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Numerous tasks at the core of statistics, learning and vision areas are specific cases of ill-posed inverse problems. Recently,learning-based (e.g., deep) iterative methods have been empirically shown to be useful for these problems. Nevertheless, integratinglearnable structures into iterations is still a laborious process, which can only be guided by intuitions or empirical insights. Moreover, thereis a lack of rigorous analysis about the convergence behaviors of these reimplemented iterations, and thus the significance of suchmethods is a little bit vague. This paper moves beyond these limits and proposes Flexible Iterative Modularization Algorithm (FIMA), ageneric and provable paradigm for nonconvex inverse problems. Our theoretical analysis reveals that FIMA allows us to generate globallyconvergent trajectories for learning-based iterative methods. Meanwhile, the devised scheduling policies on flexible modules should alsobe beneficial for classical numerical methods in the nonconvex scenario. Extensive experiments on real applications verify the superiorityof FIMA.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2019convergence,<br>
  title={On the convergence of learning-based iterative methods for nonconvex inverse problems},<br>
  author={Liu, Risheng and Cheng, Shichao and He, Yi and Fan, Xin and Lin, Zhouchen and Luo, Zhongxuan},<br>
  journal={IEEE transactions on pattern analysis and machine intelligence},<br>
  volume={42},<br>
  number={12},<br>
  pages={3027--3039},<br>
  year={2019},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss36&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TNNLS19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning Aggregated Transmission Propagation Networks for Haze Removal and Beyond.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xin Fan, Minjun Hou, Zhiying Jiang, Zhongxuan Luo, Lei Zhang<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong><b> [<a href="https://arxiv.org/pdf/1711.06787">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss36" name="111ss36" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Single  image  dehazing  is  an  important  low-levelvision task with many applications. Early researches have inves-tigated  different  kinds  of  visual  priors  to  address  this  problem.However,  they  may  fail  when  their  assumptions  are  not  validon specific images. Recent deep  networks also achieve relativelygood  performance  in  this  task.  But  unfortunately,  due  to  thedisappreciation of rich physical rules in hazes, large amounts ofdata are required for their training. More importantly, they maystill fail when there exist completely different haze distributionsin testing images. By considering the collaborations of these twoperspectives,  this  paper  designs  a  novel  residual  architecture  toaggregate both prior (i.e., domain knowledge) and data (i.e., hazedistribution) information to propagate transmissions for scene ra-diance estimation. We further present a variational energy basedperspective  to  investigate  the  intrinsic  propagation  behavior  ofour  aggregated  deep  model.  In  this  way,  we  actually  bridge  thegap between prior driven models and data driven networks andleverage  advantages  but  avoid  limitations  of  previous  dehazingapproaches.  A  lightweight  learning  framework  is  proposed  totrain  our  propagation  network.  Finally,  by  introducing  a  task-aware image separation formulation with a flexible optimizationscheme,  we  extend  the  proposed  model  for  more  challengingvision tasks, such as underwater image enhancement and singleimage  rain  removal.  Experiments  on  both  synthetic  and  real-world images demonstrate the effectiveness and efficiency of theproposed  framework.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2018learning,<br>
  title={Learning aggregated transmission propagation networks for haze removal and beyond},<br>
  author={Liu, Risheng and Fan, Xin and Hou, Minjun and Jiang, Zhiying and Luo, Zhongxuan and Zhang, Lei},<br>
  journal={IEEE transactions on neural networks and learning systems},<br>
  volume={30},<br>
  number={10},<br>
  pages={2973--2986},<br>
  year={2018},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss54&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/TCSVT19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Toward Efficient Image Representation: Sparse Concept Discriminant Matrix Factorization.
</b><br>

      <p style="line-height:2em">Meng Pang, Yiu-ming Cheung, <b>Risheng Liu</b>, Jian Lou, Chuang Lin<br><strong><span style="font-size: 12px;"><strong>IEEE TCSVT</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/abstract/document/8525292/">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss54" name="111ss54" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Abstract¡ª The key ingredients of matrix factorization lie in
basic learning and coefficient representation. To enhance the
discriminant ability of the learned basis, discriminant graph
embedding is usually introduced in the matrix factorization
model. However, the existing matrix factorization methods based
on graph embedding generally conduct discriminant analysis
via a single type of adjacency graph, either similarity-based
graphs (e.g., Laplacian eigenmaps graph) or reconstruction-based
graphs (e.g., L1-graph), while ignoring the cooperation of the
different types of adjacency graphs that can better depict the
discriminant structure of original data. To address the above
issue, we propose a novel Fisher-like criterion, based on graph
embedding, to extract sufficient discriminant information via two
different types of adjacency graphs. One graph preserves the
reconstruction relationships of neighboring samples in the same
category, and the other suppresses the similarity relationships of
neighboring samples from different categories. Moreover, we also
leverage the sparse coding to promote the sparsity of the coefficients. By virtue of the proposed Fisher-like criterion and sparse
coding, a new matrix factorization framework called Sparse
concept Discriminant Matrix Factorization (SDMF) is proposed
for efficient image representation. Furthermore, we extend the
Fisher-like criterion to an unsupervised context, thus yielding
an unsupervised version of SDMF. Experimental results on
seven benchmark datasets demonstrate the effectiveness and
efficiency of the proposed SDMFs on both image classification
and clustering tasks.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{pang2018toward,<br>
  title={Toward efficient image representation: Sparse concept discriminant matrix factorization},<br>
  author={Pang, Meng and Cheung, Yiu-Ming and Liu, Risheng and Lou, Jian and Lin, Chuang},<br>
  journal={IEEE Transactions on Circuits and Systems for Video Technology},<br>
  volume={29},<br>
  number={11},<br>
  pages={3184--3198},<br>
  year={2018},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss51&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/PR19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Robust Heterogeneous Discriminative Analysis for Face Recognition with Single Sample per Person.</b><br>

      <p style="line-height:2em">Meng Pang, Yiu-ming Cheung, Binghui Wang, <b>Risheng Liu</b><br><strong><span style="font-size: 12px;"><strong>Pattern Recognition</strong></span></strong><b> [<a href="https://www.comp.hkbu.edu.hk/~ymc/papers/journal/PR-yr2019.pdf">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss51" name="111ss51" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Single sample per person face recognition is one of the most challenging problems in face recognition
(FR), where only single sample per person (SSPP) is enrolled in the gallery set for training. Although the
existing patch-based methods have achieved great success in FR with SSPP, they still have limitations
in feature extraction and identification stages when handling complex facial variations. In this work, we
propose a new patch-based method called Robust Heterogeneous Discriminative Analysis (RHDA), for FR
with SSPP. To enhance the robustness against complex facial variations, we first present a new graphbased Fisher-like criterion, which incorporates two manifold embeddings, to learn heterogeneous discriminative representations of image patches. Specifically, for each patch, the Fisher-like criterion is able
to preserve the reconstruction relationship of neighboring patches from the same person, while suppressing the similarities between neighboring patches from the different persons. Then, we introduce
two distance metrics, i.e., patch-to-patch distance and patch-to-manifold distance, and develop a fusion
strategy to combine the recognition outputs of above two distance metrics via a joint majority voting for
identification. Experimental results on various benchmark datasets demonstrate the effectiveness of the
proposed method.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{pang2019robust,<br>
  title={Robust heterogeneous discriminative analysis for face recognition with single sample per person},<br>
  author={Pang, Meng and Cheung, Yiu-ming and Wang, Binghui and Liu, Risheng},<br>
  journal={Pattern Recognition},<br>
  volume={89},<br>
  pages={91--107},<br>
  year={2019},<br>
  publisher={Elsevier}<br>
}</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss60&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/ICCV19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Semi-supervised Skin Detection by Network with Mutual Guidances.</b><br>

      <p style="line-height:2em">Yi He, Jiayuan Shi, Chuan Wang, Haibin Huang, Jiaming Liu, Guanbin Li, <b>Risheng Liu</b>, Jue Wang<br><strong><span style="font-size: 12px;"><strong>ICCV</strong></span></strong><b> [<a href="http://openaccess.thecvf.com/content_ICCV_2019/html/He_Semi-Supervised_Skin_Detection_by_Network_With_Mutual_Guidance_ICCV_2019_paper.html">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss60" name="111ss60" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In this paper we present a new data-driven method for
robust skin detection from a single human portrait image.
Unlike previous methods, we incorporate human body as a
weak semantic guidance into this task, considering acquiring large-scale of human labeled skin data is commonly expensive and time-consuming. To be specific, we propose
a dual-task neural network for joint detection of skin and
body via a semi-supervised learning strategy. The dualtask network contains a shared encoder but two decoders
for skin and body separately. For each decoder, its output
also serves as a guidance for its counterpart, making both
decoders mutually guided. Extensive experiments were conducted to demonstrate the effectiveness of our network with
mutual guidance, and experimental results show our network outperforms the state-of-the-art in skin detection.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{wang2019asynchronous,<br>
  title={Asynchronous proximal stochastic gradient algorithm for composition optimization problems},<br>
  author={Wang, Pengfei and Liu, Risheng and Zheng, Nenggan and Gong, Zhefeng},<br>
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
  volume={33},<br>
  number={01},<br>
  pages={1633--1640},<br>
  year={2019}<br>
}</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss32&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_AAAI19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Task embedded coordinate update: A realizable framework for multivariate non-convex optimization.
</b><br>

      <p style="line-height:2em">Yiyang Wang, <b>Risheng Liu*</b>, Long Ma, Xiaoliang Song.<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong><b> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/3981/3859">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss32" name="111ss32" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">We in this paper propose a realizable framework TECU, whichembeds task-specific strategies into update schemes of coordi-nate descent, for optimizing multivariate non-convex problemswith coupled objective functions. On one hand, TECU is ca-pable of improving algorithm efficiencies through embeddingproductive numerical algorithms, for optimizing univariatesub-problems with nice properties. From the other side, it alsoaugments probabilities to receive desired results, by embed-ding advanced techniques in optimizations of realistic tasks. In-tegrating both numerical algorithms and advanced techniquestogether, TECU is proposed in a unified framework for solvinga class of non-convex problems. Although the task embeddedstrategies bring inaccuracies in sub-problem optimizations, weprovide a realizable criterion to control the errors, meanwhile,to ensure robust performances with rigid theoretical analyses.By respectively embedding ADMM and a residual-type CNNin our algorithm framework, the experimental results verifyboth efficiency and effectiveness of embedding task-orientedstrategies in coordinate descent for solving practical problems.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{wang2019task,<br>
  title={Task embedded coordinate update: A realizable framework for multivariate non-convex optimization},<br>
  author={Wang, Yiyang and Liu, Risheng and Ma, Long and Song, Xiaoliang},<br>
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
  volume={33},<br>
  number={01},<br>
  pages={1650--1657},<br>
  year={2019}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss34&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_AAAI19_2.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Theoretically Guaranteed Deep Optimization Framework for Robust Compressive Sensing MRI.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Yuxi Zhang, Shichao Cheng, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong><b> [<a href="https://ojs.aaai.org/index.php/AAAI/article/download/4347/4225">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss34" name="111ss34" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Magnetic Resonance Imaging (MRI) is one of the most dy-namic and safe imaging techniques available for clinical ap-plications.  However,  the  rather  slow  speed  of  MRI  acqui-sitions  limits  the  patient  throughput  and  potential  indica-tions.  Compressive  Sensing  (CS)  has  proven  to  be  an  effi-cient technique for accelerating MRI acquisition. The mostwidely used CS-MRI model, founded on the premise of re-constructing an image from an incompletely filled k-space,leads to an ill-posed inverse problem. In the past years, lotsof efforts have been made to efficiently optimize the CS-MRImodel. Inspired by deep learning techniques, some prelimi-nary works have tried to incorporate deep architectures intoCS-MRI process. Unfortunately, the convergence issues (dueto  the  experience-based  networks)  and  the  robustness  (i.e.,lack real-world noise modeling) of these deeply trained opti-mization methods are still missing. In this work, we developa new paradigm to integrate designed numerical solvers andthe data-driven architectures for CS-MRI. By introducing anoptimal condition checking mechanism, we can successfullyprove the convergence of our established deep CS-MRI op-timization scheme. Furthermore, we explicitly formulate theRician noise distributions within our framework and obtainan extended CS-MRI network to handle the real-world nosiesin the MRI process. Extensive experimental results verify thatthe proposed paradigm outperforms the existing state-of-the-art techniques both in reconstruction accuracy and efficiencyas well as robustness to noises in real scene.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{liu2019theoretically,<br>
  title={A theoretically guaranteed deep optimization framework for robust compressive sensing mri},<br>
  author={Liu, Risheng and Zhang, Yuxi and Cheng, Shichao and Fan, Xin and Luo, Zhongxuan},<br>
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
  volume={33},<br>
  number={01},<br>
  pages={4368--4375},<br>
  year={2019}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss35&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_AAAI19_3.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Exploiting Local Feature Patterns for Unsupervised Domain Adaptation.
</b><br>

      <p style="line-height:2em">Jun Wen, <b>Risheng Liu</b>, Nenggan Zheng, Qian Zheng, Zhefeng Gong, Junsong Yuan<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong><b> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/4479">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss35" name="111ss35" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Unsupervised domain adaptation methods aim to alleviate performance degradation caused by domain-shift by learning domain-invariant representations. Existing deep domain adaptation methods focus on holistic feature alignment by
matching source and target holistic feature distributions, without considering local features and their multi-mode statistics. We show that the learned local feature patterns are more generic and transferable and a further local feature distribution matching enables fine-grained feature alignment. In this paper, we present a method for learning domain-invariant
local feature patterns and jointly aligning holistic and local feature statistics. Comparisons to the state-of-the-art unsupervised domain adaptation methods on two popular benchmark datasets demonstrate the superiority of our approach and its effectiveness on alleviating negative transfer.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{wen2019exploiting,<br>
  title={Exploiting local feature patterns for unsupervised domain adaptation},<br>
  author={Wen, Jun and Liu, Risheng and Zheng, Nenggan and Zheng, Qian and Gong, Zhefeng and Yuan, Junsong},<br>
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
  volume={33},<br>
  number={01},<br>
  pages={5401--5408},<br>
  year={2019}<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss50&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_AAAI19_4.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Asynchronous Proximal Stochastic Gradient Algorithm for Composition Optimization Problems.</b><br>

      <p style="line-height:2em">Pengfei Wang, <b>Risheng Liu</b>, Nenggan Zheng, Zhefeng Gong<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong><b> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/3979">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss50" name="111ss50" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In machine learning research, many emerging applications
can be (re)formulated as the composition optimization problem with nonsmooth regularization penalty. To solve this
problem, traditional stochastic gradient descent (SGD) algorithm and its variants either have low convergence rate or
are computationally expensive. Recently, several stochastic
composition gradient algorithms have been proposed, however, these methods are still inefficient and not scalable to
large-scale composition optimization problem instances. To
address these challenges, we propose an asynchronous parallel algorithm, named Async-ProxSCVR, which effectively
combines asynchronous parallel implementation and variance
reduction method. We prove that the algorithm admits the
fastest convergence rate for both strongly convex and general nonconvex cases. Furthermore, we analyze the query
complexity of the proposed algorithm and prove that linear
speedup is accessible when we increase the number of processors. Finally, we evaluate our algorithm Async-ProxSCVR
on two representative composition optimization problems including value function evaluation in reinforcement learning and sparse mean-variance optimization problem. Experimental results show that the algorithm achieves significant
speedups and is much faster than existing compared methods.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{wang2019asynchronous,<br>
  title={Asynchronous proximal stochastic gradient algorithm for composition optimization problems},<br>
  author={Wang, Pengfei and Liu, Risheng and Zheng, Nenggan and Gong, Zhefeng},<br>
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
  volume={33},<br>
  number={01},<br>
  pages={1633--1640},<br>
  year={2019}<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<h2>2018 and before</h2>
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Online Low-Rank Representation Learning for Joint Multi-Subspace Recovery and Clustering.</b><br>

      <p style="line-height:2em">Bo Li, <b>Risheng Liu</b>, Junjie Cao, Jie Zhang, Yukun Lai, and Xiuping Liu<br><strong><span style="font-size: 12px;"><strong>IEEE TIP</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Explicit Shape Regression with Characteristic Number for Facial Landmark Localization.</b><br>

      <p style="line-height:2em">Xin Fan, <b>Risheng Liu</b>, Zhongxuan Luo, Yuntao Li, and Yuyao Feng<br><strong><span style="font-size: 12px;"><strong>IEEE TMM</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Learning to Diffuse: A New Perspective to Design PDEs for Visual Analysis.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Guangyu Zhong, Junjie Cao, Zhouchen Lin, Shiguang Shan, and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TPAMI</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Structure-Constrained Low-Rank Representation.</b><br>

      <p style="line-height:2em">Kewei Tang, <b>Risheng Liu</b>, Zhixun Su and Jie Zhang<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Low-Rank Structure Learning via Nonconvex Heuristic Recovery.</b><br>

      <p style="line-height:2em">Yue Deng, Qionghai Dai, <b>Risheng Liu</b>, Zengke Zhang, Sanqing Hu<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Feature Extraction by Learning Lorentzian Metric Tensor and Its Extensions.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhouchen Lin, Zhixun Su, Kewei Tang<br><strong><span style="font-size: 12px;"><strong>Pattern Recognition</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Proximal Alternating Direction Network: A Globally Converged Deep Unrolling Framework.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xin Fan, Shichao Cheng, Xiangyu Wang, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Unsupervised Representation Learning with Long-Term Dynamics for Skeleton Based Action Recognition.
</b><br>

      <p style="line-height:2em">Nenggan Zheng, Jun Wen, <b>Risheng Liu</b>, Liangqu Long, Jianhua Dai and Zhefeng Gong<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">A Bridging Framework for Model Optimization and Deep Propagation.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shichao Cheng, Xiaokun Liu, Long Ma, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>NeurIPS</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Learning Collaborative Generation Correction Modules for Blind Image Deblurring and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Yi He, Shichao Cheng, Xin Fan, and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>ACM MM</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Factorization-free On-line Kernel Learning for Unlabeled Chunk Data Streams.</b><br>

      <p style="line-height:2em">Yi Wang, Nan Xue, Xin Fan, Jiebo Luo, <b>Risheng Liu</b>, Bin Chen, Haojie Li, and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IJCAI</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Toward Designing Convergent Deep Operator Splitting Methods for Task-specific Nonconvex Optimization.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shichao Cheng, Yi He, Xin Fan, and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IJCAI</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Fast Factorization-free Kernel Learning for Unlabeled Chunk Data Streams.</b><br>

      <p style="line-height:2em">Yi Wang, Nan Xue, Xin Fan, Jiebo Luo, <b>Risheng Liu</b>, Bin Chen, Haojie Li, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IJCAI</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Proximal Alternating Direction Networks: A Globally Converged Deep Unrolling Framework.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xin Fan, Shichao Cheng, Xiangyu Wang and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Self-reinforced Cascaded Regression for Face Alignment.</b><br>

      <p style="line-height:2em">Xin Fan, <b>Risheng Liu</b>, Kang Huyan and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Unsupervised Representation Learning with Long-Term Dynamics for Skeleton Based Action Recognition.</b><br>

      <p style="line-height:2em">Nenggan Zheng, Jun Wen, <b>Risheng Liu</b>, Liangqu Long, Jianhua Dai and Zhefeng Gong<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Deep Location-Specific Tracking.</b><br>

      <p style="line-height:2em">Lingxiao Yang, <b>Risheng Liu</b>, David Zhang, and Lei Zhang<br><strong><span style="font-size: 12px;"><strong>ACM MM</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Linearized Alternating Direction Method with Penalization for Nonconvex and Nonsmooth Optimization.</b><br>

      <p style="line-height:2em">Yiyang Wang, <b>Risheng Liu</b>, Xiaoliang Song and Zhixun Su<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Adaptive Partial Differential Equation Learning for Visual Saliency Detection.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Junjie Cao, Zhouchen Lin and Shiguang Shan<br><strong><span style="font-size: 12px;"><strong>CVPR</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Fixed-Rank Representation for Unsupervised Visual Learning.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhouchen Lin, Fernando De la Torre, Zhixun Su<br><strong><span style="font-size: 12px;"><strong>CVPR</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Learning PDEs for Image Restoration via Optimal Control.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhouchen Lin, Wei Zhang, Zhixun Su<br><strong><span style="font-size: 12px;"><strong>ECCV</strong></span></strong></p></div>

  </div>

</td></tr>









</tbody></table>
<p>
  <div class="newline_10"></div>
  
  
  <div style="clear:both;"></div>

  <script type="text/javascript" src="./Publication_files/jquery.min.js.ä¸‹è½½"></script> 

  <script type="text/javascript" src="./Publication_files/jquery.fancybox-1.3.4.pack.js.ä¸‹è½½"></script>

  <link rel="stylesheet" type="text/css" href="./Publication_files/jquery.fancybox-1.3.4.css" media="screen">

  <script language="javascript" type="text/javascript">

function toggleDetails (id) {

  var element = document.getElementById(id);

	if(element.style.display=='block') element.style.display = 'none';

	else	                             element.style.display = 'block';

}

</script>


    <div id="newline_5"></div>
    <div id="footer"> Â© 2021 |  vog  </div> </div>
<div id="fancybox-tmp"></div><div id="fancybox-loading"><div></div></div><div id="fancybox-overlay"></div><div id="fancybox-wrap"><div id="fancybox-outer"><div class="fancybox-bg" id="fancybox-bg-n"></div><div class="fancybox-bg" id="fancybox-bg-ne"></div><div class="fancybox-bg" id="fancybox-bg-e"></div><div class="fancybox-bg" id="fancybox-bg-se"></div><div class="fancybox-bg" id="fancybox-bg-s"></div><div class="fancybox-bg" id="fancybox-bg-sw"></div><div class="fancybox-bg" id="fancybox-bg-w"></div><div class="fancybox-bg" id="fancybox-bg-nw"></div><div id="fancybox-content"></div><a id="fancybox-close"></a><div id="fancybox-title"></div><a href="javascript:;" id="fancybox-left"><span class="fancy-ico" id="fancybox-left-ico"></span></a><a href="javascript:;" id="fancybox-right"><span class="fancy-ico" id="fancybox-right-ico"></span></a></div></div></body></html>

<img src="http://www.vog/" alt="" class="bigimg">
<div class="mask"> <img src="./vog_files/close.png" alt=""> </div>

</body></html>


