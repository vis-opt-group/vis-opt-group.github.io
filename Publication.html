<!DOCTYPE html>
<!-- saved from url=(0024)http://www.vog/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="shortcut icon" href="./vog_files/vog.ico" type="image/x-icon">
<title>VOG</title>
<meta name="keywords" content="">
<meta name="description" content="">
<link type="image/x-icon" rel="shortcut icon" href="http://www.vog/logo.ico">

<link rel="stylesheet" type="text/css" media="all" href="./vog_files/style.css">
<script src="./vog_files/jquery.js.ä¸‹è½½"></script> 
<script src="./vog_files/zoom.js.ä¸‹è½½"></script>
<style>
body{font-family:"Georgia"}
init {list-style-type:none}
.bigimg {
	width: 1000px!important;
	height: 563px!important ;
	position: fixed;
	left: 0;
	top: 0;
	right: 0;
	bottom: 0;
	margin: auto;
	display: none;
	z-index: 9999;
	border: 10px solid #fff;
}
.mask {
	position: fixed;
	left: 0;
	top: 0;
	right: 0;
	bottom: 0;
	background-color: #fff;
	opacity: 0.5;
	filter: Alpha(opacity=50);
	z-index: 98;
	transition: all 1s;
	display: none
}
.bigbox {
	width: 840px;
	background: #fff;
	border: 1px solid #ededed;
	margin: 0 auto;
	border-radius: 10px;
	overflow: hidden;
	padding: 10px;
}
.bigbox>.imgbox {
	width: 400px;
	height: 250px;
	float: left;
	border-radius: 5px;
	overflow: hidden;
	margin: 0 10px 10px 10px;
}
.bigbox>.imgbox>img {
	width: 100%;
}
.imgbox:hover {
	cursor: zoom-in
}
.mask:hover {
	cursor: zoom-out
}
.mask>img {
	position: fixed;
	right: 10px;
	top: 10px;
	width: 60px;
}
.mask>img:hover {
	cursor: pointer
}
/*Exhibition Research field*/
.init {list-style-type: none}
.divcss{
width:110%;
//border:1px solid;
}

.divcss_left{
padding-top:10px;
float:left;
width:20%;
//border:1px solid;
}
.divcss_left img{
box-shadow:5px 2px 15px #000
}
 
.divcss_right{
float:right;
width:75%;
border-top:5px solid #DDD;
padding-top:15px;
font-family:Constantia;
font-size:1.05em;
color:#000;
//border:1px solid;
} 

.divcss_right a{
color:#55D;
text-decoration:none;
font-style:italic;
}

.divcss_right a:hover{
color:#55D;
}

.clear{ clear:both}

/****************News******************/
.divnewscss{
width:90%;
box-shadow:2px 2px 18px #222;
}

.divnewslicss{
border-bottom:1px solid #DDD;
padding-top:0.5em;
padding-bottom:0.5em;
padding-left:0.5em;
//margin-top:-1.3em;
//margin-bottom:-1.3em;
font-family:Georgia;
font-size:1.2em;
color:#444;
transition:background-color 0.5s;
-ms-transition:background-color 0.5s; 
-moz-transition:background-color 0.5s; /* Firefox 4 */
-webkit-transition:background-color 0.5s; /* Safari and Chrome */
-o-transition:background-color 0.5s; /* Opera */
//border:1px solid #F00;
}


.divnewslicss:hover{
background-color:#DDD;
}

.newsli_left{
width:80%;
float:left;
padding-right:3em;
border-right:1px solid #AAA;

}

.newsli_right{
width:20%;
text-align:center;
float:right;
}


/**********People*****************/
.divpeople-facultycontainer{
width:100%;
margin-top:-5%;
}

.divpeople-mctitle{
text-align:center;
font-family:Georgia;
font-size:2.6em;
margin-bottom:60px;
background:#E8E8E8;
}

.divpeople-mcsubtitle{
font-family:Comic Sans MS;
font-size:1.7em;
margin-top:40px;
margin-bottom:40px;
color:#222;
text-align:center;
}

.divpeople-container{
padding-top:10px;
}

a .divpeople-protrait img{
width:100%;
border-radius:110px;
box-shadow:0 5px 13px #666;
transition:all 0.5s;
-ms-transition:all 0.5s; 
-moz-transition:all 0.5s; /* Firefox 4 */
-webkit-transition:all 0.5s; /* Safari and Chrome */
-o-transition:all 0.5s; /* Opera */
transition:all 0.5s;
}

a .divpeople-protrait:hover img{ 
border-radius:20px; 
}

.divpeople-spinfo{
text-align:center;
//margin-top:-1em;
}

.divpeople-name{
width:100%;
font-family:Georgia;
font-weight:bold;
font-size:1.2em;
}

.divpeople-sname{
width:120%;
margin-left:-10%;
font-family:Georgia;
font-weight:bold;
font-size:1.2em;
}


.divpeople-grade{
width:100%;
font-family:Georgia;
font-size:1em;
}

.divpeople-appendinfo{
width:100%;
font-family:Georgia;
font-size:1em;
color:#3AF;
font-style:italic;
}

.divpeople-1people{
margin:0 auto;
width:20%;
text-align:center;
}

.divpeople-2people1{
width:20%;
text-align:center;
float:left;
margin-left:25%;
margin-right:5%;
}

.divpeople-2people2{
width:20%;
text-align:center;
float:left;
margin-left:5%;
margin-right:25%;
}

.divpeople-3people1{
width:20%;
text-align:center;
float:left;
margin-left:12%;
margin-right:4%;}

.divpeople-3people2{
width:20%;
text-align:center;
float:left;
margin-left:4%;
margin-right:4%;}

.divpeople-3people3{
width:20%;
text-align:center;
float:left;
margin-left:4%;
margin-right:12%;}


.divpeople-4people1{
width:20%;
text-align:center;
float:left;
margin-left:4%;
margin-right:2%;}


.divpeople-4people2{
width:20%;
text-align:center;
float:left;
margin-left:2%;
margin-right:2%;}

.divpeople-4people3{
width:20%;
text-align:center;
float:left;
margin-left:2%;
margin-right:2%;}

.divpeople-4people4{
width:20%;
text-align:center;
float:left;
margin-left:2%;
margin-right:4%;
}


/**************Publication************/
.divcssp{
width:100%;
//border:1px solid #f00;
border-top:5px solid #DDD;
}

.subp{
padding-top:10px;
//border:1px solid #00f;
}

/********pic blow up************/

.divcss_pleft_1{
padding-top:10px;
margin-bottom:-20px;
float:left;
width:18%;
transition:all 0.5s;
}

.divcss_pleft_1 img{
box-shadow:5px 2px 15px #000;
border:1px solid #FAA;
transition:all 0.5s;
-ms-transition:all 0.5s; 
-moz-transition:all 0.5s; /* Firefox 4 */
-webkit-transition:all 0.5s; /* Safari and Chrome */
-o-transition:all 0.5s; /* Opera */

}

.divcss_pleft_1:hover img{
position:absolute;
width:580px;
z-index:2;
}

/*****word part for high pic********/

.divcss_pright_pichigh{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:7%;
padding-bottom:7%;
font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright_pichigh a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright_pichigh a:hover{
color:#05E;
}

/*****word part for short pic********/

.divcss_pright_picshort{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:15px;
margin-bottom:-20px;
font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright_picshort a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright_picshort a:hover{
color:#05E;
}

/*****************************/
.divcss_pright_1{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:40px;
padding-bottom:18px;
margin-bottom:-20px;
font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright_1 a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright_1 a:hover{
color:#05E;
}




/*****publication general part*******/

.divcss_pleft{
padding-top:10px;
margin-bottom:-20px;
float:left;
width:18%;
}

.divcss_pleft img{
box-shadow:5px 2px 15px #000;
border:1px solid #FAA;
}
 
.divcss_pright{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:15px;

font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright a:hover{
color:#05E;
}


/***********Calendar Part**********/
.divcalendar{
width:100%;
box-shadow:5px 2px 15px #000;
text-align:center;
}

.divcalendarlicss{
border-bottom:1px solid #DDD;

padding-left:0.5em;
font-family:Georgia;
font-size:1.05em;
color:#444;

}

.calendarli_1{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
//border:1px solid ;
}

.calendarli_2{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
border-left:1px solid #AAA;
}

.calendarli_3{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
border-left:1px solid #AAA;
}

.calendarli_4{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
border-left:1px solid #AAA;
}


/*************************************/
/* Default css file for jemdoc. */

.test{
   width:110%;
   margin-left:-5%;
   margin-top:-4em;
}

table#tlayout {
    margin-top:-2em;
    border-collapse: collapse;
    background: white;
    font-family:Georgia,serif
}


#layout-menu {
	background: #fdf4f2;
	border: 1px solid #fff;
	border-top:1px solid;
	padding-top: 0.5em;
	padding-left: 8px;
	padding-right: 8px;
	font-size: 15px;
	width: auto;
	white-space: nowrap;
    text-align: left;
    vertical-align: top;
}

#layout-menu td {
	background: #f3e6e6;
    vertical-align: top;
}

#layout-content {
	padding-top: 1.0em;
	padding-left: 1.0em;
	padding-right: 1.0em;
    border: 1px solid #fff;
    border-top:1px solid;
    background: white;
    text-align: left;
    vertical-align: top;
}

table#tlayout #layout-menu a {
	line-height: 1.5em;
	margin-left: 0.5em;
    //color:#000;
}

tt {
    background: #ffffdd;
}

pre, tt {
	font-size: 90%;
	font-family: monaco, monospace;
}

table#tlayout a, a > tt {
	color: #993399;
	//text-decoration: underline;
}
a:hover {
	text-decoration:underline;
	color: #993399;
}

#layout-menu a.current:link, #layout-menu a.current:visited {
	color: #8b0000;
	border-bottom: 1px gray solid;
}
#layout-menu a:link, #layout-menu a:visited, #layout-menu a:hover {
	color: #8b0000;
	text-decoration: none;
}


div.menu-category {
	border-bottom: 1px solid gray;
	margin-top: 0.8em;
	padding-top: 0.2em;
	padding-bottom: 0.1em;
	font-weight: bold;
    margin-left:-8px;
}

table#tlayout .menu-category a{
    font-size:1.1em;
    color:#333;
}

div.menu-item {
	padding-left: 16px;
	text-indent: -16px;
}

div#toptitle {
	padding-bottom: 0.2em;
	margin-bottom: 1.5em;
	border-bottom: 3px double gray;
}

/* Reduce space if we begin the page with a title. */
div#toptitle + h2, div#toptitle + h3 {
	margin-top: -0.7em;
}

div#subtitle {
	margin-top: 0.0em;
	margin-bottom: 0.0em;
	padding-top: 0em;
	padding-bottom: 0.1em;
}

strong {
	font-weight: bold;
}


table#tlayout h1,table#tlayout h2,table#tlayout h3 {
	color: #993399;
	margin-top: 0.7em;
	margin-bottom: 0.3em;
	padding-bottom: 0.2em;
	line-height: 1.0;
	padding-top: 0.5em;
	border-bottom: 1px solid #aaaaaa;
}

table#tlayout h1{
    font-size:2.2em;  
}

table#tlayout h2 {
	padding-top: 0.8em;
	font-size: 180%;
}

table#tlayout h2 + h3 {
    padding-top: 0.2em;
}

table#tlayout h3 {
	font-size: 110%;
	border-bottom: none;
}

table#tlayout p {
	margin-top: 0.0em;
	margin-bottom: 0.8em;
	padding: 0;
	line-height: 1.3;
}

pre {
	padding: 0;
	margin: 0;
}



table#tlayout ul, ol, dl {
	margin-top: 0.2em;
	padding-top: 0;
	margin-bottom: 0.8em;
}

table#tlayout dt {
	margin-top: 0.5em;
	margin-bottom: 0;
}

table#tlayout dl {
	margin-left: 20px;
}

table#tlayout dd {
	color: #222222;
}

table#tlayout dd > *:first-child {
	margin-top: 0;
}

table#tlayout ul {
	list-style-position: outside;
	list-style-type: square;
//    border:1px solid #00f;
    padding-left:2.5em;
}

table#tlayout p + ul, p + ol {
	margin-top: -0.5em;
}

table#tlayout li ul, li ol {
	margin-top: -0.3em;
}

table#tlayout ol {
	list-style-position: outside;
	list-style-type: decimal;
}

table#tlayout li p, dd p {
	margin-bottom: 0.3em;
}


table#tlayout ol ol {
	list-style-type: lower-alpha;
}

table#tlayout ol ol ol {
	list-style-type: lower-roman;
}

table#tlayout p + div.codeblock {
	margin-top: -0.6em;
}

table#tlayout div.codeblock, div.infoblock {
	margin-right: 0%;
	margin-top: 1.2em;
	margin-bottom: 1.3em;
}

table#tlayout div.blocktitle {
	font-weight: bold;
	color: #8b0000;
	margin-top: 1.2em;
	margin-bottom: 0.1em;
}

table#tlayout div.blockcontent {
	border: 1px solid silver;
	padding: 0.3em 0.5em;
}

table#tlayout div.infoblock > div.blockcontent {
	background: #ffffee;
}

table#tlayout div.blockcontent p + ul, div.blockcontent p + ol {
	margin-top: 0.4em;
}

table#tlayout div.infoblock p {
	margin-bottom: 0em;
}

table#tlayout div.infoblock li p, div.infoblock dd p {
	margin-bottom: 0.5em;
}

table#tlayout div.infoblock p + p {
	margin-top: 0.8em;
}

table#tlayout div.codeblock > div.blockcontent {
	background: #f6f6f6;
}

table#tlayout span.pycommand {
	color: #000070;
}

table#tlayout span.statement {
	color: #008800;
}
table#tlayout span.builtin {
	color: #000088;
}
table#tlayout span.special {
	color: #990000;
}
table#tlayout span.operator {
	color: #880000;
}
table#tlayout span.error {
	color: #aa0000;
}


@media print {
	#layout-menu { display: none; }
}

#fwtitle {
	margin: 2px;
}

#fwtitle #toptitle {
	padding-left: 0.5em;
	margin-bottom: 0.5em;
}

#layout-content h1:first-child, #layout-content h2:first-child, #layout-content h3:first-child {
	margin-top: -0.7em;
}

table#tlayout div#toptitle h1, #layout-content div#toptitle h1 {
	margin-bottom: 0.0em;
	padding-bottom: 0.1em;
	padding-top: 0;
	margin-top: 0.5em;
	border-bottom: none;
}


table {
    border: 1px solid #333;
    margin: 0 auto;
    background: #fff;
}

table.icon {
    border: 0px solid #333;
    background: #fff;
}


td {
    padding: 2px;
    padding-left: 0.5em;
    padding-right: 0.5em;
    text-align: left;
    border: 0px solid gray;
}


table.imgtable, table.imgtable td {
    border: 0px solid #333;
    text-align: left;
    margin: 0 auto;
}

.project-subtitle{
font-size:1.1em;
padding-top:10px;
padding-bottom:10px;
margin-left:-2.2em;
font-weight:600;
}

/**********Recruitment part************/

@keyframes first
{
0%  {color:#ffffff;left:-10%}
100% {left:0%}
}

@keyframes second
{
0%  {color:#ffffff;bottom:-100px}
100% {bottom:0px}
}

@keyframes third
{
0%  {color:#ffffff;left:-10%}
100% {left:0%}
}

@keyframes forth
{
0%  {color:#ffffff;right:-10%}
100% {right:0%}
}


.recruitment-container{
	//font-family:Georgia;
	margin-top:-5%;
}

.summary{
line-height:1.8em;
padding-left:0.3em;
color:#ee0000;
width:100%;
font-size:1.25em;
position:relative;
animation: third 1.5s;
}
.sub-summary
{
font-size:1.1em;
padding-left:0.3em;
width:100%;
position:relative;
animation: forth 1.5s;
}
.recruitment-title{
font-family:Georgia;
background:#ddd;
padding-left:0.3em;
font-size:1.8em;
margin-top:50px;
position:relative;
animation: first 1.5s;
}

.recruitment-content{
line-height:1.8em;
padding-top:1em;
padding-left:0.3em;

font-size:1.1em;
position:relative;
animation: second 1.5s;
}

.reruitment-teacher{
color:#223399;
margin-top:-15px;
width:50%;
font-size:1.2em;
}

.reruitment-teacher1{
color:#223399;
width:50%;
font-size:1.2em;
}


</style>
<script>
        $(function(){
            var obj = new zoom('mask', 'bigimg','smallimg');
            obj.init();
        })
    </script>
</head>

<body>
<br>
<ol id="menu" class="init">
  <li><a href="https://vis-opt-group.github.io/People.html">People</a></li>
    <li><a href="https://vis-opt-group.github.io/Project.html">Project</a></li>
    <li class="cur"><a href="https://vis-opt-group.github.io/Publication.html">Publication</a></li>
    <li><a href="https://vis-opt-group.github.io/">Home</a></li>
  </ol>
<div id="container">   <div id="site_title_big"><a href="http://www.vog/" style="text-decoration: none"><img src="./vog_files/2.png" width="600"></a></div>
<br><br>
<div class="wrap">
	<div id="primary" class="content-area">
		<main id="main" class="site-main" role="main">
<article id="post-2308" class="post-2308 page type-page status-publish hentry">
	<div class="entry-content">
<div class="test">
<p></p><table id="tlayout" summary="Table for page layout.">
<tbody>
<tr>
<td id="layout-content"></td>
</tr>
<tr valign="top"><!-----------------------------ä¾§è¾¹æ ?---------------------------------------------------->
<td id="layout-content" width="1200px">
<h1>2021</h1>
<div class="publication_container" onclick="toggleDetails(&#39;111ss1&#39;);">

    <div class="publication_image"><img src="./vog_files/1.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning Deformable Image Registration from Optimization: Perspective, Modules, Bilevel Training and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zi Li, Xin Fan, Chenying Zhao, Hao Huang, Zhongxuan Luo.<br><strong><span style="font-size: 12px;"><em><strong>IEEE TPAMI (CCF-A)</strong></em></span></strong><b>, [<a href="https://arxiv.org/pdf/2004.14557">Paper</a>|Project page|Code]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss1" name="111ss1" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Conventional deformable registration methods aim at solving an optimization model carefully designed on image pairs and
their computational costs are exceptionally high. In contrast, recent deep learning-based approaches can provide fast deformation
estimation. These heuristic network architectures are fully data-driven and thus lack explicit geometric constraints which are
indispensable to generate plausible deformations, e.g., topology-preserving. Moreover, these learning-based approaches typically pose
hyper-parameter learning as a black-box problem and require considerable computational and human effort to perform many training runs.
To tackle the aforementioned problems, we propose a new learning-based framework to optimize a diffeomorphic model via multi-scale
propagation. Specifically, we introduce a generic optimization model to formulate diffeomorphic registration and develop a series of
learnable architectures to obtain propagative updating in the coarse-to-fine feature space. Further, we propose a new bilevel self-tuned
training strategy, allowing efficient search of task-specific hyper-parameters. This training strategy increases the flexibility to various types
of data while reduces computational and human burdens. We conduct two groups of image registration experiments on 3D volume
datasets including image-to-atlas registration on brain MRI data and image-to-image registration on liver CT data. Extensive results
demonstrate the state-of-the-art performance of the proposed method with diffeomorphic guarantee and extreme efficiency. We also
apply our framework to challenging multi-modal image registration, and investigate how our registration to support the down-streaming
tasks for medical image analysis including multi-modal fusion and image segmentation.</span></p><br>
<b style="font-size:15px;">Flow Chart:</b><br><br>
    <div class="publication_image"><img src="./vog_files/1_1.png" width="600"></div>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">@article{liu2021learning,<br>
  	title={Learning deformable image registration from optimization: perspective, modules, bilevel training and beyond},<br>
  	author={Liu, Risheng and Li, Zi and Fan, Xin and Zhao, Chenying and Huang, Hao and Luo, Zhongxuan},<br>
  	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},<br>
  	year={2021},<br>
  	publisher={IEEE}<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss2&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/mm_1.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Searching a Hierarchically
	Aggregated Fusion Architecture for
Fast Multi-Modality Image Fusion</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhu Liu, Jinyuan Liu, Xin Fan<br>
	  <strong><span style="font-size: 12px;"><em>
	  <strong>ACM MM (CCF-A)</strong></em></span></strong><b>, 
	  [<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475299">Paper</a>|<a href="https://github.com/LiuzhuForFun/Hierarchical-NAS-Image-Fusion">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss2" name="111ss2" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Multi-modality image fusion refers to generating a complementary
image that integrates typical characteristics from source images. In
recent years, we have witnessed the remarkable progress of deep
learning models for multi-modality fusion. Existing CNN-based
approaches strain every nerve to design various architectures for
realizing these tasks in an end-to-end manner. However, these handcrafted designs are unable to cope with the high demanding fusion
tasks, resulting in blurred targets and lost textural details. To alleviate these issues, in this paper, we propose a novel approach,
aiming at searching effective architectures according to various
modality principles and fusion mechanisms. Specifically, we construct a hierarchically aggregated fusion architecture to extract
and refine fused features from feature-level and object-level fusion
perspectives, which is responsible for obtaining complementary
target/detail representations. Then by investigating diverse effective practices, we composite a more flexible fusion-specific search
space. Motivated by the collaborative principle, we employ a new
search strategy with different principled losses and hardware constraints for sufficient discovery of components. As a result, we can
obtain a task-specific architecture with fast inference time. Extensive quantitative and qualitative results demonstrate the superiority
and versatility of our method against state-of-the-art methods.</span></p><br>
<b style="font-size:15px;">Flow Chart:</b><br><br>
    <div class="publication_image"><img src="./vog_files/Publication/mm_2.png" width="600"></div>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">@inproceedings{liu2021searching,<br>
  title={Searching a Hierarchically Aggregated Fusion Architecture for Fast Multi-Modality Image Fusion},<br>
  author={Liu, Risheng and Liu, Zhu and Liu, Jinyuan and Fan, Xin},<br>
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},<br>
  pages={1600--1608},<br>
  year={2021}<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss3&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/mu_tip_1.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Triple-level Model Inferred Collaborative Network Architecture for Video Deraining
</b><br>

      <p style="line-height:2em">Pan Mu, Zhu Liu, Yaohua Liu, <b>Risheng Liu</b>, Xin Fan<br>
	  <strong><span style="font-size: 12px;"><em>
	  <strong>IEEE TIP (CCF-A)</strong></em></span></strong><b>, 
	  [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9628137">Paper</a>|<a href="https://github.com/vis-opt-group/TMICS">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss3" name="111ss3" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Video deraining is an important issue for outdoor
vision systems and has been investigated extensively. However,
designing optimal architectures by the aggregating model formation and data distribution is a challenging task for video
deraining. In this paper, we develop a model-guided triplelevel optimization framework to deduce network architecture
with cooperating optimization and auto-searching mechanism, named Triple-level Model Inferred Cooperating Searching
(TMICS), for dealing with various video rain circumstances.
In particular, to mitigate the problem that existing methods
cannot cover various rain streaks distribution, we first design
a hyper-parameter optimization model about task variable and
hyper-parameter. Based on the proposed optimization model,
we design a collaborative structure for video deraining. This
structure includes Dominant Network Architecture (DNA) and
Companionate Network Architecture (CNA) that is cooperated
by introducing an Attention-based Averaging Scheme (AAS). To
better explore inter-frame information from videos, we introduce
a macroscopic structure searching scheme that searches from
Optical Flow Module (OFM) and Temporal Grouping Module
(TGM) to help restore latent frame. In addition, we apply
the differentiable neural architecture searching from a compact
candidate set of task-specific operations to discover desirable rain
streaks removal architectures automatically. Extensive experiments on various datasets demonstrate that our model shows
significant improvements in fidelity and temporal consistency
over the state-of-the-art works.</span></p><br>
<b style="font-size:15px;">Flow Chart:</b><br><br>
    <div class="publication_image"><img src="./vog_files/Publication/mu_tip_2.png" width="600"></div>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">@ARTICLE{Mu_TIP_2021,<br>
  author={Mu, Pan and Liu, Zhu and Liu, Yaohua and Liu, Risheng and Fan, Xin},<br>
  journal={IEEE Transactions on Image Processing}, <br>
  title={Triple-level Model Inferred Collaborative Network Architecture for Video Deraining}, <br>
  year={2021},<br>
  doi={10.1109/TIP.2021.3128327}}
  </p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss4&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/liu_spl_1.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning to Discover a Unified Architecture for
Low-Level Vision</b><br>

      <p style="line-height:2em">Zhu Liu, Long Ma, <b>Risheng Liu</b>, Xin Fan<br>
	  <strong><span style="font-size: 12px;"><em>
	  <strong>IEEE SPL (CCF-C)</strong></em></span></strong><b>, 
	  [<a href="https://ieeexplore.ieee.org/document/9483692">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss4" name="111ss4" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Neural Architecture Search (NAS) has pioneered various constructive principles to push forward the development
	of deep learning and achieved dramatic performances for diverse tasks recently. Existing NAS methods mainly focus on a single specific task to discover the architecture 
	automatically. But actually, these methods lack ample exploitation and exploration for the latent ability of architecture search mechanism, e.g.,
	from diverse cross-task distributions to discover a unified architecture automatically. In this work, we propose a Cross-task Differentiable ARchiTecture Search
	(Cross-DARTS for short) framework to discover a unified architecture for different low-level vision tasks automatically, to further widen the capacity of NAS.
	Specifically, we establish a new model to bridge different low-level vision tasks under the architecture search perspective. By performing a new data construction
	that integrates multi-task distributions, Cross-DARTS is obtained based on the differentiable search scheme. A multi-scale fusion cell with powerful contextual 
	representation capacity is designed as the basic component of search space towards the low-level vision. Consistent achievements of promising results on three vision tasks,
	including noise, rain, joint rain and haze removal fully show our superiority.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@ARTICLE{9483692,<br>
  author={Liu, Zhu and Ma, Long and Liu, Risheng and Fan, Xin},<br>
  journal={IEEE Signal Processing Letters}, <br>
  title={Learning to Discover a Unified Architecture for Low-Level Vision}, <br>
  year={2021},<br>
  volume={28},<br>
  number={},<br>
  pages={1470-1474},<br>
  doi={10.1109/LSP.2021.3096456}}<br>
}</p> </div>
</td>
</tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss5&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/zhu_suolue.JPG " width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px"> Collaborative Reflectance-and-Illumination Learning for High-Efficient Low-light Image Enhancement </b><br>

      <p style="line-height:2em"> Guijing Zhu <br><strong><span style="font-size: 12px;"><em><strong>ICME (CCF-B) (Oral)</strong></em></span></strong><b>, [<a href=" https://ieeexplore.ieee.org/abstract/document/9428268">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss5" name="111ss5" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);"> In this paper, we settle the low-light image enhancement problem by developing a collaborative learning framework, which not only improves lightness and suppresses noises simultaneously but also with fast speed and requires few computational resources. The approach is inspired by the fact that reflectance
and illumination are highly correlated to satisfy the well-known Retinex decomposition principle. With this in mind, we establish a Reflectance-and-Illumination Collaborative (RIC) block to depict the compact physical relationship between reflectance and illumination. By cascading multiple RIC blocks, we obtain an end-to-end RICNet to interactively optimize these two components in a collaborative manner. Benefiting from the RIC block that integrates powerful task cues, RICNet just needs few parameters to simultaneously improve brightness and remove noises. Extensive experiments demonstrate our superiority against existing stateof-the-art methods. We also make meticulous analysis for the RIC block. The results reveal the rationality and effectiveness of our built mechanism.</span></p><br>
<b style="font-size:15px;">Flow Chart:</b><br><br>
    <div class="publication_image"><img src="./vog_files/Publication/zhu_flow.jpg " width="600"></div>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em"> 
@inproceedings{zhu2021collaborative, <br>
  title={Collaborative Reflectance-And-Illumination Learning For High-Efficient Low-Light Image Enhancement},<br>
  author={Zhu, Guijing and Ma, Long and Liu, Risheng and Fan, Xin and Luo, Zhongxuan},<br>
  booktitle={2021 IEEE International Conference on Multimedia and Expo (100)}, <br>
  pages={1--6},<br>
  year={2021},<br>
  organization={IEEE}<br>
}

}</p> </div>
</div>
</td></tr>

</tbody></table>
<p>
  <div class="newline_10"></div>
  
  
  <div style="clear:both;"></div>

  <script type="text/javascript" src="./Publication_files/jquery.min.js.ä¸‹è½½"></script> 

  <script type="text/javascript" src="./Publication_files/jquery.fancybox-1.3.4.pack.js.ä¸‹è½½"></script>

  <link rel="stylesheet" type="text/css" href="./Publication_files/jquery.fancybox-1.3.4.css" media="screen">

  <script language="javascript" type="text/javascript">

function toggleDetails (id) {

  var element = document.getElementById(id);

	if(element.style.display=='block') element.style.display = 'none';

	else	                             element.style.display = 'block';

}

</script>


    <div id="newline_5"></div>
    <div id="footer"> Â© 2021 |  vog  </div> </div>
<div id="fancybox-tmp"></div><div id="fancybox-loading"><div></div></div><div id="fancybox-overlay"></div><div id="fancybox-wrap"><div id="fancybox-outer"><div class="fancybox-bg" id="fancybox-bg-n"></div><div class="fancybox-bg" id="fancybox-bg-ne"></div><div class="fancybox-bg" id="fancybox-bg-e"></div><div class="fancybox-bg" id="fancybox-bg-se"></div><div class="fancybox-bg" id="fancybox-bg-s"></div><div class="fancybox-bg" id="fancybox-bg-sw"></div><div class="fancybox-bg" id="fancybox-bg-w"></div><div class="fancybox-bg" id="fancybox-bg-nw"></div><div id="fancybox-content"></div><a id="fancybox-close"></a><div id="fancybox-title"></div><a href="javascript:;" id="fancybox-left"><span class="fancy-ico" id="fancybox-left-ico"></span></a><a href="javascript:;" id="fancybox-right"><span class="fancy-ico" id="fancybox-right-ico"></span></a></div></div></body></html>

<img src="http://www.vog/" alt="" class="bigimg">
<div class="mask"> <img src="./vog_files/close.png" alt=""> </div>

</body></html>

