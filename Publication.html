<!DOCTYPE html>
<!-- saved from url=(0024)http://www.vog/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="shortcut icon" href="./vog_files/vog.ico" type="image/x-icon">
<title>VOG</title>
<meta name="keywords" content="">
<meta name="description" content="">
<link type="image/x-icon" rel="shortcut icon" href="http://www.vog/logo.ico">

<link rel="stylesheet" type="text/css" media="all" href="./vog_files/style.css">
<script src="./vog_files/jquery.js.???：：??"></script> 
<script src="./vog_files/zoom.js.???：：??"></script>
<style>
body{font-family:"Georgia"}
init {list-style-type:none}
.bigimg {
	width: 1000px!important;
	height: 563px!important ;
	position: fixed;
	left: 0;
	top: 0;
	right: 0;
	bottom: 0;
	margin: auto;
	display: none;
	z-index: 9999;
	border: 10px solid #fff;
}
.mask {
	position: fixed;
	left: 0;
	top: 0;
	right: 0;
	bottom: 0;
	background-color: #fff;
	opacity: 0.5;
	filter: Alpha(opacity=50);
	z-index: 98;
	transition: all 1s;
	display: none
}
.bigbox {
	width: 840px;
	background: #fff;
	border: 1px solid #ededed;
	margin: 0 auto;
	border-radius: 10px;
	overflow: hidden;
	padding: 10px;
}
.bigbox>.imgbox {
	width: 400px;
	height: 250px;
	float: left;
	border-radius: 5px;
	overflow: hidden;
	margin: 0 10px 10px 10px;
}
.bigbox>.imgbox>img {
	width: 100%;
}
.imgbox:hover {
	cursor: zoom-in
}
.mask:hover {
	cursor: zoom-out
}
.mask>img {
	position: fixed;
	right: 10px;
	top: 10px;
	width: 60px;
}
.mask>img:hover {
	cursor: pointer
}
/*Exhibition Research field*/
.init {list-style-type: none}
.divcss{
width:110%;
//border:1px solid;
}

.divcss_left{
padding-top:10px;
float:left;
width:20%;
//border:1px solid;
}
.divcss_left img{
box-shadow:5px 2px 15px #000
}
 
.divcss_right{
float:right;
width:75%;
border-top:5px solid #DDD;
padding-top:15px;
font-family:Constantia;
font-size:1.05em;
color:#000;
//border:1px solid;
} 

.divcss_right a{
color:#55D;
text-decoration:none;
font-style:italic;
}

.divcss_right a:hover{
color:#55D;
}

.clear{ clear:both}

/****************News******************/
.divnewscss{
width:90%;
box-shadow:2px 2px 18px #222;
}

.divnewslicss{
border-bottom:1px solid #DDD;
padding-top:0.5em;
padding-bottom:0.5em;
padding-left:0.5em;
//margin-top:-1.3em;
//margin-bottom:-1.3em;
font-family:Georgia;
font-size:1.2em;
color:#444;
transition:background-color 0.5s;
-ms-transition:background-color 0.5s; 
-moz-transition:background-color 0.5s; /* Firefox 4 */
-webkit-transition:background-color 0.5s; /* Safari and Chrome */
-o-transition:background-color 0.5s; /* Opera */
//border:1px solid #F00;
}


.divnewslicss:hover{
background-color:#DDD;
}

.newsli_left{
width:80%;
float:left;
padding-right:3em;
border-right:1px solid #AAA;

}

.newsli_right{
width:20%;
text-align:center;
float:right;
}


/**********People*****************/
.divpeople-facultycontainer{
width:100%;
margin-top:-5%;
}

.divpeople-mctitle{
text-align:center;
font-family:Georgia;
font-size:2.6em;
margin-bottom:60px;
background:#E8E8E8;
}

.divpeople-mcsubtitle{
font-family:Comic Sans MS;
font-size:1.7em;
margin-top:40px;
margin-bottom:40px;
color:#222;
text-align:center;
}

.divpeople-container{
padding-top:10px;
}

a .divpeople-protrait img{
width:100%;
border-radius:110px;
box-shadow:0 5px 13px #666;
transition:all 0.5s;
-ms-transition:all 0.5s; 
-moz-transition:all 0.5s; /* Firefox 4 */
-webkit-transition:all 0.5s; /* Safari and Chrome */
-o-transition:all 0.5s; /* Opera */
transition:all 0.5s;
}

a .divpeople-protrait:hover img{ 
border-radius:20px; 
}

.divpeople-spinfo{
text-align:center;
//margin-top:-1em;
}

.divpeople-name{
width:100%;
font-family:Georgia;
font-weight:bold;
font-size:1.2em;
}

.divpeople-sname{
width:120%;
margin-left:-10%;
font-family:Georgia;
font-weight:bold;
font-size:1.2em;
}


.divpeople-grade{
width:100%;
font-family:Georgia;
font-size:1em;
}

.divpeople-appendinfo{
width:100%;
font-family:Georgia;
font-size:1em;
color:#3AF;
font-style:italic;
}

.divpeople-1people{
margin:0 auto;
width:20%;
text-align:center;
}

.divpeople-2people1{
width:20%;
text-align:center;
float:left;
margin-left:25%;
margin-right:5%;
}

.divpeople-2people2{
width:20%;
text-align:center;
float:left;
margin-left:5%;
margin-right:25%;
}

.divpeople-3people1{
width:20%;
text-align:center;
float:left;
margin-left:12%;
margin-right:4%;}

.divpeople-3people2{
width:20%;
text-align:center;
float:left;
margin-left:4%;
margin-right:4%;}

.divpeople-3people3{
width:20%;
text-align:center;
float:left;
margin-left:4%;
margin-right:12%;}


.divpeople-4people1{
width:20%;
text-align:center;
float:left;
margin-left:4%;
margin-right:2%;}


.divpeople-4people2{
width:20%;
text-align:center;
float:left;
margin-left:2%;
margin-right:2%;}

.divpeople-4people3{
width:20%;
text-align:center;
float:left;
margin-left:2%;
margin-right:2%;}

.divpeople-4people4{
width:20%;
text-align:center;
float:left;
margin-left:2%;
margin-right:4%;
}


/**************Publication************/
.divcssp{
width:100%;
//border:1px solid #f00;
border-top:5px solid #DDD;
}

.subp{
padding-top:10px;
//border:1px solid #00f;
}

/********pic blow up************/

.divcss_pleft_1{
padding-top:10px;
margin-bottom:-20px;
float:left;
width:18%;
transition:all 0.5s;
}

.divcss_pleft_1 img{
box-shadow:5px 2px 15px #000;
border:1px solid #FAA;
transition:all 0.5s;
-ms-transition:all 0.5s; 
-moz-transition:all 0.5s; /* Firefox 4 */
-webkit-transition:all 0.5s; /* Safari and Chrome */
-o-transition:all 0.5s; /* Opera */

}

.divcss_pleft_1:hover img{
position:absolute;
width:580px;
z-index:2;
}

/*****word part for high pic********/

.divcss_pright_pichigh{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:7%;
padding-bottom:7%;
font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright_pichigh a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright_pichigh a:hover{
color:#05E;
}

/*****word part for short pic********/

.divcss_pright_picshort{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:15px;
margin-bottom:-20px;
font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright_picshort a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright_picshort a:hover{
color:#05E;
}

/*****************************/
.divcss_pright_1{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:40px;
padding-bottom:18px;
margin-bottom:-20px;
font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright_1 a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright_1 a:hover{
color:#05E;
}




/*****publication general part*******/

.divcss_pleft{
padding-top:10px;
margin-bottom:-20px;
float:left;
width:18%;
}

.divcss_pleft img{
box-shadow:5px 2px 15px #000;
border:1px solid #FAA;
}
 
.divcss_pright{
float:right;
width:78%;
//border:1px solid #f00;
padding-top:15px;

font-family:Constantia;
font-size:1.1em;
} 

.divcss_pright a{
color:#05E;
text-decoration:none;
font-style:italic;
}

.divcss_pright a:hover{
color:#05E;
}


/***********Calendar Part**********/
.divcalendar{
width:100%;
box-shadow:5px 2px 15px #000;
text-align:center;
}

.divcalendarlicss{
border-bottom:1px solid #DDD;

padding-left:0.5em;
font-family:Georgia;
font-size:1.05em;
color:#444;

}

.calendarli_1{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
//border:1px solid ;
}

.calendarli_2{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
border-left:1px solid #AAA;
}

.calendarli_3{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
border-left:1px solid #AAA;
}

.calendarli_4{
width:25%;
float:left;
padding-top:0.5em;
padding-bottom:0.5em;
border-left:1px solid #AAA;
}


/*************************************/
/* Default css file for jemdoc. */

.test{
   width:110%;
   margin-left:-5%;
   margin-top:-4em;
}

table#tlayout {
    margin-top:-2em;
    border-collapse: collapse;
    background: white;
    font-family:Georgia,serif
}


#layout-menu {
	background: #fdf4f2;
	border: 1px solid #fff;
	border-top:1px solid;
	padding-top: 0.5em;
	padding-left: 8px;
	padding-right: 8px;
	font-size: 15px;
	width: auto;
	white-space: nowrap;
    text-align: left;
    vertical-align: top;
}

#layout-menu td {
	background: #f3e6e6;
    vertical-align: top;
}

#layout-content {
	padding-top: 1.0em;
	padding-left: 1.0em;
	padding-right: 1.0em;
    border: 1px solid #fff;
    border-top:1px solid;
    background: white;
    text-align: left;
    vertical-align: top;
}

table#tlayout #layout-menu a {
	line-height: 1.5em;
	margin-left: 0.5em;
    //color:#000;
}

tt {
    background: #ffffdd;
}

pre, tt {
	font-size: 90%;
	font-family: monaco, monospace;
}

table#tlayout a, a > tt {
	color: #430086;
	//text-decoration: underline;
}
a:hover {
	text-decoration:underline;
	color: #430086;
}

#layout-menu a.current:link, #layout-menu a.current:visited {
	color: #8b0000;
	border-bottom: 1px gray solid;
}
#layout-menu a:link, #layout-menu a:visited, #layout-menu a:hover {
	color: #8b0000;
	text-decoration: none;
}


div.menu-category {
	border-bottom: 1px solid gray;
	margin-top: 0.8em;
	padding-top: 0.2em;
	padding-bottom: 0.1em;
	font-weight: bold;
    margin-left:-8px;
}

table#tlayout .menu-category a{
    font-size:1.1em;
    color:#333;
}

div.menu-item {
	padding-left: 16px;
	text-indent: -16px;
}

div#toptitle {
	padding-bottom: 0.2em;
	margin-bottom: 1.5em;
	border-bottom: 3px double gray;
}

/* Reduce space if we begin the page with a title. */
div#toptitle + h2, div#toptitle + h3 {
	margin-top: -0.7em;
}

div#subtitle {
	margin-top: 0.0em;
	margin-bottom: 0.0em;
	padding-top: 0em;
	padding-bottom: 0.1em;
}

strong {
	font-weight: bold;
}


table#tlayout h1,table#tlayout h2,table#tlayout h3 {
	color: #6937CD;
	margin-top: 0.7em;
	margin-bottom: 0.3em;
	padding-bottom: 0.2em;
	line-height: 1.0;
	padding-top: 0.5em;
	border-bottom: 1px solid #aaaaaa;
}

table#tlayout h1{
    font-size:2.2em;  
}

table#tlayout h2 {
	padding-top: 0.8em;
	font-size: 180%;
}

table#tlayout h2 + h3 {
    padding-top: 0.2em;
}

table#tlayout h3 {
	font-size: 110%;
	border-bottom: none;
}

table#tlayout p {
	margin-top: 0.0em;
	margin-bottom: 0.8em;
	padding: 0;
	line-height: 1.3;
}

pre {
	padding: 0;
	margin: 0;
}



table#tlayout ul, ol, dl {
	margin-top: 0.2em;
	padding-top: 0;
	margin-bottom: 0.8em;
}

table#tlayout dt {
	margin-top: 0.5em;
	margin-bottom: 0;
}

table#tlayout dl {
	margin-left: 20px;
}

table#tlayout dd {
	color: #222222;
}

table#tlayout dd > *:first-child {
	margin-top: 0;
}

table#tlayout ul {
	list-style-position: outside;
	list-style-type: square;
//    border:1px solid #00f;
    padding-left:2.5em;
}

table#tlayout p + ul, p + ol {
	margin-top: -0.5em;
}

table#tlayout li ul, li ol {
	margin-top: -0.3em;
}

table#tlayout ol {
	list-style-position: outside;
	list-style-type: decimal;
}

table#tlayout li p, dd p {
	margin-bottom: 0.3em;
}


table#tlayout ol ol {
	list-style-type: lower-alpha;
}

table#tlayout ol ol ol {
	list-style-type: lower-roman;
}

table#tlayout p + div.codeblock {
	margin-top: -0.6em;
}

table#tlayout div.codeblock, div.infoblock {
	margin-right: 0%;
	margin-top: 1.2em;
	margin-bottom: 1.3em;
}

table#tlayout div.blocktitle {
	font-weight: bold;
	color: #8b0000;
	margin-top: 1.2em;
	margin-bottom: 0.1em;
}

table#tlayout div.blockcontent {
	border: 1px solid silver;
	padding: 0.3em 0.5em;
}

table#tlayout div.infoblock > div.blockcontent {
	background: #ffffee;
}

table#tlayout div.blockcontent p + ul, div.blockcontent p + ol {
	margin-top: 0.4em;
}

table#tlayout div.infoblock p {
	margin-bottom: 0em;
}

table#tlayout div.infoblock li p, div.infoblock dd p {
	margin-bottom: 0.5em;
}

table#tlayout div.infoblock p + p {
	margin-top: 0.8em;
}

table#tlayout div.codeblock > div.blockcontent {
	background: #f6f6f6;
}

table#tlayout span.pycommand {
	color: #000070;
}

table#tlayout span.statement {
	color: #008800;
}
table#tlayout span.builtin {
	color: #000088;
}
table#tlayout span.special {
	color: #990000;
}
table#tlayout span.operator {
	color: #880000;
}
table#tlayout span.error {
	color: #aa0000;
}


@media print {
	#layout-menu { display: none; }
}

#fwtitle {
	margin: 2px;
}

#fwtitle #toptitle {
	padding-left: 0.5em;
	margin-bottom: 0.5em;
}

#layout-content h1:first-child, #layout-content h2:first-child, #layout-content h3:first-child {
	margin-top: -0.7em;
}

table#tlayout div#toptitle h1, #layout-content div#toptitle h1 {
	margin-bottom: 0.0em;
	padding-bottom: 0.1em;
	padding-top: 0;
	margin-top: 0.5em;
	border-bottom: none;
}


table {
    border: 1px solid #333;
    margin: 0 auto;
    background: #fff;
}

table.icon {
    border: 0px solid #333;
    background: #fff;
}


td {
    padding: 2px;
    padding-left: 0.5em;
    padding-right: 0.5em;
    text-align: left;
    border: 0px solid gray;
}


table.imgtable, table.imgtable td {
    border: 0px solid #333;
    text-align: left;
    margin: 0 auto;
}

.project-subtitle{
font-size:1.1em;
padding-top:10px;
padding-bottom:10px;
margin-left:-2.2em;
font-weight:600;
}

/**********Recruitment part************/

@keyframes first
{
0%  {color:#ffffff;left:-10%}
100% {left:0%}
}

@keyframes second
{
0%  {color:#ffffff;bottom:-100px}
100% {bottom:0px}
}

@keyframes third
{
0%  {color:#ffffff;left:-10%}
100% {left:0%}
}

@keyframes forth
{
0%  {color:#ffffff;right:-10%}
100% {right:0%}
}


.recruitment-container{
	//font-family:Georgia;
	margin-top:-5%;
}

.summary{
line-height:1.8em;
padding-left:0.3em;
color:#ee0000;
width:100%;
font-size:1.25em;
position:relative;
animation: third 1.5s;
}
.sub-summary
{
font-size:1.1em;
padding-left:0.3em;
width:100%;
position:relative;
animation: forth 1.5s;
}
.recruitment-title{
font-family:Georgia;
background:#ddd;
padding-left:0.3em;
font-size:1.8em;
margin-top:50px;
position:relative;
animation: first 1.5s;
}

.recruitment-content{
line-height:1.8em;
padding-top:1em;
padding-left:0.3em;

font-size:1.1em;
position:relative;
animation: second 1.5s;
}

.reruitment-teacher{
color:#223399;
margin-top:-15px;
width:50%;
font-size:1.2em;
}

.reruitment-teacher1{
color:#223399;
width:50%;
font-size:1.2em;
}


</style>
<script>
        $(function(){
            var obj = new zoom('mask', 'bigimg','smallimg');
            obj.init();
        })
    </script>
</head>

<body>
<br>
<ol id="menu" class="init">
  <li><a href="https://vis-opt-group.github.io/People.html">People</a></li>
    <li><a href="https://vis-opt-group.github.io/Project.html">Project</a></li>
    <li class="cur"><a href="https://vis-opt-group.github.io/Publication.html">Publication</a></li>
    <li><a href="https://vis-opt-group.github.io/">Home</a></li>
  </ol>
<div id="container">   <div id="site_title_big"><a href="https://rsliu.tech/" style="text-decoration: none"><img src="./vog_files/3.png" width="600"></a></div>
<br><br>
<div class="wrap">
	<div id="primary" class="content-area">
		<main id="main" class="site-main" role="main">
<article id="post-2308" class="post-2308 page type-page status-publish hentry">
	<div class="entry-content">
<div class="test">
<p></p><table id="tlayout" summary="Table for page layout.">
<tbody>
<tr>
<td id="layout-content"></td>
</tr>
<tr valign="top"><!-----------------------------???━：：?????---------------------------------------------------->
<td id="layout-content" width="1200px">
<h1>2022</h1></td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss151&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rsliu_TIP22.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Twin Adversarial Contrastive Learning for Underwater Image Enhancement and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu*</b>, Zhiying Jiang, Shuzhou Yang, Xin Fan<br><strong><span style="font-size: 12px;"><strong>IEEE TIP</strong></span></strong><b></b></p></div>

  </div>

  <div class="publication_detail" id="111ss151" name="111ss151" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Underwater images suffer from severe distortion,which degrades the accuracy of object detection performed in
an underwater environment. Existing underwater image enhancement algorithms focus on the restoration of contrast and scene
reflection. In practice, the enhanced images may not benefit the
effectiveness of detection and even lead to a severe performance
drop. In this paper, we propose an object-guided twin adversarial
contrastive learning based underwater enhancement method to
achieve both visual-friendly and task-orientated enhancement.
Concretely, we first develop a bilateral constrained closed-loop
adversarial enhancement module, which eases the requirement
of paired data with the unsupervised manner and preserves
more informative features by coupling with the twin inverse
mapping. In addition, to confer the restored images with a more
realistic appearance, we also adopt the contrastive cues in the
training phase. To narrow the gap between visually-oriented and
detection-favorable target images, a task-aware feedback module
is embedded in the enhancement process, where the coherent
gradient information of the detector is incorporated to guide
the enhancement towards the detection-pleasing direction. To
validate the performance, we allocate a series of prolific detectors
into our framework. Extensive experiments demonstrate that the
enhanced results of our method show remarkable amelioration
in visual quality, the accuracy of different detectors conducted
on our enhanced images has been promoted notably. Moreover,
we also conduct a study on semantic segmentation to illustrate
how object guidance improves high-level tasks.</span></p><br>

</div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss631&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/Jiang_TCSVT_2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Target Oriented Perceptual Adversarial Fusion Network for Underwater Image Enhancement.</b><br>

      <p style="line-height:2em">Zhiying Jiang, Zhuoxiao Li, Shuzhou Yang, Xin Fan, <b>Risheng Liu*</b><br><strong><span style="font-size: 12px;"><strong>IEEE TCSVT <b style="color:red"></b></strong></span></strong><b>[<a href="https://ieeexplore.ieee.org/abstract/document/9774330">Paper</a>|<a href="https://github.com/Jzy2017/TOPAL">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss631" name="111ss631" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Due  to  the  refraction  and  absorption  of  light  bywater, underwater images usually suffer from severe degradation,such  as  color  cast,  hazy  blur,  and  low  visibility,  which  woulddegrade  the  effectiveness  of  marine  applications  equipped  onautonomous  underwater  vehicles.  To  eliminate  the  degradationof  underwater  images,  we  propose  a  target  oriented  perceptualadversarial   fusion   network,   dubbed   TOPAL.   Concretely,   weconsider the degradation factors of underwater images in termsof turbidity and chromatism. And according to the degradationissues,   we   first   develop   a   multi-scale   dense   boosted   moduleto  strengthen  the  visual  contrast  and  a  deep  aesthetic  rendermodule to perform the color correction, respectively. After that,we  employ  the  dual  channel-wise  attention  module  and  guidethe  adaptive  fusion  of  latent  features,  in  which  both  diversedetails  and  credible  appearance  are  integrated.  To  bridge  thegap between synthetic and real-world  images, a global-local ad-versarial mechanism is introduced in the reconstruction. Besides,perceptual  information  is  also  embedded  into  the  process  toassist  the  understanding  of  scenery  content.  To  evaluate  theperformance  of  TOPAL,  we  conduct  extensive  experiments  onseveral  benchmarks  and  make  comparisons  among  state-of-the-art  methods.  Quantitative  and  qualitative  results  demonstratethat  our  TOPAL  improves  the  quality  of  underwater  imagesgreatly and achieves superior  performance than others.</span></p><br>

<div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{jiang2022target,<br>
  title={Target Oriented Perceptual Adversarial Fusion Network for Underwater Image Enhancement},<br>
  author={Jiang, Zhiying and Li, Zhuoxiao and Yang, Shuzhou and Fan, Xin and Liu, Risheng},<br>
  journal={IEEE Transactions on Circuits and Systems for Video Technology},<br>
  year={2022},<br>
  publisher={IEEE}<br>
}</p> </div>
</div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss181&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rsliu_PR_2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Hierarchical domain adaptation with local feature patterns.</b><br>

      <p style="line-height:2em">Jun Wen, Junsong Yuan, Qian Zheng, <b>Risheng Liu</b>, Zhefeng Gong, Nenggan Zheng<br><strong><span style="font-size: 12px;"><strong>Pattern Recognition</strong></span></strong><b> [<a href="https://www.sciencedirect.com/science/article/pii/S003132032100621X">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss181" name="111ss181" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Domain adaptation is proposed to generalize learning machines and address performance degradation
of models that are trained from one specific source domain but applied to novel target domains. Exist-
ing domain adaptation methods focus on transferring holistic features whose discriminability is generally
tailored to be source-specific and inferiorly generic to be transferable. As a result, standard domain adap-
tation on holistic features usually damages feature structures, especially local feature statistics, and dete-
riorates the learned discriminability. To alleviate this issue, we propose to transfer primitive local feature
patterns, whose discriminability are shown to be inherently more sharable, and perform hierarchical fea-
ture adaptation. Concretely, we first learn a cluster of domain-shared local feature patterns and partition
the feature space into cells. Local features are adaptively aggregated inside each cell to obtain cell fea-
tures, which are further integrated into holistic features. To achieve fine-grained adaptations, we simulta-
neously perform alignment on local features, cell features and holistic features, within which process the
local and cell features are aligned independently inside each cell to maintain the learned local structures
and prevent negative transfer. Experimenting on typical one-to-one unsupervised domain adaptation for
both image classification and action recognition tasks, partial domain adaptation, and domain-agnostic
adaptation, we show that the proposed method achieves more reliable feature transfer by consistently
outperforming state-of-the-art models and the learned domain-invariant features generalize well to novel
domains.</span></p><br>

<div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{wen2022hierarchical,<br>
  title={Hierarchical domain adaptation with local feature patterns},<br>
  author={Wen, Jun and Yuan, Junsong and Zheng, Qian and Liu, Risheng and Gong, Zhefeng and Zheng, Nenggan},<br>
  journal={Pattern Recognition},<br>
  volume={124},<br>
  pages={108445},<br>
  year={2022},<br>
  publisher={Elsevier}<br>
}</p> </div>


</div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss180&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/TMM_malong_22.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Low-Light Image Enhancement via Self-Reinforced Retinex Projection Model.</b><br>

      <p style="line-height:2em">Long Ma, <b>Risheng Liu</b>, Yiyang Wang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TMM</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/abstract/document/9743313">Paper</a>|<a href="https://github.com/LongMa319/SRRP">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss180" name="111ss180" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Low-light image enhancement aims to improve the
quality of images captured under low-lightening conditions,
which is a fundamental problem in computer vision and multimedia
areas. Although many efforts have been invested over
the years, existing illumination-based models tend to generate
unnatural-looking results (e.g., over-exposure). It is because that
the widely-adopted illumination adjustment (e.g., Gamma Correction)
breaks down the favorable smoothness property of the
original illumination derived from the well-designed illumination
estimation model. To settle this issue, a great-efficiency and
high-quality Self-Reinforced Retinex Projection (SRRP) model is
developed in this paper, which contains optimization modules of
both illumination and reflectance layers. Specifically, we construct
a new fidelity term with the self-reinforced function for the
illumination optimization to eliminate the dependence of the
illumination adjustment to obtain a desired illumination with the
excellent smoothing property. By introducing a flexible feasible
constraint, we obtain a reflectance optimization module with
projection. Owing to its flexibility, we can extend our model
to an enhanced version by integrating a data-driven denoising
mechanism as the projection, which is able to effectively handle
the generated noises/artifacts in the enhanced procedure. In the
experimental part, on one side, we make ample comparative
assessments on multiple benchmarks with considerable state-ofthe-
art methods. These evaluations fully verify the outstanding
performance of our method, in terms of the qualitative and
quantitative analyses and execution efficiency. On the other side,
we also conduct extensive analytical experiments to indicate the
effectiveness and advantages of our proposed model. Code is
available at https://github.com/LongMa319/SRRP.</span></p><br>

<div class="newline_10"></div>

      
      </div>


</div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss150&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/jinyuan_TCSVT22.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Attention-guided Global-local Adversarial Learning for Detail-preserving Multi-exposure Image Fusion.</b><br>

      <p style="line-height:2em">Jinyuan Liu, Jingjie Shang, <b>Risheng Liu</b>, Xin Fan<br><strong><span style="font-size: 12px;"><strong>IEEE TCSVT</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/document/9684913">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss150" name="111ss150" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Deep learning networks have recently demonstrated
yielded impressive progress for multi-exposure image fusion.
However, how to restore realistic texture details while correcting
color distortion is still a challenging problem to be solved. To
alleviate the aforementioned issues, in this paper, we propose
an attention-guided global-local adversarial learning network
for fusing extreme exposure images in a coarse-to-fine manner.
Firstly, the coarse fusion result is generated under the guidance
of attention weight maps, which acquires the essential region
of interest from both sides. Secondly, we formulate an edge
loss function, along with a spatial feature transform layer, for
refining the fusion process. So that it can take full use of
the edge information to deal with blurry edges. Moreover, by
incorporating global-local learning, our method can balance pixel
intensity distribution and correct the color distortion on spatially
varying source images from both image/patch perspectives. Such
a global-local discriminator ensures all the local patches of the
fused images align with realistic normal-exposure ones. Extensive
experimental results on two publicly available datasets show that
our method drastically outperforms state-of-the-art methods in
visual inspection and objective analysis. Furthermore, sufficient
ablation experiments prove that our method has significant advantages in generating high-quality fused results with appealing
details, clear targets, and faithful color.</span></p><br>

<div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2022attention,<br>
  title={Attention-guided Global-local Adversarial Learning for Detail-preserving Multi-exposure Image Fusion},<br>
  author={Liu, Jinyuan and Shang, Jingjie and Liu, Risheng and Fan, Xin},<br>
  journal={IEEE Transactions on Circuits and Systems for Video Technology},<br>
  year={2022},<br>
  publisher={IEEE}<br>
}</p> </div>


</div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss152&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/jiang_MM2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Towards All Weather and Unobstructed Multi-Spectral
Image Stitching: Algorithm and Benchmark.</b><br>

      <p style="line-height:2em">Zhiying Jiang, Zengxi Zhang, Xin Fan, <b>Risheng Liu*</b><br><strong><span style="font-size: 12px;"><strong>ACM MM <b style="color:red"></b></strong></span></strong><b></b></p></div>

  </div>

  <div class="publication_detail" id="111ss152" name="111ss152" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Image stitching is a fundamental task that requires multiple images
from different viewpoints to generate a wide field-of-viewing (FOV)
scene. Previous methods are developed on RGB images. However,
the severe weather and harsh conditions, such as rain, fog, low light,
strong light, etc., on visible images may introduce evident interference, leading to the distortion and misalignment of the stitched
results. To remedy the deficient imaging of optical sensors, we investigate the complementarity across infrared and visible images
to improve the perception of scenes in terms of visual information and viewing ranges. Instead of the cascaded fusion-stitching
process, where the inaccuracy accumulation caused by image fusion hinders the stitch performance, especially content loss and
ghosting effect, we develop a learnable feature adaptive network
to investigate a stitch-oriented feature representation and perform
the information complementary at the feature-level. By introducing a pyramidal structure along with the global fast correlation
regression, the quadrature attention based correspondence is more
responsible for feature alignment, and the estimation of sparse offsets can be realized in a coarse-to-fine manner. Furthermore, we
propose the first infrared and visible image based multi-spectral
image stitching dataset, covering a more comprehensive range of
scenarios and diverse viewing baselines. Extensive experiments
on real-world data demonstrate that our method reconstructs the
wide FOV images with more credible structure and complementary
information against state-of-the-arts.</span></p><br>

<div class="newline_10"></div>

</div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss153&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/mty_MM2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">PIA: Parallel Architecture with Illumination Allocator for
Joint Enhancement and Detection in Low-Light.</b><br>

      <p style="line-height:2em">Tengyu Ma, Long Ma, Xin Fan, Zhongxuan Luo, <b>Risheng Liu*</b><br><strong><span style="font-size: 12px;"><strong>ACM MM <b style="color:red"></b></strong></span></strong><b></b></p></div>

  </div>

  <div class="publication_detail" id="111ss153" name="111ss153" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Visual perception in low-light conditions (e.g., nighttime)
plays an important role in various multimedia-related applications (e.g., autonomous driving). The enhancement (provides a visual-friendly appearance) and detection (detects the instances of objects) in low-light are two fundamental and crucial visual perception tasks. In this paper, we make efforts on how to simultaneously realize low-light enhancement
and detection from two aspects. First, we define a parallel
architecture to satisfy the task demand for both two tasks.
In which, a decomposition-type warm-start acting on the
entrance of parallel architecture is developed to narrow down
the adverse effects brought by low-light scenes to some extent.
Second, a novel illumination allocator is designed by encoding the key illumination component (the inherent difference
between normal-light and low-light) to extract hierarchical
features for assisting in enhancement and detection. Further,
we make a substantive discussion for our proposed method.
That is, we solve enhancement in a coarse-to-fine manner and
handle detection in a decomposed-to-integrated fashion. Finally, multidimensional analytical and evaluated experiments
are performed to indicate our effectiveness and superiority.
Code and results will be public if this paper can be accepted.</span></p><br>

<div class="newline_10"></div>

</div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss154&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/xue_MM2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Best of Both Worlds: See and Understand Clearly in the Dark.</b><br>

      <p style="line-height:2em">Xinwei Xue, Jia He, Long Ma, Yi Wang, Xin Fan, <b>Risheng Liu</b><br><strong><span style="font-size: 12px;"><strong>ACM MM <b style="color:red"></b></strong></span></strong><b></b></p></div>

  </div>

  <div class="publication_detail" id="111ss154" name="111ss154" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Recently, with the development of intelligent technology, the perception of low-light scenes has been gaining widespread attention. However, existing techniques usually focus on only
one task (e.g., enhancement) and lose sight of the others (e.g.,
detection), making it difficult to perform all of them well
at the same time. To overcome this limitation, we propose
a new method that can handle visual quality enhancement
and semantic-related tasks (e.g., detection, segmentation)
simultaneously in a unified framework. Specifically, we build
a cascaded architecture to meet the task requirements. To
better enhance the entanglement in both tasks and achieve
mutual guidance, we develop a new contrastive-alternative
learning strategy for learning the model parameters, to largely improve the representational capacity of the cascaded
architecture. Notably, the contrastive learning mechanism
establishes the communication between two objective tasks in
essence, which actually extends the capability of contrastive
learning to some extent. Finally, extensive experiments are
performed to fully validate the advantages of our method
over other state-of-the-art works in enhancement, detection,
and segmentation. A series of analytical evaluations are also conducted to reveal our effectiveness. The code will be
available after this work is accepted.</span></p><br>

<div class="newline_10"></div>

</div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss630&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/RishengLiu_ICML_22.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Optimization-Derived Learning with Essential Convergence Analysis of Training and Hyper-training.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xuan Liu, Shangzhi Zeng, Jin Zhang, Yixuan Zhang<br><strong><span style="font-size: 12px;"><strong>ICML <b style="color:red"></b></strong></span></strong><b>[<a onclick='return confirm("Coming Soon!")'>Paper</a>|<a onclick='return confirm("Coming Soon!")'>Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss630" name="111ss630" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Recently, Optimization-Derived Learning (ODL)
has attracted attention from learning and vision
areas, which designs learning models from the perspective
of optimization. However, previous ODL
approaches regard the training and hyper-training
procedures as two separated stages, meaning that
the learnable parameters have to be fixed during
the training process, and thus simultaneously
obtaining the convergence on training variables
and learnable parameters is also impossible. In
this work, we design a Generalized Krasnoselskii-
Mann (GKM) scheme based on fixed-point iterations
as our fundamental ODL module, which
unifies existing ODL methods as special cases.
Under GKM scheme, the Bilevel Meta Optimization
(BMO) algorithmic framework is constructed
to solve the optimal iterative variables for training
and learnable parameters for hyper-training together.
We rigorously prove the joint convergence
of fixed-point iteration and learning processes,
both on the approximation quality, and on the stationary
analysis. Experiments demonstrate the
efficiency of BMO with competitive performance
on sparse coding and real-world applications such
as image deconvolution and rain streak removal.</span></p><br>

<div class="newline_10"></div>

</div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss622&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/wang_ijcai_2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration.</b><br>

      <p style="line-height:2em">Di Wang, Jinyuan Liu, Xin Fan, <b>Risheng Liu*</b><br><strong><span style="font-size: 12px;"><strong>IJCAI <b style="color:red">(Long Presentation, Acceptance Rate &le; 3.75%)</b></strong></span></strong><b> [<a href="https://github.com/wdhudiekou/UMF-CMGR">Code</>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss622" name="111ss622" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Recent learning-based image fusion methods have
marked numerous progress in pre-registered multimodality
data, but suffered serious ghosts dealing
with misaligned multi-modality data, due to
the spatial deformation and the difficulty narrowing
cross-modality discrepancy. To overcome the
obstacles, in this paper, we present a robust crossmodality
generation-registration paradigm for unsupervised
misaligned infrared and visible image
fusion (IVIF). Specifically, we propose a Crossmodality
Perceptual Style Transfer Network (CPSTN)
to generate a pseudo infrared image taking
a visible image as input. Benefiting from the favorable
geometry preservation ability of the CPSTN,
the generated pseudo infrared image embraces
a sharp structure, which is more conducive
to transforming cross-modality image alignment
into mono-modality registration coupled with the
structure-sensitive of the infrared image. In this
case, we introduce a Multi-level Refinement Registration
Network (MRRN) to predict the displacement
vector field between distorted and pseudo infrared
images and reconstruct registered infrared
image under the mono-modality setting. Moreover,
to better fuse the registered infrared image and visible
images, we present a feature Interaction Fusion
Module (IFM) to adaptively select more meaningful
features for fusion in the Dual-path Interaction
Fusion Network (DIFN). Extensive experimental
results suggest that the proposed method
performs superior capability on misaligned crossmodality
image fusion.</span></p><br>

<div class="newline_10"></div>

</div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss621&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/zhu_ijcai_2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Hierarchical Bilevel Learning with Architecture and Loss Search for
Hadamard-based Image Restoration.</b><br>

      <p style="line-height:2em">Guijing Zhu, Long Ma, Xin Fan, <b>Risheng Liu*</b><br><strong><span style="font-size: 12px;"><strong>IJCAI <b style="color:red"></b></strong></span></strong><b>[<a href="https://github.com/zhudear/HBLALS">Code</>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss621" name="111ss621" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In the past few decades, Hadamard-based image
restoration problems (e.g., low-light image enhancement)
attract wide concerns in multiple areas
related to artificial intelligence. However, existing
works mostly focus on heuristically defining
efficient architecture and loss by the engineering
experiences that came from extensive practices.
This way brings about expensive verification costs
for seeking out the optimal solution. To this end,
we develop a novel hierarchical bilevel learning
scheme to discover the architecture and loss simultaneously
towards different Hadamard-based image
restoration tasks. More concretely, we first establish
a new Hadamard-inspired neural unit to aggregate
domain knowledge into the network design.
Then we model a triple-level optimization
that consists of the architecture, loss and parameters
optimizations, to deliver a macro perspective
for network learning. Then we introduce a new hierarchical
bilevel learning scheme for solving the
built triple-level model to progressively generate
the desired architecture and loss. We also define
an architecture search space consists of a series
of simple operations and an image quality-oriented
loss search space. Extensive experiments on three
Hadamard-based image restoration tasks (including
low-light image enhancement, image dehazing, and
underwater image enhancement) fully verify our
superiority against other state-of-the-art methods.</span></p><br>

<div class="newline_10"></div>

</div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss600&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/ma_CVPR_2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Toward Fast, Flexible, and Robust Low-Light Image Enhancement.</b><br>

      <p style="line-height:2em">Long Ma, Tengyu Ma, <b>Risheng Liu*</b>, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>CVPR <b style="color:red">(Oral, Acceptance Rate &le; 4.5%) </b></strong></span></strong><b>[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Toward_Fast_Flexible_and_Robust_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf">Paper</a>|<a href="https://github.com/vis-opt-group/SCI">Code</a>|<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Ma_Toward_Fast_Flexible_CVPR_2022_supplemental.pdf">Supplement Material</a>|<a href="https://replicate.com/vis-opt-group/sci">Online Demo</a>|<a href="https://mp.weixin.qq.com/s/sac_TOQA16rvpArNYEa7yA">Media Report (in Chinese)</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss600" name="111ss600" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Existing low-light image enhancement techniques are
mostly not only difficult to deal with both visual quality
and computational efficiency but also commonly invalid in
unknown complex scenarios. In this paper, we develop
a new Self-Calibrated Illumination (SCI) learning framework
for fast, flexible, and robust brightening images in
real-world low-light scenarios. To be specific, we establish
a cascaded illumination learning process with weight
sharing to handle this task. Considering the computational
burden of the cascaded pattern, we construct the selfcalibrated
module which realizes the convergence between
results of each stage, producing the gains that only use the
single basic block for inference (yet has not been exploited
in previous works), which drastically diminishes computation
cost. We then define the unsupervised training
loss to elevate the model capability that can adapt general
scenes. Further, we make comprehensive explorations to excavate
SCI' s inherent properties (lacking in existing works)
including operation-insensitive adaptability (acquiring stable
performance under the settings of different simple operations)
and model-irrelevant generality (can be applied to
illumination-based existing works to improve performance).
Finally, plenty of experiments and ablation studies fully indicate
our superiority in both quality and efficiency. Applications
on low-light face detection and nighttime semantic
segmentation fully reveal the latent practical values for SCI.
The source code will be made publicly available.</span></p><br>

<div class="newline_10"></div>

</div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss599&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/jinyuan_CVPR_2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection.</b><br>

      <p style="line-height:2em">Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, <b>Risheng Liu</b>, Wei Zhong, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>CVPR <b style="color:red">(Oral, Acceptance Rate &le; 4.5%)</b></strong></span></strong><b> [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Target-Aware_Dual_Adversarial_Learning_and_a_Multi-Scenario_Multi-Modality_Benchmark_To_CVPR_2022_paper.pdf">Paper</a>|<a href="https://github.com/JinyuanLiu-CV/TarDAL">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss599" name="111ss599" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">This study addresses the issue of fusing infrared and visible
images that appear differently for object detection. Aiming
at generating an image of high visual quality, previous
approaches discover commons underlying the two modalities
and fuse upon the common space either by iterative
optimization or deep networks. These approaches neglect
that modality differences implying the complementary information
are extremely important for both fusion and subsequent
detection task. This paper proposes a bilevel optimization
formulation for the joint problem of fusion and detection,
and then unrolls to a target-aware Dual Adversarial
Learning (TarDAL) network for fusion and a commonly
used detection network. The fusion network with one generator
and dual discriminators seeks commons while learning
from differences, which preserves structural information of
targets from the infrared and textural details from the visible.
Furthermore, we build a synchronized imaging system
with calibrated infrared and optical sensors, and collect
currently the most comprehensive benchmark covering
a wide range of scenarios. Extensive experiments on several
public datasets and our benchmark demonstrate that
our method outputs not only visually appealing fusion but
also averagely 10:9% higher detection mAP than the stateof-
the-art approaches on various challenging scenarios.</span></p><br>

<div class="newline_10"></div>

</div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss597&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/Yang_CVPR_2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Self-augmented Unpaired Image Dehazing via Density and Depth Decomposition.</b><br>

      <p style="line-height:2em">Yang Yang, Chaoyue Wang, <b>Risheng Liu</b>, Lin Zhang, Xiaojie Guo, Dacheng Tao<br><strong><span style="font-size: 12px;"><strong>CVPR</strong></span></strong><b> [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Self-Augmented_Unpaired_Image_Dehazing_via_Density_and_Depth_Decomposition_CVPR_2022_paper.pdf">Paper</a>|<a href="https://github.com/YaN9-Y/D4">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss597" name="111ss597" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">To overcome the overfitting issue of dehazing models
trained on synthetic hazy-clean image pairs, many recent
methods attempted to improve models?? generalization ability
by training on unpaired data. Most of them simply formulate
dehazing and rehazing cycles, yet ignore the physical
properties of the real-world hazy environment, i.e. the
haze varies with density and depth. In this paper, we propose
a self-augmented image dehazing framework, termed
D4 (Dehazing via Decomposing transmission map into
Density and Depth) for haze generation and removal. Instead
of merely estimating transmission maps or clean content,
the proposed framework focuses on exploring scattering
coefficient and depth information contained in hazy and
clean images. With estimated scene depth, our method is
capable of re-rendering hazy images with different thicknesses
which further benefits the training of the dehazing
network. It is worth noting that the whole training process
needs only unpaired hazy and clean images, yet succeeded
in recovering the scattering coefficient, depth map
and clean content from a single hazy image. Comprehensive
experiments demonstrate our method outperforms state-ofthe-
art unpaired dehazing methods with much fewer parameters
and FLOPs. Our code will be made publicly available.</span></p><br>

<div class="newline_10"></div>

</div>
</div>
</td></tr>



<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss598&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/Jiaqi_CVPR_2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Segment, Magnify and Reiterate: Detecting Camouflaged Objects the Hard Way.</b><br>

      <p style="line-height:2em">Qi Jia, Shuilian Yao, Yu Liu, Xin Fan, <b>Risheng Liu</b>, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>CVPR</strong></span></strong><b> [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jia_Segment_Magnify_and_Reiterate_Detecting_Camouflaged_Objects_the_Hard_Way_CVPR_2022_paper.pdf">Paper</a>|<a href="https://github.com/dlut-dimt/SegMaR">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss598" name="111ss598" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">It is challenging to accurately detect camouflaged objects
from their highly similar surroundings. Existing methods
mainly leverage a single-stage detection fashion, while
neglecting small objects with low-resolution fine edges requires
more operations than the larger ones. To tackle
camouflaged object detection (COD), we are inspired by
humans attention and the coarse-to-fine detection strategy,
and thereby propose an iterative refinement framework,
coined SegMaR, which integrates Segment, Magnify and
Reiterate in a multi-stage detection fashion. Specifically, we
design a new discriminative mask which makes the model
attend on the fixation and edge regions. In addition, we
leverage an attention-based sampler to magnify the object
region progressively with no need of enlarging the image
size. Extensive experiments show our SegMaR achieves remarkable
and consistent improvements over other state-ofthe-
art methods. Our performance on small objects surpasses
two competitive methods by 7.4% and 20.0% respectively
in average over standard evaluation metrics. Code is
available at supplementary material.</span></p><br>

<div class="newline_10"></div>

</div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss182&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/wangdi_ICRA_2022.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Semantic-aware Texture-Structure Feature Collaboration for
Underwater Image Enhancement.</b><br>

      <p style="line-height:2em">Di Wang, Long Ma, <b>Risheng Liu</b>, Xin Fan<br><strong><span style="font-size: 12px;"><strong>ICRA</strong></span></strong><b> </b></p></div>

  </div>

  <div class="publication_detail" id="111ss182" name="111ss182" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Underwater image enhancement has become an
attractive topic as a significant technology in marine engineering
and aquatic robotics. However, the limited number of
datasets and imperfect hand-crafted ground truth weaken its
robustness to unseen scenarios, and hamper the application
to high-level vision tasks. To address the above limitations,
we develop an efficient and compact enhancement network
in collaboration with a high-level semantic-aware pretrained
model, aiming to exploit its hierarchical feature representation
as an auxiliary for the low-level underwater image enhancement.
Specifically, we tend to characterize the shallow layer
features as textures while the deep layer features as structures in
the semantic-aware model, and propose a multi-path Contextual
Feature Refinement Module (CFRM) to refine features in
multiple scales and model the correlation between different
features. In addition, a feature dominative network is devised
to perform channel-wise modulation on the aggregated texture
and structure features for the adaptation to different feature
patterns of the enhancement network. Extensive experiments on
benchmarks demonstrate that the proposed algorithm achieves
more appealing results and outperforms state-of-the-art methods
by large margins. We also apply the proposed algorithm
to the underwater salient object detection task to reveal the
favorable semantic-aware ability for high-level vision tasks.</span></p><br>



</div>
</div>
</td></tr>


<tr valign="top"><!-----------------------------???━：：?????---------------------------------------------------->
<td id="layout-content" width="1200px">
<h1>2021</h1></td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss99&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/TPAMI_21.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A General Descent Aggregation Framework for Gradient-based Bi-level Optimization.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>,  Pan Mu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang<br><strong><span style="font-size: 12px;"><strong>IEEE TPAMI</strong></span></strong><b> [<a href="https://arxiv.org/abs/2102.07976">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss99" name="111ss99" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In recent years, a variety of gradient-based methods have been developed to solve Bi-Level Optimization (BLO) problems in
machine learning and computer vision areas. However, the theoretical correctness and practical effectiveness of these existing
approaches always rely on some restrictive conditions (e.g., Lower-Level Singleton, LLS), which could hardly be satisfied in real-world
applications. Moreover, previous literature only proves theoretical results based on their specific iteration strategies, thus lack a general
recipe to uniformly analyze the convergence behaviors of different gradient-based BLOs. In this work, we formulate BLOs from an
optimistic bi-level viewpoint and establish a new gradient-based algorithmic framework, named Bi-level Descent Aggregation (BDA), to
partially address the above issues. Specifically, BDA provides a modularized structure to hierarchically aggregate both the upper- and
lower-level subproblems to generate our bi-level iterative dynamics. Theoretically, we establish a general convergence analysis template
and derive a new proof recipe to investigate the essential theoretical properties of gradient-based BLO methods. Furthermore, this work
systematically explores the convergence behavior of BDA in different optimization scenarios, i.e., considering various solution qualities
(i.e., global/local/stationary solution) returned from solving approximation subproblems. Extensive experiments justify our theoretical
results and demonstrate the superiority of the proposed algorithm for hyper-parameter optimization and meta-learning tasks.</span></p><br>

<div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021generic,<br>
  title={A generic descent aggregation framework for gradient-based bi-level optimization},<br>
  author={Liu, Risheng and Mu, Pan and Yuan, Xiaoming and Zeng, Shangzhi and Zhang, Jin},<br>
  journal={arXiv preprint arXiv:2102.07976},<br>
  year={2021}<br>
}</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss13&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/gao_TPAMI.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>,  Jiaxin Gao, Jin Zhang, Deyu Meng, Zhouchen Lin<br><strong><span style="font-size: 12px;"><strong>IEEE TPAMI</strong></span></strong><b> [<a href="https://arxiv.org/pdf/2101.11517">Paper</a>|<a href="https://github.com/vis-opt-group/BLO">Project Page</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss13" name="111ss13" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Bi-Level Optimization (BLO) is originated from the area of economic game theory and then introduced into the optimization community. BLO is able to handle problems with a hierarchical structure, involving two levels of optimization tasks, where one task is nested inside the other. In machine learning and computer vision fields, despite the different motivations and mechanisms, a lot of complex problems, such as hyper-parameter optimization, multi-task and meta learning, neural architecture search, adversarial learning and deep reinforcement learning, actually all contain a series of closely related subproblms. In this paper, we first uniformly express these complex learning and vision problems from the perspective of BLO. Then we construct a best-response-based single-level reformulation and establish a unified algorithmic framework to understand and formulate mainstream gradient-based BLO methodologies, covering aspects ranging from fundamental automatic differentiation schemes to various accelerations, simplifications, extensions and their convergence and complexity properties. Last but not least, we discuss the potentials of our unified BLO framework for designing new algorithms and point out some promising directions for future research.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021investigating,<br>
  title={Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond},<br>
  author={Liu, Risheng and Gao, Jiaxin and Zhang, Jin and Meng, Deyu and Lin, Zhouchen},<br>
  journal={arXiv preprint arXiv:2101.11517},<br>
  year={2021}<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss1&#39;);">

    <div class="publication_image"><img src="./vog_files/1.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning Deformable Image Registration from Optimization: Perspective, Modules, Bilevel Training and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zi Li, Xin Fan, Chenying Zhao, Hao Huang, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TPAMI</strong></span></strong><b> [<a href="https://arxiv.org/pdf/2004.14557">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss1" name="111ss1" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Conventional deformable registration methods aim at solving an optimization model carefully designed on image pairs and
their computational costs are exceptionally high. In contrast, recent deep learning-based approaches can provide fast deformation
estimation. These heuristic network architectures are fully data-driven and thus lack explicit geometric constraints which are
indispensable to generate plausible deformations, e.g., topology-preserving. Moreover, these learning-based approaches typically pose
hyper-parameter learning as a black-box problem and require considerable computational and human effort to perform many training runs.
To tackle the aforementioned problems, we propose a new learning-based framework to optimize a diffeomorphic model via multi-scale
propagation. Specifically, we introduce a generic optimization model to formulate diffeomorphic registration and develop a series of
learnable architectures to obtain propagative updating in the coarse-to-fine feature space. Further, we propose a new bilevel self-tuned
training strategy, allowing efficient search of task-specific hyper-parameters. This training strategy increases the flexibility to various types
of data while reduces computational and human burdens. We conduct two groups of image registration experiments on 3D volume
datasets including image-to-atlas registration on brain MRI data and image-to-image registration on liver CT data. Extensive results
demonstrate the state-of-the-art performance of the proposed method with diffeomorphic guarantee and extreme efficiency. We also
apply our framework to challenging multi-modal image registration, and investigate how our registration to support the down-streaming
tasks for medical image analysis including multi-modal fusion and image segmentation.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021learning,<br>
  	title={Learning deformable image registration from optimization: perspective, modules, bilevel training and beyond},<br>
  	author={Liu, Risheng and Li, Zi and Fan, Xin and Zhao, Chenying and Huang, Hao and Luo, Zhongxuan},<br>
  	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},<br>
  	year={2021},<br>
  	publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss80&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/TOLF.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Task-Oriented Convex Bilevel Optimization with Latent Feasibility.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Long Ma, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang<br>
	  <strong><span style="font-size: 12px;">
	  <strong>IEEE TIP</strong></span></strong><b> [<a href="https://rsliu.tech/TOLF.html">Paper</a>]</b></p></div>

	   </b></p></div>

  </div>

  <div class="publication_detail" id="111ss80" name="111ss80" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">This paper firstly proposes a convex bilevel optimization paradigm to formulate and optimize popular learning
and vision problems in real-world scenarios. Different from conventional approaches, which directly design their iteration schemes based on given problem formulation, we introduce a task-oriented energy as our latent constraint which integrates richer task information. By explicitly re-characterizing the feasibility, we establish an efficient and flexible algorithmic framework to tackle convex models with both shrunken solution space and powerful auxiliary (based on domain knowledge and data distribution of the task). In theory, we present the convergence analysis of our latent feasibility re-characterization based numerical strategy. We also analyze the stability of the theoretical convergence under computational error perturbation. Extensive numerical experiments are conducted to verify our theoretical findings and evaluate the practical performance of our method on different applications.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
<!--@article{mu2021triple,<br>
  title={Triple-level Model Inferred Collaborative Network Architecture for Video Deraining},<br>
  author={Mu, Pan and Liu, Zhu and Liu, Yaohua and Liu, Risheng and Fan, Xin},<br>
  journal={IEEE Transactions on Image Processing},<br>
  year={2021},<br>
  publisher={IEEE}<br>
}--!>  </p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss3&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/mu_tip_1.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Triple-level Model Inferred Collaborative Network Architecture for Video Deraining.
</b><br>

      <p style="line-height:2em">Pan Mu, Zhu Liu, Yaohua Liu, <b>Risheng Liu*</b>, Xin Fan<br>
	  <strong><span style="font-size: 12px;">
	  <strong>IEEE TIP</strong></span></strong><b> 
	   [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9628137">Paper</a>|<a href="https://github.com/vis-opt-group/TMICS">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss3" name="111ss3" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Video deraining is an important issue for outdoor
vision systems and has been investigated extensively. However,
designing optimal architectures by the aggregating model formation and data distribution is a challenging task for video
deraining. In this paper, we develop a model-guided triplelevel optimization framework to deduce network architecture
with cooperating optimization and auto-searching mechanism, named Triple-level Model Inferred Cooperating Searching
(TMICS), for dealing with various video rain circumstances.
In particular, to mitigate the problem that existing methods
cannot cover various rain streaks distribution, we first design
a hyper-parameter optimization model about task variable and
hyper-parameter. Based on the proposed optimization model,
we design a collaborative structure for video deraining. This
structure includes Dominant Network Architecture (DNA) and
Companionate Network Architecture (CNA) that is cooperated
by introducing an Attention-based Averaging Scheme (AAS). To
better explore inter-frame information from videos, we introduce
a macroscopic structure searching scheme that searches from
Optical Flow Module (OFM) and Temporal Grouping Module
(TGM) to help restore latent frame. In addition, we apply
the differentiable neural architecture searching from a compact
candidate set of task-specific operations to discover desirable rain
streaks removal architectures automatically. Extensive experiments on various datasets demonstrate that our model shows
significant improvements in fidelity and temporal consistency
over the state-of-the-art works.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{mu2021triple,<br>
  title={Triple-level Model Inferred Collaborative Network Architecture for Video Deraining},<br>
  author={Mu, Pan and Liu, Zhu and Liu, Yaohua and Liu, Risheng and Fan, Xin},<br>
  journal={IEEE Transactions on Image Processing},<br>
  year={2021},<br>
  publisher={IEEE}<br>
}  </p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss70&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/TIP21.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Investigating Customization Strategies and Convergence Behaviors of Task-Specific ADMM.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Pan Mu, Jin Zhang<br>
	  <strong><span style="font-size: 12px;">
	  <strong>IEEE TIP</strong></span></strong><b> 
	   [<a href="https://ieeexplore.ieee.org/abstract/document/9547710/">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss70" name="111ss70" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Alternating Direction Method of Multiplier (ADMM)   has   been   a   popular   algorithmic   framework   forseparable  optimization  problems  with  linear  constraints.  Fornumerical  ADMM fail  to  exploit  the particular structure  of  theproblem at hand nor the input data information, leveraging task-specific  modules  (e.g.,  neural  networks  and  other  data-drivenarchitectures)  to  extend  ADMM  is  a  significant  but  challengingtask.   This   work   focuses   on   designing   a   flexible   algorithmic framework  to  incorporate  various  task-specific  modules  (withno   additional   constraints)   to   improve   the   performance   of ADMM   in   real-world   applications.   Specifically,   we   proposeGuidance  from  Optimality  (GO),  anew  customization  strategy,to   embed   task-specific   modules   into   ADMM   (GO-ADMM).By   introducing   an   optimality-based   criterion   to   guide   thepropagation,    GO-ADMM    establishes    anupdating    schemeagnostic   to   the   choice   of   additional   modules.   The   existing task-specific  methods  just  plug  their  task-specific  modules  intothe   numerical   iterations   in   a   straightforward   manner.   Evenwith  some  restrictive  constraints  on  the  plug-in  modules,  theycan  only  obtain  some  relatively  weaker  convergence  propertiesfor  the  resulted  ADMM  iterations.  Fortunately,  without  anyrestrictions on the embedded modules, we prove the convergenceof    GO-ADMM   regarding    objective    values    and   constraintviolations, and derive the worst-case  convergence  rate measuredby iteration complexity. Extensive experiments are conducted toverify  the  theoretical  results  and  demonstrate  the  efficiency  of GO-ADMM..</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021investigating,<br>
  title={Investigating customization strategies and convergence behaviors of task-specific admm},<br>
  author={Liu, Risheng and Mu, Pan and Zhang, Jin},<br>
  journal={IEEE Transactions on Image Processing},<br>
  volume={30},<br>
  pages={8278--8292},<br>
  year={2021},<br>
  publisher={IEEE}<br>
}</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss15&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/long_TNNLS2.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Underexposed Image Correction via Hybrid Priors Navigated Deep Propagation.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Long Ma, Jiaao Zhang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong><b> [<a href="https://arxiv.org/pdf/1907.07408">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss15" name="111ss15" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Enhancing visual qualities for underexposed imagesis  an  extensively  concerned  task  that  plays  important  roles  invarious areas of multimedia and computer vision. Most existingmethods  often  fail  to  generate  high-quality  results  with  appro-priate  luminance  and  abundant  details.  To  address  these  issues,we  in  this  work  develop  a  novel  framework,  integrating  bothknowledge  from  physical  principles  and  implicit  distributionsfrom  data  to  solve  the  underexposed  image  correction  task.More  concretely,  we  propose  a  new  perspective  to  formulatethis  task  as  an  energy-inspired  model  with  advanced  hybridpriors. A propagation procedure navigated by the hybrid priorsis  well  designed  for  simultaneously  propagating  the  reflectanceand  illumination  toward  desired  results.  We  conduct  extensiveexperiments to verify the necessity of integrating both underlyingprinciples (i.e., with knowledge) and distributions (i.e., from data)as navigated deep propagation. Plenty of experimental results ofunderexposed  image  correction  demonstrate  that  our  proposedmethod  performs  favorably  against  the  state-of-the-art  methodson  both  subjective  and  objective  assessments.  Additionally,  weexecute the task of face detection to further verify the naturalnessand  practical  value  of  underexposed  image  correction.  What'smore, we employ our method to single image haze removal whoseexperimental  results  further  demonstrate  its  superiorities.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021underexposed,<br>
  title={Underexposed Image Correction via Hybrid Priors Navigated Deep Propagation},<br>
  author={Liu, Risheng and Ma, Long and Zhang, Yuxi and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Neural Networks and Learning Systems},<br>
  year={2021},<br>
  publisher={IEEE}<br>
}
</p> </div>

    <div class="publication_links">  </div>

  </div>

</td>
</tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss6&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/long_TNNLS.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning Deep Context-Sensitive Decomposition for Low-Light Image Enhancement.</b><br>

      <p style="line-height:2em">Long Ma, <b>Risheng Liu*</b>, Jiaao Zhang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9420270">Paper</a>|<a href="https://github.com/KarelZhang/CSDNet-CSDGAN">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss6" name="111ss6" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Enhancing  the  quality  of  low-light  (LOL)  imagesplays  a   very   important  role  in  many   image   processing  andmultimedia applications. In recent years, a variety of deep learn-ing  techniques  have  been  developed  to  address  this  challengingtask.  A  typical  framework  is  tosimultaneously  estimate  theillumination  and  reflectance,  but  they  disregard  the  scene-levelcontextual  information  encapsulated  in  feature  spaces,  causingmany  unfavorable  outcomes,  e.g.,  details  loss,  color  unsatura-tion,  and  artifacts.  To  address  these  issues,  we  develop  a  newcontext-sensitive  decomposition  network  (CSDNet)  architectureto  exploit   the  scene-level   contextual   dependencies  on   spatialscales. More concretely, we build a two-stream estimation mech-anism including reflectance and illumination estimation network.We design a novel context-sensitive decomposition connection tobridge  the  two-stream  mechanism  by  incorporating  the  phys-ical  principle.  The  spatially  varying  illumination  guidance  isfurther  constructed  for  achieving  the  edge-aware  smoothnessproperty  of  the  illumination  component.  According  to  differenttraining patterns, we construct CSDNet (paired supervision) andcontext-sensitive  decomposition  generative  adversarial  network(CSDGAN) (unpaired supervision) to fully evaluate our designedarchitecture.  We  test  our  method  on  seven  testing  benchmarks[including  massachusetts  institute  of  technology  (MIT)-AdobeFiveK,  LOL,  ExDark,  and  naturalness  preserved  enhancement(NPE)] to conduct plenty of analytical and evaluated experiments.Thanks to our designed context-sensitive  decomposition connec-tion,  we  successfully  realized  excellent  enhanced  results  (withsufficient details,  vivid  colors,  and  few  noises),  which  fully indi-cates our superiority against existing state-of-the-art approaches.Finally,   considering   the   practical   needs   for   high   efficiency,we develop a lightweight CSDNet (named LiteCSDNet) by reduc-ing the number of channels. Furthermore, by sharing an encoderfor these two components, we obtain a more lightweight version(SLiteCSDNet  for  short).  SLiteCSDNet  just  contains  0.0301Mparameters  but  achieves  the  almost  same  performance  as  CSD-Net.Code  is  available  at  https://github.com/KarelZhang/CSDNet-CSDGAN.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{ma2021learning,<br>
  title={Learning deep context-sensitive decomposition for low-light image enhancement},<br>
  author={Ma, Long and Liu, Risheng and Zhang, Jiaao and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2021},<br>
  publisher={IEEE}<br>
}</p> </div>

    <div class="publication_links">  </div>

  </div>

</td>
</tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss20&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/jinyuan_TCSVT.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning a Deep Multi-scale Feature Ensemble and an Edge-attention Guidance for Image Fusion.</b><br>

      <p style="line-height:2em">Jinyuan Liu, Xin Fan, Ji Jiang, <b>Risheng Liu</b>, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TCSVT</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9349250">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss20" name="111ss20" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Image fusion integrates a series of images acquiredfrom   different   sensors,e.g.,   infrared   and   visible,   outputtingan  image  with  richer  information  than  either  one.  Traditionaland  recent  deep-based  methods  have  difficulties  in  preservingprominent  structures  and  recovering  vital  textural  details  forpractical applications. In this paper, we propose a deep networkfor infrared and visible image fusion cascading a feature learningmodule  with  a  fusion  learning  mechanism.  Firstly,  we  applya  coarse-to-fine  deep  architecture  to  learn  multi-scale  featuresfor  multi-modal  images,  which  enables  discovering  prominentcommon  structures  for  later  fusion  operations.  The  proposedfeature learning module requires no well-aligned image pairs fortraining.  Compared  with  the  existing  learning-based  methods,the  proposed  feature  learning  module  can  ensemble  numerousexamples  from  respective  modals  for  training,  increasing  theability  of  feature  representation.  Secondly,  we  design  an  edge-guided  attention  mechanism  upon  the  multi-scale  features  toguide the fusion focusing on common structures, thus recoveringdetails  while  attenuating  noise.  Moreover,  we  provide  a  newaligned  infrared  and  visible  image  fusion  dataset,  RealStreet,collected  in  various  practical  scenarios  for  comprehensive  eval-uation.  Extensive  experiments  on  two  benchmarks,  TNO  andRealStreet, demonstrate the superiority of the proposed methodover  the  state-of-the-art  in  terms  of  both  visual  inspection  andobjective analysis on six evaluation metrics. We also conduct theexperiments  on  the  FLIR  and  NIR  datasets,  containing  foggyweather  and  poor  light  conditions,  to  verify  the  generalizationand robustness of the proposed method.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021learning,<br>
  title={Learning a deep multi-scale feature ensemble and an edge-attention guidance for image fusion},<br>
  author={Liu, Jinyuan and Fan, Xin and Jiang, Ji and Liu, Risheng and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Circuits and Systems for Video Technology},<br>
  year={2021},<br>
  publisher={IEEE}<br>
}
</p> </div>

    <div class="publication_links">  </div>

  </div>

</td>
</tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss71&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/CVIU.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Convergent Framework with Learnable Feasibility for Hadamard-based Image Recovery.</b><br>

      <p style="line-height:2em">Yiyang Wang, Long Ma, <b>Risheng Liu</b><br><strong><span style="font-size: 12px;"><strong>CVIU</strong></span></strong><b> [<a href="https://www.sciencedirect.com/science/article/pii/S1077314220301259?casa_token=HObOEUzinGcAAAAA:CYEJdxkPC01baAXZBW7geayBValdF3jHTL2wa70ebnTRXbawGSHNIBYFK73NGCOvwTAo8AxCa0tz">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss71" name="111ss71" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In this paper, we propose a framework for recovering image degradations that can be formulated by theHadamard product of clear images with degradation factors. By training the mapping from datasets, weshow that implicit feasibilities can be learned in forms of latent domains. Then with the feasibilities andacknowledged data priors, the recovery problems are formulated as a general optimization model in whichthe domain knowledge of degradations are also nicely involved. Then we solve the model based on the classicalcoordinate update with plugged-in networks so that all the variables can be well estimated. Even better,our updating scheme is designed under the guidance of theoretical analyses, thus its stability can always beguaranteed in practice. We show that different recovery problems can be solved under our unified framework,and the extensive experimental results verify that the proposed framework is superior to state-of-the-artmethods in both benchmark datasets and real-world images.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{wang2021convergent,<br>
  title={A convergent framework with learnable feasibility for Hadamard-based image recovery},<br>
  author={Wang, Yiyang and Ma, Long and Liu, Risheng},<br>
  journal={Computer Vision and Image Understanding},<br>
  volume={202},<br>
  pages={103095},<br>
  year={2021},<br>
  publisher={Elsevier}<br>
}</p> </div>

    <div class="publication_links">  </div>

  </div>

</td>
</tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss14&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/long_CVPR.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Retinex-Inspired Unrolling With Cooperative Prior Architecture Search for Low-Light Image Enhancement.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Long Ma, Jiaao Zhang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>CVPR</strong></span></strong><b> [<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Retinex-Inspired_Unrolling_With_Cooperative_Prior_Architecture_Search_for_Low-Light_Image_CVPR_2021_paper.html">Paper</a>|<a href="http://dutmedia.org/RUAS/">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss14" name="111ss14" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Low-light image enhancement plays very important rolesin low-level vision areas. Recent works have built a greatdeal of deep learning models to address this task. Howev-er, these approaches mostly rely on significant architectureengineering and suffer from high computational burden.In this paper, we propose a new method, named Retinex-inspired Unrolling with Architecture Search (RUAS), to con-struct lightweight yet effective enhancement network forlow-light images in real-world scenario. Specifically, build-ing upon Retinex rule, RUAS first establishes models tocharacterize the intrinsic underexposed structure of low-light images and unroll their optimization processes to con-struct our holistic propagation structure. Then by design-ing a cooperative reference-free learning strategy to dis-cover low-light prior architectures from a compact searchspace, RUAS is able to obtain a top-performing image en-hancement network, which is with fast speed and requiresfew computational resources. Extensive experiments veri-fy the superiority of our RUAS framework against recent-ly proposed state-of-the-art methods. The project page isavailable at http://dutmedia.org/RUAS/.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{liu2021retinex,<br>
  title={Retinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement},<br>
  author={Liu, Risheng and Ma, Long and Zhang, Jiaao and Fan, Xin and Luo, Zhongxuan},<br>
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>
  pages={10561--10570},<br>
  year={2021}<br>
}</p> </div>

    <div class="publication_links">  </div>

  </div>

</td>
</tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss16&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/long_ACM.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Bridging the Gap between Low-Light Scenes: Bilevel Learning for Fast Adaptation.</b><br>

      <p style="line-height:2em">Dian Jin, Long Ma, <b>Risheng Liu*</b>, Xin Fan<br>
	  <strong><span style="font-size: 12px;">
	  <strong>ACM Multimedia</strong></span></strong><b> 
	   [<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475404?casa_token=GJC2_EvYXPMAAAAA:l-Q7Ilk6vAbAXWN_VnwNFWJ8h-HN6E2XsRuGshcW7FkJifNb1MbKn5emQGZ12xpqimgXIh_EkSDn0fs">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss16" name="111ss16" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Brightening low-light images of diverse scenes is a challenging but
widely concerned task in the multimedia community. Convolutional
Neural Networks (CNNs) based approaches mostly acquire the enhanced model by learning the data distribution from the specific
scenes. However, these works present poor adaptability (even fail)
when meeting real-world scenarios that never encountered before.
To conquer it, we develop a novel bilevel learning scheme for fast
adaptation to bridge the gap between low-light scenes. Concretely,
we construct a Retinex-induced encoder-decoder with an adaptive denoising mechanism, aiming at covering more practical cases.
Different from existing works that directly learn model parameters by using the massive data, we provide a new hyperparameter
optimization perspective to formulate a bilevel learning scheme
towards general low-light scenarios. This scheme depicts the latent
correspondence (i.e., scene-irrelevant encoder) and the respective
characteristic (i.e., scene-specific decoder) among different data distributions. Due to the expensive inner optimization, estimating the
hyper-parameter gradient exactly can be prohibitive, we develop an approximate hyper-parameter gradient method by introducing the
one-step forward approximation and finite difference approximation to ensure the high-efficient inference. Extensive experiments
are conducted to reveal our superiority against other state-of-theart methods. A series of analytical experiments are also executed
to verify our effectiveness.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{jin2021bridging,<br>
  title={Bridging the Gap between Low-Light Scenes: Bilevel Learning for Fast Adaptation},<br>
  author={Jin, Dian and Ma, Long and Liu, Risheng and Fan, Xin},<br>
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},<br>
  pages={2401--2409},<br>
  year={2021}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss2&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/mm_1.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Searching a Hierarchically
	Aggregated Fusion Architecture for
Fast Multi-Modality Image Fusion.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhu Liu, Jinyuan Liu, Xin Fan<br>
	  <strong><span style="font-size: 12px;">
	  <strong>ACM Multimedia</strong></span></strong><b> 
	   [<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475299">Paper</a>|<a href="https://github.com/LiuzhuForFun/Hierarchical-NAS-Image-Fusion">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss2" name="111ss2" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Multi-modality image fusion refers to generating a complementary
image that integrates typical characteristics from source images. In
recent years, we have witnessed the remarkable progress of deep
learning models for multi-modality fusion. Existing CNN-based
approaches strain every nerve to design various architectures for
realizing these tasks in an end-to-end manner. However, these handcrafted designs are unable to cope with the high demanding fusion
tasks, resulting in blurred targets and lost textural details. To alleviate these issues, in this paper, we propose a novel approach,
aiming at searching effective architectures according to various
modality principles and fusion mechanisms. Specifically, we construct a hierarchically aggregated fusion architecture to extract
and refine fused features from feature-level and object-level fusion
perspectives, which is responsible for obtaining complementary
target/detail representations. Then by investigating diverse effective practices, we composite a more flexible fusion-specific search
space. Motivated by the collaborative principle, we employ a new
search strategy with different principled losses and hardware constraints for sufficient discovery of components. As a result, we can
obtain a task-specific architecture with fast inference time. Extensive quantitative and qualitative results demonstrate the superiority
and versatility of our method against state-of-the-art methods.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">@inproceedings{liu2021searching,<br>
  title={Searching a Hierarchically Aggregated Fusion Architecture for Fast Multi-Modality Image Fusion},<br>
  author={Liu, Risheng and Liu, Zhu and Liu, Jinyuan and Fan, Xin},<br>
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},<br>
  pages={1600--1608},<br>
  year={2021}<br>
}</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss75&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rsliu_ACMMM21.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Underwater Species Detection using Channel Sharpening Attention.</b><br>

      <p style="line-height:2em">Lihao Jiang, Yi Wang, Qi Jia, Shengwei Xu. Yu Liu, Xin Fan, Haojie Li, <b>Risheng Liu</b>, Xinwei Xue, Ruili Wang<br>
	  <strong><span style="font-size: 12px;">
	  <strong>ACM Multimedia</strong></span></strong><b> 
	   [<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475563?casa_token=Qon1x6xsmhQAAAAA:wl_sxpo4_PLptBpoPp-t1HLRbZT7ekSgwxsSDpe59XiBiUb1GJfmvu4FqKD_-LZnas_GQJOzxUhvRjU">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss75" name="111ss75" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">
	Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">With the continuous exploration of marine resources, underwater
artificial intelligent robots play an increasingly important role in
the fish industry. However, the detection of underwater objects
is a very challenging problem due to the irregular movement of
underwater objects, the occlusion of sand and rocks, the diversity of
water illumination, and the poor visibility and low color contrast in
the underwater environment. In this article, we first propose a realworld underwater object detection dataset (UODD), which covers
more than 3K images of the most common aquatic products. Then
we propose Channel Sharpening Attention Module (CSAM) as a
plug-and-play module to further fuse high-level image information,
providing the network with the privilege of selecting feature maps.
Fusion of original images through CSAM can improve the accuracy
of detecting small and medium objects, thereby improving the overall detection accuracy. We also use Water-Net as a preprocessing
method to remove the haze and color cast in complex underwater
scenes, which shows a satisfactory detection result on small-sized objects. In addition, we use the class weighted loss as the training loss, which can accurately describe the relationship between
classification and precision of bounding boxes of targets, and the
loss function converges faster during the training process. Experimental results show that the proposed method reaches a maximum
AP of 50.1%, outperforming other traditional and state-of-the-art
detectors. In addition, our model only needs an average inference
time of 25.4 ms per image, which is quite fast and might suit the
real-time scenario.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{jiang2021underwater,<br>
  title={Underwater Species Detection using Channel Sharpening Attention},<br>
  author={Jiang, Lihao and Wang, Yi and Jia, Qi and Xu, Shengwei and Liu, Yu and Fan, Xin and Li, Haojie and Liu, Risheng and Xue, Xinwei and Wang, Ruili},<br>
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},<br>
  pages={4259--4267},<br>
  year={2021}<br>
}</p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss9&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/xuan_BVFIM.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang<br><strong><span style="font-size: 12px;"><strong>ICML</strong></span></strong><b> [<a href="https://proceedings.mlr.press/v139/liu21o.html">Paper</a>|<a href="https://github.com/vis-opt-group/BVFSM">Code</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss9" name="111ss9" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Bi-level optimization model is able to capture a wide range of complex learning tasks with practical interest. Due to the witnessed efficiency in solving bi-level programs, gradient-based methods have gained popularity in the machine learning community. In this work, we propose a new gradient-based solution scheme, namely, the Bi-level Value-Function-based Interior-point Method (BVFIM). Following the main idea of the log-barrier interior-point scheme, we penalize the regularized value function of the lower level problem into the upper level objective. By further solving a sequence of differentiable unconstrained approximation problems, we consequently derive a sequential programming scheme. The numerical advantage of our scheme relies on the fact that, when gradient methods are applied to solve the approximation problem, we successfully avoid computing any expensive Hessian-vector or Jacobian-vector product. We prove the convergence without requiring any convexity assumption on either the upper level or the lower level objective. Experiments demonstrate the efficiency of the proposed BVFIM on non-convex bi-level problems.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@InProceedings{pmlr-v139-liu21o,<br>
title = {A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization},<br>
author = {Liu, Risheng and Liu, Xuan and Yuan, Xiaoming and Zeng, Shangzhi and Zhang, Jin},<br>
booktitle = {Proceedings of the 38th International Conference on Machine Learning},<br>
pages = {6882--6892},<br>
year = {2021},<br>
editor = {Meila, Marina and Zhang, Tong},<br>
volume = {139},<br>
series = {Proceedings of Machine Learning Research},<br>
month = {18--24 Jul},<br>
publisher = {PMLR},<br>
pdf = {http://proceedings.mlr.press/v139/liu21o/liu21o.pdf},<br>
url = {https://proceedings.mlr.press/v139/liu21o.html},<br>
}</p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss11&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/yaohua_NeurIPS.png " width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Yaohua Liu, Shanghai Zeng, Jin Zhang<br><strong><span style="font-size: 12px;"><strong>NeurIPS <b style="color:red">(Spotlight, Acceptance Rate &le; 3%)</b></strong></span></strong><b> [<a href="https://arxiv.org/pdf/2110.00455.pdf">Paper</a>|<a href="https://github.com/vis-opt-group/IAPTT-GM">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss11" name="111ss11" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In recent years, Bi-Level Optimization (BLO) techniques have received extensive attentions from both learning and vision communities. A variety of BLO models in complex and practical tasks are of Non-convex follower structure in nature (a.k.a., without Lower-Level Convexity, LLC for short). However, this challenging class of BLOs is lack of developments on both efficient solution strategies and solid theoretical guarantees. In this work, we propose a new algorithmic framework, named Initialization Auxiliary and Pessimistic Trajectory Truncated Gradient Method (IAPTT-GM), to partially address the above issues. In particular, by introducing an auxiliary as initialization to guide the optimization dynamics and designing a pessimistic trajectory truncation operation, we construct a reliable approximate version of the original BLO in the absence of LLC hypothesis. Our theoretical investigations establish the convergence of solutions returned by IAPTT-GM towards those of the original BLO without LLC. As an additional bonus, we also theoretically justify the quality of our IAPTT-GM embedded with Nesterov's accelerated dynamics under LLC. The experimental results confirm both the convergence of our algorithm without LLC, and the theoretical findings under LLC.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2021towards,<br>  title={Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond},<br>  author={Liu, Risheng and Liu, Yaohua and Zeng, Shangzhi and Zhang, Jin},<br>  journal={Advances in Neural Information Processing Systems},<br>  year={2021}<br>}
</p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss12&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/yaohua_ICMEW.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">BOML: A Modularized Bilevel Optimization Library in Python for Meta Learning.</b><br>

      <p style="line-height:2em">Yaohua Liu, <B>Risheng Liu*</b><br><strong><span style="font-size: 12px;"><strong>ICME <b style="color:red">(Best Open Source Project Award)</b></strong></span></strong><b> [<a href="https://arxiv.org/pdf/2009.13357.pdf">Paper</a>|<a href="https://github.com/vis-opt-group/BOML">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss12" name="111ss12" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Meta-learning (a.k.a. learning to learn) has recently emerged as a promising paradigm for a variety of applications. There are now many meta-learning methods, each focusing on different modeling aspects of base and meta learners, but all can be (re)formulated as specific bilevel optimization problems. This work presents BOML, a modularized optimization library that unifies several meta-learning algorithms into a common bilevel optimization framework. It provides a hierarchical optimization pipeline together with a variety of iteration modules, which can be used to solve the mainstream categories of meta-learning methods, such as meta-feature-based and meta-initialization-based formulations. The library is written in Python and is available at https://github.com/dut-media-lab/BOML.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{liu2021boml,<br>  title={BOML: A Modularized Bilevel Optimization Library in Python for Meta Learning},<br>  author={Liu, Yaohua and Liu, Risheng},<br>  booktitle={2021 IEEE International Conference on Multimedia \& Expo Workshops (ICMEW)},<br>  year={2021},<br>  organization={IEEE}<br>}
</p> </div>
</div>
</td></tr>

<!--<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss72&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/zhu_suolue.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Collaborative Reflectance-and-Illumination Learning for High-Efficient Low-light Image Enhancement.</b><br>

      <p style="line-height:2em">Guijing Zhu, Long Ma, <b>Risheng Liu*</b>, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>ICME <b style="color:red">(Oral)</b></strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/abstract/document/9428268/">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss72" name="111ss72" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);"> In this paper, we settle the low-light image enhancement problem by developing a collaborative learning framework, which not only improves lightness and suppresses noises simultaneously but also with fast speed and requires few computational resources. The approach is inspired by the fact that reflectance
and illumination are highly correlated to satisfy the well-known Retinex decomposition principle. With this in mind, we establish a Reflectance-and-Illumination Collaborative (RIC) block to depict the compact physical relationship between reflectance and illumination. By cascading multiple RIC blocks, we obtain an end-to-end RICNet to interactively optimize these two components in a collaborative manner. Benefiting from the RIC block that integrates powerful task cues, RICNet just needs few parameters to simultaneously improve brightness and remove noises. Extensive experiments demonstrate our superiority against existing stateof-the-art methods. We also make meticulous analysis for the RIC block. The results reveal the rationality and effectiveness of our built mechanism.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{zhu2021collaborative, <br>
  title={Collaborative Reflectance-And-Illumination Learning For High-Efficient Low-Light Image Enhancement},<br>
  author={Zhu, Guijing and Ma, Long and Liu, Risheng and Fan, Xin and Luo, Zhongxuan},<br>
  booktitle={2021 IEEE International Conference on Multimedia and Expo (ICME)}, <br>
  pages={1--6},<br>
  year={2021},<br>
  organization={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss73&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/jinyuan_ICME.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">HALDeR: Hierarchical Attention-guided Learning with Detail-refinement for Multi-Exposure Image Fusion.</b><br>

      <p style="line-height:2em">Jinyuan Liu, JingJie Shang, <b>Risheng Liu</b>, Xin Fan<br><strong><span style="font-size: 12px;"><strong>ICME <b style="color:red">(Oral)</b></strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/abstract/document/9428192/>Paper</a>|<a href="https://github.com/JinyuanLiu-CV/HALDeR/">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss73" name="111ss73" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);"> Deep  learning  techniques  have  yielded  impressive  progressin  the  field  of  computational  imaging.   Existing  approach-es  ignore  designing  specific  constrain  on  illumination  oredges,  making  them  limited  in  handling  asymmetric  halosand  more  likely  to  generate  a  fusion  result  with  color  dis-crepancy  or  blurred  edges.    To  alleviate  these  issues,  wepropose a hierarchical attention-guided learning with detail-refinement, termed as HALDeR, to tackle the multi-exposurefusion (MEF) task in a coarse-to-fine manner.  Firstly, a hi-erarchical attention network is designed to produce a fusionresult by calculating well-exposed areas under different illu-mination.  Secondly, we develop a collaborative-refine mod-ule for preventing the missing details and correcting distortedcolor simultaneously.  Moreover, adversarial learning is em-ployed at end of our network,  which can effectively allevi-ate other remaining artifacts (e.g.,ringing effect and noises).Extensive quantitative and qualitative results on two publiclyavailable  datasets  demonstrate  that  our  HALDeR  performsfavorably  against  the  state-of-the-art  methods  in  generatingvivid color and faithful detail.  Source code will be availableathttps://github.com/JinyuanLiu-CV/HALDeR.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{liu2021halder,<br>
  title={Halder: Hierarchical Attention-Guided Learning with Detail-Refinement for Multi-Exposure Image Fusion},<br>
  author={Liu, Jinyuan and Shang, Jingjie and Liu, Risheng and Fan, Xin},<br>
  booktitle={2021 IEEE International Conference on Multimedia and Expo (ICME)},<br>
  pages={1--6},<br>
  year={2021},<br>
  organization={IEEE}<br>
}</p> </div>
</div>
</td></tr>--!>


<tr>
<td id="layout-content" width="1200px">
<h2>2020</h2>
<div class="publication_container" onclick="toggleDetails(&#39;111ss21&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/jinyuan_TIP.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Bilevel Integrated Model with Data-driven Layer Ensemble for Multi-modality Image Fusion.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Jinyuan Liu, Zhiying Jiang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TIP</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9293146">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss21" name="111ss21" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Image  fusion  plays  a  critical  role  in  a  variety  ofvision  and learning applications. Current fusion approaches  aredesigned  to  characterize  source  images,  focusing  on  a  certaintype  of  fusion  task  while  limited  in  a  wide  scenario.  More-over,  other  fusion  strategies  (i.e.,  weighted  averaging,  choose-max)   cannot   undertake   the   challenging   fusion   tasks,   whichfurthermore  leads  to  undesirable  artifacts  facilely  emerged  intheir  fused  results.  In  this  paper,  we  propose  a  generic  imagefusion  method  with  a  bilevel  optimization  paradigm,  targetingon multi-modality image fusion tasks. Corresponding alternationoptimization is conducted on certain components decoupled fromsource  images.  Via  adaptive  integration  weight  maps,  we  areable  to  get  the  flexible  fusion  strategy  across  multi-modalityimages. We successfully applied it to three types of image fusiontasks, including infrared and visible, computed tomography andmagnetic  resonance  imaging,  and  magnetic  resonance  imagingand single-photon emission computed tomography image fusion.Results highlight the performance and versatility of our approachfrom both  quantitative and  qualitative aspects.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020bilevel,<br>
  title={A bilevel integrated model with data-driven layer ensemble for multi-modality image fusion},<br>
  author={Liu, Risheng and Liu, Jinyuan and Jiang, Zhiying and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Image Processing},<br>
  volume={30},<br>
  pages={1261--1274},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss22&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TMI.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Deep Framework Assembling Principled Modules for CS-MRI: Unrolling Perspective, Convergence Behaviors, and Practical Modeling.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Yuxi Zhang, Shichao Cheng, Zhongxuan Luo, Xin Fan<br><strong><span style="font-size: 12px;"><strong>IEEE TMI</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9157934">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss22" name="111ss22" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Compressed  Sensing  Magnetic  ResonanceImaging (CS-MRI) significantly accelerates MR acquisitionat  a  sampling  rate  much  lower  than  the  Nyquist  crite-rion.  A  major  challenge  for  CS-MRI  lies  in  solving  theseverely ill-posed inverse problem to reconstruct aliasing-free MR images from the sparsek-space data. Conventionalmethods typically optimize an energy function, producingrestoration  of  high  quality,  but  their  iterative  numericalsolvers unavoidably bring extremely large time consump-tion.   Recent   deep   techniques  provide   fast   restorationby  either  learning  direct  prediction  to  final  reconstruc-tion or plugging learned modules into the energy optimizer.Nevertheless,  these  data-driven  predictors  cannot  guar-antee  the  reconstruction following  principled  constraintsunderlying  the  domain  knowledge  so  that  the  reliabilityof  their  reconstruction  process  is  questionable.  In  thispaper, we propose a deep framework assembling principledmodules for CS-MRI that fuses learning strategy with theiterative  solver  of  a  conventional  reconstruction  energy.This  framework  embeds  an  optimal  condition  checkingmechanism, fosteringefficientandreliablereconstruction.We also apply the framework to three practical tasks,i.e., complex-valued data reconstruction, parallel imaging andreconstruction with Rician noise. Extensive experiments onboth benchmark and manufacturer-testing images demon-strate that the proposed method reliably converges to theoptimal  solution more  efficiently and  accurately than  thestate-of-the-art in various scenarios.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020deep,<br>
  title={A Deep Framework Assembling Principled Modules for CS-MRI: Unrolling Perspective, Convergence Behaviors, and Practical Modeling},<br>
  author={Liu, Risheng and Zhang, Yuxi and Cheng, Shichao and Luo, Zhongxuan and Fan, Xin},<br>
  journal={IEEE Transactions on Medical Imaging},<br>
  volume={39},<br>
  number={12},<br>
  pages={4150--4163},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss23&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TNNLS.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Location-aware and Regularization-adaptive Correlation Filters for Robust Visual Tracking.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Qianru Chen, Yuansheng Yao, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9158561">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss23" name="111ss23" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Correlation filter (CF) has recently been widely usedfor visual tracking. The estimation of the search window and thefilter-learning strategies is the key component of the CF trackers.Nevertheless, prevalent CF models separately address these issuesin  heuristic  manners.  The  commonly  used  CF  models  directlyset  the  estimated  location  in  the  previous  frame  as  the  searchcenter  for  the  current  one.  Moreover,  these  models  usually  relyon  simple  and  fixed  regularization  for  filter  learning,  and  thus,their performance is compromised by the search window size andoptimization heuristics. To break these limits, this article proposesa  location-aware  and  regularization-adaptive  CF  (LRCF)  forrobust  visual  tracking.  LRCF  establishes  a  novel  bilevel  opti-mization model to address simultaneously the location-estimationand   filter-training  problems.  We   prove   that   our   bilevel  for-mulation  can  successfully  obtain  a  globally  converged  CF  andthe  corresponding  object  location  in  a  collaborative  manner.Moreover, based on the LRCF framework, we design two trackersnamed  LRCF-S  and  LRCF-SA  and  a  series  of  comparisons  toprove  the  flexibility  and  effectiveness  of  the  LRCF  framework.Extensive  experiments  on  different challenging benchmark datasets  demonstrate  that  our  LRCF  trackers  perform  favorablyagainst  the state-of-the-art  methods in practice.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020location,<br>
  title={Location-aware and regularization-adaptive correlation filters for robust visual tracking},<br>
  author={Liu, Risheng and Chen, Qianru and Yao, Yuansheng and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={32},<br>
  number={6},<br>
  pages={2430--2442},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss24&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/mu_TIP.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Investigating Task-driven Latent Feasibility for Nonconvex Image Modeling.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Pan Mu, Jian Chen, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TIP</strong></span></strong><b> [<a href="https://arxiv.org/pdf/1910.08242">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss24" name="111ss24" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Properly  modeling  latent  image  distributions  playsan important role in a variety of image-related vision problems.Most   exiting   approaches   aim   to   formulate   this   problem   asoptimization  models  (e.g.,  Maximum  A  Posterior,  MAP)  withhandcrafted  priors.  In  recent  years,  different  CNN  modules  arealso  considered  as  deep  priors  to  regularize  the  image  model-ing  process.  However,  these  explicit  regularization  techniquesrequire  deep  understandings  on  the  problem  and  elaboratelymathematical skills. In this work, we provide a new perspective,named   Task-driven   Latent   Feasibility   (TLF),   to   incorporatespecific  task  information  to  narrow  down  the  solution  spacefor  the  optimization-based  image  modeling  problem.  Thanks  tothe  flexibility  of  TLF,  both  designed  and  trained  constraintscan  be  embedded  into  the  optimization  process.  By  introducingcontrol mechanisms based on the monotonicity and boundednessconditions,  we  can  also  strictly  prove  the  convergence  of  ourproposed inference process. We demonstrate that different typesof image modeling problems, such as image deblurring and rainstreaks removals, can all be appropriately addressed within ourTLF framework. Extensive experiments also verify the theoreticalresults  and  show  the  advantages  of  our  method  against  existingstate-of-the-art  approaches.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020investigating,<br>
  title={Investigating task-driven latent feasibility for nonconvex image modeling},<br>
  author={Liu, Risheng and Mu, Pan and Chen, Jian and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Image Processing},<br>
  volume={29},<br>
  pages={7629--7640},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss25&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TCSVT20.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning Hadamard-Product-Propagation for Image Dehazing and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shiqi Li, Jinyuan Liu, Long Ma, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TCSVT</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9125952">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss25" name="111ss25" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Image   dehazing   has   evolved   into   an   attractiveresearch  field  in  the  computer  vision  community  in  the  pastfew  decades.  Previous  traditional  approaches  attempt  to  designenergy-based  objective  functions.  However,  they  cannot  accu-rately  express  the  intrinsic characteristics  of  the  images,  posingweak  adaptation  ability for  real-world  complex  scenarios.  Morerecently,   deep   learning   techniques   for   image   dehazing   havematured and become more reliable, showing outstanding perfor-mance.  Nevertheless,  these  methods  heavily  depend  on  trainingdata, restricting their application ranges. More importantly, bothtraditional  and  deep  learning  approaches  all  ignore  a  commonissue,  noises/artifacts  always  appear  in  the  recovery  process.To  this end, a  new Hadamard-Product (HP) model is proposed,which  consists  of  a  series  of  data-driven  priors.  Based  on  thismodel,  we  derive  a  Learnable  Hadamard-Product-Propagation(LHPP)  by  cascading  a  series  of  principle-inspired  guidanceand recovery modules. In which, the principle-inspired guidancerelated  to  transmission  is  endowed  the  smoothness  property,the  other  recovery  module  satisfies  the  distribution  of  naturalimages. The Hadamard-product-based propagations is generatedin  our  developed  learnable  framework  for  the  task  of  imagedehazing.  In  this  way,  we  can  eliminate  noises/artifacts  in  therecovery  procedure  to  obtain  the  ideal  outputs.  Subsequently,since the generality of our HP model, we successfully extend ourLHPP  to  settle  low-light  image  enhancement  and  underwaterimage enhancement problems. A series of analytical experimentsare performed to verify our effectiveness. Plenty of performanceevaluations  on  three  complex  tasks  fully  reveal  our  superiorityagainst  multiple state-of-the-art  methods.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020learning,<br>
  title={Learning Hadamard-Product-Propagation for Image Dehazing and Beyond},<br>
  author={Liu, Risheng and Li, Shiqi and Liu, Jinyuan and Ma, Long and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Circuits and Systems for Video Technology},<br>
  volume={31},<br>
  number={4},<br>
  pages={1366--1379},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss26&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TMM.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Dual Neural Networks Coupling Data Regression with Explicit Priors for Monocular 3D Face Reconstruction.</b><br>

      <p style="line-height:2em">Xin Fan, Shichao Cheng, Kang Huyan, Minjun Hou, <b>Risheng Liu</b>, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TMM</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9093203">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss26" name="111ss26" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">We address the challenging issue of reconstructinga  3D  face  from  one  single  image  under  various  expressionsand illuminations, which is widely applied in multimedia tasks.Methods   built   upon   classical   parametric   morphable   models(3DMMs) gain success on reconstructing the global geometry ofa  3D  face,  but  fail  to  precisely  characterize  local  facial  details.Recently,  deep  neural  networks  (DNN)  have  been  applied  tothe  reconstruction  that  directly  predicts  depth  maps,  showingcompelling performance on detail recovery. Unfortunately, theirreconstruction  is  prone  to  structural  distortions  owing  to  thelack of explicit prior constraints. In this paper, we propose dualneural  networks  that  optimize  one  energy  coupling  data  fittingwith  local  explicit  geometric  prior.  Specifically,  we  build  oneresidual network upon traditional convolution layers in order todirectly predict 3D structures by fitting an input image. Meanwhile,we  devise  a  novel  architecture  stacking  shallow  networks  torefine 3D clouds with geometric priors given by Markov randomfields (MRFs). Quantitative evaluations demonstrate the superiorperformance of the dual networks over either end-to-end DNNsor parametric models. Comparisons with the state-of-the-art alsoshow competitive reconstruction quality on various conditions.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{fan2020dual,<br>
  title={Dual neural networks coupling data regression with explicit priors for monocular 3D face reconstruction},<br>
  author={Fan, Xin and Cheng, Shichao and Huyan, Kang and Hou, Minjun and Liu, Risheng and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Multimedia},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss27&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TCSVT20_2.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Real-world Underwater Enhancement: Challenges, Benchmarks, and Solutions under Natural Light.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xin Fan, Ming Zhu, Minjun Hou, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TCSVT</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8949763">Paper</a>|<a href="https://github.com/dlut-dimt/Realworld-Underwater-Image-Enhancement-RUIE-Benchmark">Data</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss27" name="111ss27" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Underwater  image  enhancement  is  such  an  impor-tant low-level vision task with many applications that numerousalgorithms have been proposed in recent years. These algorithmsdeveloped upon various assumptions demonstrate successes fromvarious  aspects  usingdifferentdata  sets  anddifferentmetrics.In   this   work,   we   setup   an   undersea   image   capturing   sys-tem,  and  construct  a  large-scaleReal-world  Underwater  ImageEnhancement(RUIE)  data  set  divided  into  three  subsets.  Thethree  subsets  target  at  three  challenging  aspects  for  enhance-ment,  i.e.,  image  visibility  quality,  color  casts,  and  higher-leveldetection/classification,  respectively.  We  conduct  extensive  andsystematic experiments on RUIE to evaluate the effectiveness andlimitations of various algorithms to enhance visibility and correctcolor casts on images with hierarchical categories of degradation.Moreover,  underwater  image  enhancement  in  practice  usuallyserves as a preprocessing step for mid-level and high-level visiontasks.   We   thus   exploit   the   object   detection   performance   onenhanced  images  as  a  brand  newtask-specificevaluation  crite-rion. The findings from these evaluations not only confirm whatis  commonly  believed,  but  also  suggest  promising  solutions  andnew  directions  for  visibility  enhancement,  color  correction,  andobject  detection  on  real-world  underwater  images.  The  bench-mark   is   available   at:   https://github.com/dlut-dimt/Realworld-Underwater-Image-Enhancement-RUIE-Benchmark.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2020real,<br>
  title={Real-world underwater enhancement: Challenges, benchmarks, and solutions under natural light},<br>
  author={Liu, Risheng and Fan, Xin and Zhu, Ming and Hou, Minjun and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Circuits and Systems for Video Technology},<br>
  volume={30},<br>
  number={12},<br>
  pages={4861--4875},<br>
  year={2020},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss10&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/lizi_IJCAI.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px"> Bi-level Probabilistic Feature Learning for Deformable Image Registration.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zi Li, Yuxi Zhang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IJCAI</strong></span></strong><b> [<a href="https://www.ijcai.org/proceedings/2020/101">Paper</a>|<a href="https://github.com/Alison-brie/BiLevelReg">Code</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss10" name="111ss10" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">We address the challenging issue of deformable registration that robustly and efficiently builds dense correspondences between images. Traditional approaches upon iterative energy optimization typically invoke expensive computational load. Recent learning-based methods are able to efficiently predict deformation maps by incorporating learnable deep networks. Unfortunately, these deep networks are designated to learn deterministic features for classification tasks, which are not necessarily optimal for registration. In this paper, we propose a novel bi-level optimization model that enables jointly learning deformation maps and features for image registration. The bi-level model takes the energy for deformation computation as the upper-level optimization while formulates the maximum a posterior (MAP) for features as the lower-level optimization. Further, we design learnable deep networks to simultaneously optimize the cooperative bi-level model, yielding robust and efficient registration. These deep networks derived from our bi-level optimization constitute an unsupervised end-to-end framework for learning both features and deformations. Extensive experiments of image-to-atlas and image-to-image deformable registration on 3D brain MR datasets demonstrate that we achieve state-of-the-art performance in terms of accuracy, efficiency, and robustness.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{LiuLZFL20,<br>
  	title={Bi-level Probabilistic Feature Learning for Deformable Image Registration},<br>
  	author={Risheng Liu, Zi Li, Yuxi Zhang, Xin Fan and Zhongxuan Luo},<br>
  	booktitle={International Joint inproceedings on Artificial Intelligence},<br>
  	year={2020},<br>
  	pages={723--730},<br>
}</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss100&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rsliu_ijcai2020.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px"> Optimization Learning: Perspective, Method, and Applications.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b><br><strong><span style="font-size: 12px;"><strong>IJCAI</strong></span></strong><b> [<a href="https://www.ijcai.org/proceedings/2020/101">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss100" name="111ss100" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Numerous  tasks  at  the  core  of  statistics,  learn-ing,  and  vision  areas  are  specific  cases  of  ill-posed inverse problems.  Recently, learning-based(e.g., deep) iterative methods have been empiricallyshown to be useful for these problems. Nevertheless,integrating learnable structures into iterations is stilla laborious process, which can only be guided byintuitions or empirical insights. Moreover, there isa lack of rigorous analysis of the convergence be-haviors of these reimplemented iterations, and thusthe significance of such methods is a little bit vague.We move beyond these limits and propose a theoret-ically guaranteed optimization learning paradigm,a generic and provable paradigm for nonconvex in-verse problems, and develop a series of convergentdeep models. Our theoretical analysis reveals thatthe proposed optimization learning paradigm allowsus to generate globally convergent trajectories forlearning-based  iterative  methods.   Thanks  to  thesuperiority of our framework, we achieve state-of-the-art performance on different real applications.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{liu2021optimization,<br>
  title={Optimization learning: perspective, method, and applications},
  author={Liu, Risheng},<br>
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},<br>
  pages={5146--5168},<br>
  year={2021}<br>
}</p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss29&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_ICML20.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Generic First-Order Algorithmic Framework for Bi-Level Programming Beyond Lower-Level Singleton.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang<br><strong><span style="font-size: 12px;"><strong>ICML</strong></span></strong><b> [<a href="http://proceedings.mlr.press/v119/liu20l/liu20l.pdf">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss29" name="111ss29" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In recent years,  a variety of gradient-based bi-level optimization methods have been developedfor learning tasks.  However, theoretical guaran-tees of these existing approaches often heavily re-ly on the simplification that for each fixed upper-level variable, the lower-level solution must bea singleton (a.k.a., Lower-Level Singleton, LL-S). In this work, by formulating bi-level modelsfrom the optimistic viewpoint and aggregatinghierarchical objective information, we establishBi-level Descent Aggregation (BDA), a flexibleand modularized algorithmic framework for bi-level programming.  Theoretically, we derive anew methodology to prove the convergence ofBDA without the LLS condition.  Furthermore,we improve the convergence properties of conven-tional first-order bi-level schemes (under the LLSsimplification) based on our proof recipe. Exten-sive experiments justify our theoretical results anddemonstrate the superiority of the proposed BDAfor different tasks, including hyper-parameter op-timization and meta learning.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{liu2020generic,<br>
  title={A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton},<br>
  author={Liu, Risheng and Mu, Pan and Yuan, Xiaoming and Zeng, Shangzhi and Zhang, Jin},<br>
  booktitle={International Conference on Machine Learning},<br>
  pages={6305--6315},<br>
  year={2020},<br>
  organization={PMLR}<br>
}
</p> </div>
</div>
</td></tr>

<!--<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss110&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_CVPR20.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Wasserstein Loss based Deep Object Detection.</b><br>

      <p style="line-height:2em">Yuzhuo Han, Xiaofeng Liu, Zhenfei Sheng, Yutao Ren, Xu Han, Jane You, <b>Risheng Liu</b>, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>CVPR</strong></span></strong><b> [<a href="http://openaccess.thecvf.com/content_CVPRW_2020/papers/w60/Han_Wasserstein_Loss-Based_Deep_Object_Detection_CVPRW_2020_paper.pdf">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss110" name="111ss110" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Object detection locates the objects with bounding boxesand identifies their classes, which is valuable in many com-puter vision applications (e.g. autonomous driving). Mostexisting deep learning-based methods output a probabilityvector for instance classification trained with the one-hotlabel. However, the limitation of these models lies in at-tribute perception because they do not take the severity ofdifferent misclassifications into consideration. In this pa-per, we propose a novel method based on the Wassersteindistance called Wasserstein Loss based Model for ObjectDetection (WLOD). Different from the commonly used dis-tance metric such as cross-entropy (CE), the Wassersteinloss assigns different weights for one sample identified todifferent classes with different values. Our distance metricis designed by combining the CE or binary cross-entropy(BCE) with Wasserstein distance to learn the detector con-sidering both the discrimination and the seriousness of dif-ferent misclassifications. The misclassified objects are iden-tified to similar classes with a higher probability to reduceintolerable misclassifications. Finally, the model is testedon the BDD100K and KITTI datasets and reaches state-of-the-art performance.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{han2020wasserstein,<br>
  title={Wasserstein loss-based deep object detection},<br>
  author={Han, Yuzhuo and Liu, Xiaofeng and Sheng, Zhenfei and Ren, Yutao and Han, Xu and You, Jane and Liu, Risheng and Luo, Zhongxuan},<br>
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},<br>
  pages={998--999},<br>
  year={2020}<br>
}</p> </div>
</div>
</td></tr>--!>


<tr>
<td id="layout-content" width="1200px">
<h2>2019</h2>
<div class="publication_container" onclick="toggleDetails(&#39;111ss30&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TIP19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning Converged Propagations with Deep Prior Ensemble for Image Enhancement.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Long Ma, Yiyang Wang, Lei Zhang<br><strong><span style="font-size: 12px;"><strong>IEEE TIP</strong></span></strong><b> [<a href="https://arxiv.org/pdf/1810.04012">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss30" name="111ss30" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Enhancing   visual   qualities   of   images   plays   veryimportant   roles   in   various   vision   and   learning   applications.In   the   past   few   years,   both   knowledge-driven   maximum   aposterior (MAP) with prior modelings and fully data-dependentconvolutional  neural  network  (CNN)  techniques  have  been  in-vestigated  to  address  specific  enhancement  tasks.  In  this  paper,by  exploiting  the  advantages  of  these  two  types  of  mechanismswithin  a  complementary  propagation  perspective,  we  proposea  unified  framework,  named  deep  prior  ensemble  (DPE),  forsolving  various  image  enhancement  tasks.  Specifically,  we  firstestablish the basic propagation scheme based on the fundamentalimage   modeling   cues   and   then   introduce   residual   CNNs   tohelp   predicting   the   propagation   direction   at   each   stage.   Bydesigning  prior  projections  to  perform  feedback  control,  wetheoretically  prove  that  even  with  experience-inspired  CNNs,DPE  is  definitely  converged  and  the  output  will  always  satisfyour  fundamental  task  constraints.  The  main  advantage  againstconventional  optimization-based  MAP  approaches  is  that  ourdescent directions are learned from collected training data, thusare  much  more  robust  to  unwanted  local  minimums.  While,compared with existing CNN type networks, which are often de-signed in heuristic manners without theoretical guarantees, DPEis able to gain advantages from rich task cues investigated on thebases of domain knowledges. Therefore, DPE actually provides ageneric  ensemble  methodology  to  integrate  both  knowledge  anddata-based  cues  for  different  image  enhancement  tasks.  Moreimportantly,  our  theoretical  investigations  verify  that  the  feed-forward  propagations  of  DPE  are  properly  controlled  towardour  desired  solution.  Experimental  results  demonstrate  that  theproposed DPE outperforms state-of-the-arts on a variety of imageenhancement  tasks  in  terms  of  both  quantitative  measure  andvisual  perception  quality.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2019learning,<br>
  title={Learning Converged Propagations With Deep Prior Ensemble for Image Enhancement},<br>
  author={Liu, Risheng and Ma, Long and Wang, Yiyang and Zhang, Lei},<br>
  journal={IEEE Transactions on Image Processing},<br>
  volume={28},<br>
  number={3},<br>
  pages={1528--1543},<br>
  year={2019},<br>
  publisher={IEEE}<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss31&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TIP19_2.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Deep Proximal Unrolling: Algorithmic Framework, Convergence Analysis and Applications.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shichao Cheng, Long Ma, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TIP</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8704990">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss31" name="111ss31" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Deep  learning  models  have  gained  great  successin  many  real-world  applications.  However,  most  existing  net-works  are  typically  designed  in  heuristic  manners,  thus  theseapproaches  lack  rigorous  mathematical  derivations  and  clearinterpretations.  Several  recent  studies  try  to  build  deep  modelsby  unrolling  a  particular  optimization  model  that  involves  taskinformation. Unfortunately, due to the dynamic nature of networkparameters, their resultant deep propagations do not possess thenice  convergence  property  as  the  original  optimization  schemedoes.  In  this  work,  we  develop  a  generic  paradigm  to  unrollnonconvex  optimization  for  deep  model  design.  Different  frommost  existing  frameworks,  which  just  replace  the  iterations  bynetwork  architectures,  we  prove  in  theory  that  the  propagationgenerated  by  our  proximally  unrolled  deep  model  can  globallyconverge to the critical-point of the original optimization model.Moreover, even if the task information is only partially available(e.g.,   no   prior   regularization),   we   can   still   train   convergentdeep  propagations.  We  also  extend  these  theoretical  investiga-tions  on  the  more  general  multi-block  models  and  thus  a  lotof  real-world  applications  can  be  successfully  handled  by  theproposed framework. Finally, we conduct experiments on variouslow-level vision tasks (i.e., non-blind deconvolution, dehazing, andlow-light image enhancement) and demonstrate the superiority ofour proposed framework, compared with existing state-of-the-artapproaches.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2019deep,<br>
  title={Deep proximal unrolling: Algorithmic framework, convergence analysis and applications},<br>
  author={Liu, Risheng and Cheng, Shichao and Ma, Long and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE Transactions on Image Processing},<br>
  volume={28},<br>
  number={10},<br>
  pages={5013--5026},<br>
  year={2019},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss33&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TPAMI19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">On the Convergence of Learning-based Iterative Methods for Nonconvex Inverse Problems.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shichao Cheng, Yi He, Xin Fan, Zhouchen Lin, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TPAMI</strong></span></strong><b> [<a href="https://arxiv.org/pdf/1808.05331">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss33" name="111ss33" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Numerous tasks at the core of statistics, learning and vision areas are specific cases of ill-posed inverse problems. Recently,learning-based (e.g., deep) iterative methods have been empirically shown to be useful for these problems. Nevertheless, integratinglearnable structures into iterations is still a laborious process, which can only be guided by intuitions or empirical insights. Moreover, thereis a lack of rigorous analysis about the convergence behaviors of these reimplemented iterations, and thus the significance of suchmethods is a little bit vague. This paper moves beyond these limits and proposes Flexible Iterative Modularization Algorithm (FIMA), ageneric and provable paradigm for nonconvex inverse problems. Our theoretical analysis reveals that FIMA allows us to generate globallyconvergent trajectories for learning-based iterative methods. Meanwhile, the devised scheduling policies on flexible modules should alsobe beneficial for classical numerical methods in the nonconvex scenario. Extensive experiments on real applications verify the superiorityof FIMA.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2019convergence,<br>
  title={On the convergence of learning-based iterative methods for nonconvex inverse problems},<br>
  author={Liu, Risheng and Cheng, Shichao and He, Yi and Fan, Xin and Lin, Zhouchen and Luo, Zhongxuan},<br>
  journal={IEEE transactions on pattern analysis and machine intelligence},<br>
  volume={42},<br>
  number={12},<br>
  pages={3027--3039},<br>
  year={2019},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss36&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TNNLS19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Learning Aggregated Transmission Propagation Networks for Haze Removal and Beyond.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xin Fan, Minjun Hou, Zhiying Jiang, Zhongxuan Luo, Lei Zhang<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong><b> [<a href="https://arxiv.org/pdf/1711.06787">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss36" name="111ss36" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Single  image  dehazing  is  an  important  low-levelvision task with many applications. Early researches have inves-tigated  different  kinds  of  visual  priors  to  address  this  problem.However,  they  may  fail  when  their  assumptions  are  not  validon specific images. Recent deep  networks also achieve relativelygood  performance  in  this  task.  But  unfortunately,  due  to  thedisappreciation of rich physical rules in hazes, large amounts ofdata are required for their training. More importantly, they maystill fail when there exist completely different haze distributionsin testing images. By considering the collaborations of these twoperspectives,  this  paper  designs  a  novel  residual  architecture  toaggregate both prior (i.e., domain knowledge) and data (i.e., hazedistribution) information to propagate transmissions for scene ra-diance estimation. We further present a variational energy basedperspective  to  investigate  the  intrinsic  propagation  behavior  ofour  aggregated  deep  model.  In  this  way,  we  actually  bridge  thegap between prior driven models and data driven networks andleverage  advantages  but  avoid  limitations  of  previous  dehazingapproaches.  A  lightweight  learning  framework  is  proposed  totrain  our  propagation  network.  Finally,  by  introducing  a  task-aware image separation formulation with a flexible optimizationscheme,  we  extend  the  proposed  model  for  more  challengingvision tasks, such as underwater image enhancement and singleimage  rain  removal.  Experiments  on  both  synthetic  and  real-world images demonstrate the effectiveness and efficiency of theproposed  framework.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2019learning,<br>
  title={Learning Aggregated Transmission Propagation Networks for Haze Removal and Beyond},<br>
  author={Liu, Risheng and Fan, Xin and Hou, Minjun and Jiang, Zhiying and Luo, Zhongxuan and Zhang, Lei},<br>
  journal={IEEE transactions on neural networks and learning systems},<br>
  volume={30},<br>
  number={10},<br>
  pages={2973--2986},<br>
  year={2019}<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss28&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_TNNLS20_2.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Knowledge-driven Deep Unrolling for Robust Image Layer Separation.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhiying Jiang, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8760253">Paper</a>]</b></p></div>

  </div>

  <div class="publication_detail" id="111ss28" name="111ss28" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Single-image layer separation targets to decomposethe observed image into two independent components in terms ofdifferent application demands. It is known that many vision andmultimedia  applications  can  be  (re)formulated  as  a  separationproblem.  Due  to  the  fundamentally  ill-posed  natural  of  theseseparations,  existing  methods  are  inclined  to  investigate  modelpriors  on  the  separated  components  elaborately.  Nevertheless,it is knotty to optimize the cost function with complicated modelregularizations.  Effectiveness  is  greatly  conceded  by  the  settlediteration mechanism, and the adaption cannot be guaranteed dueto the poor data fitting. What is more, for a universal framework,the  most  taxing  point  is  that  one  type  of  visual  cue  cannot  beshared  with  different  tasks.  To  partly  overcome  the  weaknessesmentioned earlier, we delve into a generic optimization unrollingtechnique  to  incorporate  deep  architectures  into  iterations  foradaptive  image  layer  separation.  First,  we  propose  a  generalenergy model with implicit priors, which is based on maximum aposterior, and employ the extensively accepted alternating direc-tion method of  multiplier to determine our elementary iterationmechanism. By  unrolling with one general  residual architectureprior  and  one  task-specific  prior,  we  attain  a  straightforward,flexible,  and  data-dependent  image  separation  framework  suc-cessfully. We apply our method to four different tasks, includingsingle-image-rain streak removal, high-dynamic-range tone map-ping,  low-light  image  enhancement,  and  single-image  reflectionremoval.  Extensive  experiments  demonstrate  that  the  proposedmethod is applicable to multipletasks and outperforms the stateof the arts by a  large  margin  qualitatively and quantitatively.</span></p><br>


      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2019knowledge,<br>
  title={Knowledge-driven deep unrolling for robust image layer separation},<br>
  author={Liu, Risheng and Jiang, Zhiying and Fan, Xin and Luo, Zhongxuan},<br>
  journal={IEEE transactions on neural networks and learning systems},<br>
  volume={31},<br>
  number={5},<br>
  pages={1653--1666},<br>
  year={2019},<br>
  publisher={IEEE}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss54&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/TCSVT19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Toward Efficient Image Representation: Sparse Concept Discriminant Matrix Factorization.
</b><br>

      <p style="line-height:2em">Meng Pang, Yiu-ming Cheung, <b>Risheng Liu</b>, Jian Lou, Chuang Lin<br><strong><span style="font-size: 12px;"><strong>IEEE TCSVT</strong></span></strong><b> [<a href="https://ieeexplore.ieee.org/abstract/document/8525292/">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss54" name="111ss54" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">The key ingredients of matrix factorization lie in
basic learning and coefficient representation. To enhance the
discriminant ability of the learned basis, discriminant graph
embedding is usually introduced in the matrix factorization
model. However, the existing matrix factorization methods based
on graph embedding generally conduct discriminant analysis
via a single type of adjacency graph, either similarity-based
graphs (e.g., Laplacian eigenmaps graph) or reconstruction-based
graphs (e.g., L1-graph), while ignoring the cooperation of the
different types of adjacency graphs that can better depict the
discriminant structure of original data. To address the above
issue, we propose a novel Fisher-like criterion, based on graph
embedding, to extract sufficient discriminant information via two
different types of adjacency graphs. One graph preserves the
reconstruction relationships of neighboring samples in the same
category, and the other suppresses the similarity relationships of
neighboring samples from different categories. Moreover, we also
leverage the sparse coding to promote the sparsity of the coefficients. By virtue of the proposed Fisher-like criterion and sparse
coding, a new matrix factorization framework called Sparse
concept Discriminant Matrix Factorization (SDMF) is proposed
for efficient image representation. Furthermore, we extend the
Fisher-like criterion to an unsupervised context, thus yielding
an unsupervised version of SDMF. Experimental results on
seven benchmark datasets demonstrate the effectiveness and
efficiency of the proposed SDMFs on both image classification
and clustering tasks.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{pang2019toward,<br>
  title={Toward Efficient Image Representation: Sparse Concept Discriminant Matrix Factorization},<br>
  author={Pang, Meng and Cheung, Yiu-Ming and Liu, Risheng and Lou, Jian and Lin, Chuang},<br>
  journal={IEEE Transactions on Circuits and Systems for Video Technology},<br>
  volume={29},<br>
  number={11},<br>
  pages={3184--3198},<br>
  year={2019},<br>
  publisher={IEEE}<br>
}</p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss51&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/PR19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Robust Heterogeneous Discriminative Analysis for Face Recognition with Single Sample per Person.</b><br>

      <p style="line-height:2em">Meng Pang, Yiu-ming Cheung, Binghui Wang, <b>Risheng Liu</b><br><strong><span style="font-size: 12px;"><strong>Pattern Recognition</strong></span></strong><b> [<a href="https://www.comp.hkbu.edu.hk/~ymc/papers/journal/PR-yr2019.pdf">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss51" name="111ss51" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Single sample per person face recognition is one of the most challenging problems in face recognition
(FR), where only single sample per person (SSPP) is enrolled in the gallery set for training. Although the
existing patch-based methods have achieved great success in FR with SSPP, they still have limitations
in feature extraction and identification stages when handling complex facial variations. In this work, we
propose a new patch-based method called Robust Heterogeneous Discriminative Analysis (RHDA), for FR
with SSPP. To enhance the robustness against complex facial variations, we first present a new graphbased Fisher-like criterion, which incorporates two manifold embeddings, to learn heterogeneous discriminative representations of image patches. Specifically, for each patch, the Fisher-like criterion is able
to preserve the reconstruction relationship of neighboring patches from the same person, while suppressing the similarities between neighboring patches from the different persons. Then, we introduce
two distance metrics, i.e., patch-to-patch distance and patch-to-manifold distance, and develop a fusion
strategy to combine the recognition outputs of above two distance metrics via a joint majority voting for
identification. Experimental results on various benchmark datasets demonstrate the effectiveness of the
proposed method.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{pang2019robust,<br>
  title={Robust heterogeneous discriminative analysis for face recognition with single sample per person},<br>
  author={Pang, Meng and Cheung, Yiu-ming and Wang, Binghui and Liu, Risheng},<br>
  journal={Pattern Recognition},<br>
  volume={89},<br>
  pages={91--107},<br>
  year={2019},<br>
  publisher={Elsevier}<br>
}</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss300&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/JCST19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Blind Image Deblurring via Adaptive Optimization with Flexible Sparse Structure Control.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Caisheng Mao, Zhi-Hui Wang, Haojie Li<br><strong><span style="font-size: 12px;"><strong>JCST</strong></span></strong><b> [<a href="https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/content/pdf/10.1007/s11390-019-1930-z.pdf&casa_token=hpXOD8cDdc8AAAAA:6amEbXgsrR-UhmnXlaG5DwX3qxxHRh81iHn97tguxLxeW56gRRsBsDqVKjj-ntILnbtpHiEYf0QGZLtU-g">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss300" name="111ss300" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Blind image deblurring is a long-standing ill-posed inverse problem which aims to recover a latent sharp imagegiven only a blurry observation. So far, existing studies have designed many effective priors w.r.t. the latent image withinthe maximum a posteriori (MAP) framework in order to narrow down the solution space.  These non-convex priors arealways integrated into the final deblurring model, which makes the optimization challenging. However, due to unknownimage distribution, complex kernel structure and non-uniform noises in real-world scenarios, it is indeed challenging toexplicitly design a fixed prior for all cases.  Thus we adopt the idea of adaptive optimization and propose the sparsestructure control (SSC) for the latent image during the optimization process. In this paper, we only formulate the necessaryoptimization constraints in a lightweight MAP model with no priors. Then we develop an inexact projected gradient schemeto incorporate flexible SSC in MAP inference. Besideslp-norm based SSC in our previous work, we also train a group ofdenoising convolutional neural networks (CNNs) to learn the sparse image structure automatically from the training dataunder different noise levels, and we show that CNNs-based SSC can achieve similar results compared withlp-norm but aremore robust to noise. Extensive experiments demonstrate that the proposed adaptive optimization scheme with two typesof SSC achieves the state-of-the-art results on both synthetic data and real-world images.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@article{liu2019blind,<br>
  title={Blind Image Deblurring via Adaptive Optimization with Flexible Sparse Structure Control},<br>
  author={Liu, Ri-Sheng and Mao, Cai-Sheng and Wang, Zhi-Hui and Li, Hao-Jie},<br>
  journal={Journal of Computer Science and Technology},<br>
  volume={34},<br>
  number={3},<br>
  pages={609--621},<br>
  year={2019},<br>
  publisher={Springer}<br>
}}
</p> </div>
</div>
</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss60&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/ICCV19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Semi-supervised Skin Detection by Network with Mutual Guidances.</b><br>

      <p style="line-height:2em">Yi He, Jiayuan Shi, Chuan Wang, Haibin Huang, Jiaming Liu, Guanbin Li, <b>Risheng Liu</b>, Jue Wang<br><strong><span style="font-size: 12px;"><strong>ICCV</strong></span></strong><b> [<a href="http://openaccess.thecvf.com/content_ICCV_2019/html/He_Semi-Supervised_Skin_Detection_by_Network_With_Mutual_Guidance_ICCV_2019_paper.html">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss60" name="111ss60" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In this paper we present a new data-driven method for
robust skin detection from a single human portrait image.
Unlike previous methods, we incorporate human body as a
weak semantic guidance into this task, considering acquiring large-scale of human labeled skin data is commonly expensive and time-consuming. To be specific, we propose
a dual-task neural network for joint detection of skin and
body via a semi-supervised learning strategy. The dualtask network contains a shared encoder but two decoders
for skin and body separately. For each decoder, its output
also serves as a guidance for its counterpart, making both
decoders mutually guided. Extensive experiments were conducted to demonstrate the effectiveness of our network with
mutual guidance, and experimental results show our network outperforms the state-of-the-art in skin detection.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{wang2019asynchronous,<br>
  title={Asynchronous proximal stochastic gradient algorithm for composition optimization problems},<br>
  author={Wang, Pengfei and Liu, Risheng and Zheng, Nenggan and Gong, Zhefeng},<br>
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
  volume={33},<br>
  number={01},<br>
  pages={1633--1640},<br>
  year={2019}<br>
}</p> </div>
</div>
</td></tr>



<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss32&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_AAAI19.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Task Embedded Coordinate Update: A Realizable Framework for Multivariate Non-convex Optimization.
</b><br>

      <p style="line-height:2em">Yiyang Wang, <b>Risheng Liu*</b>, Long Ma, Xiaoliang Song.<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong><b> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/3981/3859">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss32" name="111ss32" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">We in this paper propose a realizable framework TECU, whichembeds task-specific strategies into update schemes of coordi-nate descent, for optimizing multivariate non-convex problemswith coupled objective functions. On one hand, TECU is ca-pable of improving algorithm efficiencies through embeddingproductive numerical algorithms, for optimizing univariatesub-problems with nice properties. From the other side, it alsoaugments probabilities to receive desired results, by embed-ding advanced techniques in optimizations of realistic tasks. In-tegrating both numerical algorithms and advanced techniquestogether, TECU is proposed in a unified framework for solvinga class of non-convex problems. Although the task embeddedstrategies bring inaccuracies in sub-problem optimizations, weprovide a realizable criterion to control the errors, meanwhile,to ensure robust performances with rigid theoretical analyses.By respectively embedding ADMM and a residual-type CNNin our algorithm framework, the experimental results verifyboth efficiency and effectiveness of embedding task-orientedstrategies in coordinate descent for solving practical problems.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{wang2019task,<br>
  title={Task embedded coordinate update: A realizable framework for multivariate non-convex optimization},<br>
  author={Wang, Yiyang and Liu, Risheng and Ma, Long and Song, Xiaoliang},<br>
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
  volume={33},<br>
  number={01},<br>
  pages={1650--1657},<br>
  year={2019}<br>
}
</p> </div>
</div>
</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss34&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_AAAI19_2.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">A Theoretically Guaranteed Deep Optimization Framework for Robust Compressive Sensing MRI.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Yuxi Zhang, Shichao Cheng, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong><b> [<a href="https://ojs.aaai.org/index.php/AAAI/article/download/4347/4225">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss34" name="111ss34" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Magnetic Resonance Imaging (MRI) is one of the most dy-namic and safe imaging techniques available for clinical ap-plications.  However,  the  rather  slow  speed  of  MRI  acqui-sitions  limits  the  patient  throughput  and  potential  indica-tions.  Compressive  Sensing  (CS)  has  proven  to  be  an  effi-cient technique for accelerating MRI acquisition. The mostwidely used CS-MRI model, founded on the premise of re-constructing an image from an incompletely filled k-space,leads to an ill-posed inverse problem. In the past years, lotsof efforts have been made to efficiently optimize the CS-MRImodel. Inspired by deep learning techniques, some prelimi-nary works have tried to incorporate deep architectures intoCS-MRI process. Unfortunately, the convergence issues (dueto  the  experience-based  networks)  and  the  robustness  (i.e.,lack real-world noise modeling) of these deeply trained opti-mization methods are still missing. In this work, we developa new paradigm to integrate designed numerical solvers andthe data-driven architectures for CS-MRI. By introducing anoptimal condition checking mechanism, we can successfullyprove the convergence of our established deep CS-MRI op-timization scheme. Furthermore, we explicitly formulate theRician noise distributions within our framework and obtainan extended CS-MRI network to handle the real-world nosiesin the MRI process. Extensive experimental results verify thatthe proposed paradigm outperforms the existing state-of-the-art techniques both in reconstruction accuracy and efficiencyas well as robustness to noises in real scene.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{liu2019theoretically,<br>
  title={A theoretically guaranteed deep optimization framework for robust compressive sensing mri},<br>
  author={Liu, Risheng and Zhang, Yuxi and Cheng, Shichao and Fan, Xin and Luo, Zhongxuan},<br>
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
  volume={33},<br>
  number={01},<br>
  pages={4368--4375},<br>
  year={2019}<br>
}
</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss35&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_AAAI19_3.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Exploiting Local Feature Patterns for Unsupervised Domain Adaptation.
</b><br>

      <p style="line-height:2em">Jun Wen, <b>Risheng Liu</b>, Nenggan Zheng, Qian Zheng, Zhefeng Gong, Junsong Yuan<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong><b> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/4479">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss35" name="111ss35" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">Unsupervised domain adaptation methods aim to alleviate performance degradation caused by domain-shift by learning domain-invariant representations. Existing deep domain adaptation methods focus on holistic feature alignment by
matching source and target holistic feature distributions, without considering local features and their multi-mode statistics. We show that the learned local feature patterns are more generic and transferable and a further local feature distribution matching enables fine-grained feature alignment. In this paper, we present a method for learning domain-invariant
local feature patterns and jointly aligning holistic and local feature statistics. Comparisons to the state-of-the-art unsupervised domain adaptation methods on two popular benchmark datasets demonstrate the superiority of our approach and its effectiveness on alleviating negative transfer.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{wen2019exploiting,<br>
  title={Exploiting local feature patterns for unsupervised domain adaptation},<br>
  author={Wen, Jun and Liu, Risheng and Zheng, Nenggan and Zheng, Qian and Gong, Zhefeng and Yuan, Junsong},<br>
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
  volume={33},<br>
  number={01},<br>
  pages={5401--5408},<br>
  year={2019}<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss50&#39;);">

    <div class="publication_image"><img src="./vog_files/Publication/rs_AAAI19_4.png" width="100" height="100"></div>

    <div class="publication_title"><b style="font-size:14px">Asynchronous Proximal Stochastic Gradient Algorithm for Composition Optimization Problems.</b><br>

      <p style="line-height:2em">Pengfei Wang, <b>Risheng Liu</b>, Nenggan Zheng, Zhefeng Gong<br><strong><span style="font-size: 12px;"><strong>AAAI</strong></span></strong><b> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/3979">Paper</a>]</b></p></div>

  </div>

<div class="publication_detail" id="111ss50" name="111ss50" style="display: none;">

    <div class="publication_abstract"><b style="font-size:15px">Abstract:</b><p style="text-align:justify;line-height:1.5em;"><span style="font-size: 15px; background-color: rgb(255, 255, 255);">In machine learning research, many emerging applications
can be (re)formulated as the composition optimization problem with nonsmooth regularization penalty. To solve this
problem, traditional stochastic gradient descent (SGD) algorithm and its variants either have low convergence rate or
are computationally expensive. Recently, several stochastic
composition gradient algorithms have been proposed, however, these methods are still inefficient and not scalable to
large-scale composition optimization problem instances. To
address these challenges, we propose an asynchronous parallel algorithm, named Async-ProxSCVR, which effectively
combines asynchronous parallel implementation and variance
reduction method. We prove that the algorithm admits the
fastest convergence rate for both strongly convex and general nonconvex cases. Furthermore, we analyze the query
complexity of the proposed algorithm and prove that linear
speedup is accessible when we increase the number of processors. Finally, we evaluate our algorithm Async-ProxSCVR
on two representative composition optimization problems including value function evaluation in reinforcement learning and sparse mean-variance optimization problem. Experimental results show that the algorithm achieves significant
speedups and is much faster than existing compared methods.</span></p><br>

      <div class="newline_10"></div>

      <b style="font-size:15px;"><br>Latex Bibtex Citation:</b><br>

      <p style="line-height:1.5em">
@inproceedings{wang2019asynchronous,<br>
  title={Asynchronous proximal stochastic gradient algorithm for composition optimization problems},<br>
  author={Wang, Pengfei and Liu, Risheng and Zheng, Nenggan and Gong, Zhefeng},<br>
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
  volume={33},<br>
  number={01},<br>
  pages={1633--1640},<br>
  year={2019}<br>
}</p> </div>
</div>
</td></tr>
<tr>
<td id="layout-content" width="1200px">
<h2>2018 and before</h2>
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Online Low-Rank Representation Learning for Joint Multi-Subspace Recovery and Clustering.</b><br>

      <p style="line-height:2em">Bo Li, <b>Risheng Liu</b>, Junjie Cao, Jie Zhang, Yukun Lai, and Xiuping Liu<br><strong><span style="font-size: 12px;"><strong>IEEE TIP 2018</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Explicit Shape Regression with Characteristic Number for Facial Landmark Localization.</b><br>

      <p style="line-height:2em">Xin Fan, <b>Risheng Liu</b>, Zhongxuan Luo, Yuntao Li, and Yuyao Feng<br><strong><span style="font-size: 12px;"><strong>IEEE TMM 2018</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Learning to Diffuse: A New Perspective to Design PDEs for Visual Analysis.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Guangyu Zhong, Junjie Cao, Zhouchen Lin, Shiguang Shan, and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IEEE TPAMI 2016</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning.</b><br>

      <p style="line-height:2em">Zhouchen Lin, <b>Risheng Liu</b>, Huan Li<br><strong><span style="font-size: 12px;"><strong>Machine Learning 2015</strong></span></strong></p></div>

  </div>

</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Structure-Constrained Low-Rank Representation.</b><br>

      <p style="line-height:2em">Kewei Tang, <b>Risheng Liu</b>, Zhixun Su and Jie Zhang<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS 2014</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Low-Rank Structure Learning via Nonconvex Heuristic Recovery.</b><br>

      <p style="line-height:2em">Yue Deng, Qionghai Dai, <b>Risheng Liu</b>, Zengke Zhang, Sanqing Hu<br><strong><span style="font-size: 12px;"><strong>IEEE TNNLS 2013</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Feature Extraction by Learning Lorentzian Metric Tensor and Its Extensions.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhouchen Lin, Zhixun Su, Kewei Tang<br><strong><span style="font-size: 12px;"><strong>Pattern Recognition 2010</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Learning Collaborative Generation Correction Modules for Blind Image Deblurring and Beyond.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Yi He, Shichao Cheng, Xin Fan, and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>ACM Multimedia 2018</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">A Bridging Framework for Model Optimization and Deep Propagation.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shichao Cheng, Xiaokun Liu, Long Ma, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>NeurIPS 2018</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Toward Designing Convergent Deep Operator Splitting Methods for Task-specific Nonconvex Optimization.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shichao Cheng, Yi He, Xin Fan, and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IJCAI 2018</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Fast Factorization-free Kernel Learning for Unlabeled Chunk Data Streams.</b><br>

      <p style="line-height:2em">Yi Wang, Nan Xue, Xin Fan, Jiebo Luo, <b>Risheng Liu</b>, Bin Chen, Haojie Li, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>IJCAI 2018</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Proximal Alternating Direction Network: A Globally Converged Deep Unrolling Framework.
</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xin Fan, Shichao Cheng, Xiangyu Wang, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>AAAI 2018</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Unsupervised Representation Learning with Long-Term Dynamics for Skeleton Based Action Recognition.</b><br>

      <p style="line-height:2em">Nenggan Zheng, Jun Wen, <b>Risheng Liu</b>, Liangqu Long, Jianhua Dai and Zhefeng Gong<br><strong><span style="font-size: 12px;"><strong>AAAI 2018</strong></span></strong></p></div>

  </div>

</td></tr>
<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Self-reinforced Cascaded Regression for Face Alignment.</b><br>

      <p style="line-height:2em">Xin Fan, <b>Risheng Liu</b>, Kang Huyan and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>AAAI 2018</strong></span></strong></p></div>

  </div>

</td></tr>

<!--<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Single Image Layer Separation via Deep ADMM Unrolling.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhiying Jiang, Xin Fan, Haojie Li, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>ICME 2018 <b style="color:red">(Oral)</b></strong></span></strong></p></div>

  </div>

</td></tr>--!>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Deep Location-Specific Tracking.</b><br>

      <p style="line-height:2em">Lingxiao Yang, <b>Risheng Liu</b>, David Zhang, and Lei Zhang<br><strong><span style="font-size: 12px;"><strong>ACM Multimedia 2017</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Deep Hybrid Residual Learning with Statistic Priors for Single Image Super-Resolution.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Xiangyu Wang, Xin Fan, Haojie Li, and Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>ICME 2017 <b style="color:red">(Best Paper Finalist)</b></strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Blind Image Deblurring via Adaptive Dynamical System Learning.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Shichao Cheng, Xin Fan, Zhongxuan Luo<br><strong><span style="font-size: 12px;"><strong>ICME 2017 <b style="color:red">(Best Paper Finalist)</b></strong></span></strong></p></div>

  </div>

</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Linearized Alternating Direction Method with Penalization for Nonconvex and Nonsmooth Optimization.</b><br>

      <p style="line-height:2em">Yiyang Wang, <b>Risheng Liu</b>, Xiaoliang Song and Zhixun Su<br><strong><span style="font-size: 12px;"><strong>AAAI 2016</strong></span></strong></p></div>

  </div>

</td></tr>



<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Characteristic Number Regression for Facial Feature Extraction.</b><br>

      <p style="line-height:2em">Yuntao Li, Xin Fan, <b>Risheng Liu</b>, Yuyao Feng, Zhongxuan Luo and Zezhou Li<br><strong><span style="font-size: 12px;"><strong>ICME 2015<b style="color:red"> (Best Student Paper Award)</b></strong></span></strong></p></div>

  </div>

</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Adaptive Partial Differential Equation Learning for Visual Saliency Detection.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Junjie Cao, Zhouchen Lin and Shiguang Shan<br><strong><span style="font-size: 12px;"><strong>CVPR 2014</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Robust Visual Tracking Using Latent Subspace Projection Pursuit.</b><br>

      <p style="line-height:2em">Wei Jin, <b>Risheng Liu*</b>, Zhixun Su, Changcheng Zhang and Shanshan Bai<br><strong><span style="font-size: 12px;"><strong>ICME 2014<b style="color:red"> (Best Student Paper Award)</b></strong></span></strong></p></div>

  </div>

</td></tr>

<!--<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Motion Blur Kernel Estimation via Salient Edges and Low-Rank Prior.</b><br>

      <p style="line-height:2em">Jinshan Pan, <b>Risheng Liu</b>, Zhixun Su and Guili Liu<br><strong><span style="font-size: 12px;"><strong>ICME 2014<b style="color:red"> (Oral)</b></strong></span></strong></p></div>

  </div>

</td></tr>--!>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Fixed-Rank Representation for Unsupervised Visual Learning.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhouchen Lin, Fernando De la Torre, Zhixun Su<br><strong><span style="font-size: 12px;"><strong>CVPR 2012</strong></span></strong></p></div>

  </div>

</td></tr>

<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation.</b><br>

      <p style="line-height:2em">Zhouchen Lin, <b>Risheng Liu</b>, Zhixun Su<br><strong><span style="font-size: 12px;"><strong>NIPS 2011</strong></span></strong></p></div>

  </div>

</td></tr>


<tr>
<td id="layout-content" width="1200px">
<div class="publication_container" onclick="toggleDetails(&#39;111ss40&#39;);">

    <div class="publication_title"><b style="font-size:14px">Learning PDEs for Image Restoration via Optimal Control.</b><br>

      <p style="line-height:2em"><b>Risheng Liu</b>, Zhouchen Lin, Wei Zhang, Zhixun Su<br><strong><span style="font-size: 12px;"><strong>ECCV 2010</strong></span></strong></p></div>

  </div>

</td></tr>









</tbody></table>
<p>
  <div class="newline_10"></div>
  
  
  <div style="clear:both;"></div>

  <script type="text/javascript" src="./Publication_files/jquery.min.js.???：：??"></script> 

  <script type="text/javascript" src="./Publication_files/jquery.fancybox-1.3.4.pack.js.???：：??"></script>

  <link rel="stylesheet" type="text/css" href="./Publication_files/jquery.fancybox-1.3.4.css" media="screen">

  <script language="javascript" type="text/javascript">

function toggleDetails (id) {

  var element = document.getElementById(id);

	if(element.style.display=='block') element.style.display = 'none';

	else	                             element.style.display = 'block';

}

</script>


    <div id="newline_5"></div>
    <div id="footer"> ?? 2021 |  vog  </div> </div>
<div id="fancybox-tmp"></div><div id="fancybox-loading"><div></div></div><div id="fancybox-overlay"></div><div id="fancybox-wrap"><div id="fancybox-outer"><div class="fancybox-bg" id="fancybox-bg-n"></div><div class="fancybox-bg" id="fancybox-bg-ne"></div><div class="fancybox-bg" id="fancybox-bg-e"></div><div class="fancybox-bg" id="fancybox-bg-se"></div><div class="fancybox-bg" id="fancybox-bg-s"></div><div class="fancybox-bg" id="fancybox-bg-sw"></div><div class="fancybox-bg" id="fancybox-bg-w"></div><div class="fancybox-bg" id="fancybox-bg-nw"></div><div id="fancybox-content"></div><a id="fancybox-close"></a><div id="fancybox-title"></div><a href="javascript:;" id="fancybox-left"><span class="fancy-ico" id="fancybox-left-ico"></span></a><a href="javascript:;" id="fancybox-right"><span class="fancy-ico" id="fancybox-right-ico"></span></a></div></div></body></html>

<img src="http://www.vog/" alt="" class="bigimg">
<div class="mask"> <img src="./vog_files/close.png" alt=""> </div>

</body></html>